%!TEX root = ../dissertation.tex
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------
% the code below specifies where the figures are stored
\graphicspath{{2-mir/figures/}}

\chapter{Music Information Retrieval for Transcription}
\label{ch:mir}

Being able to accurately identify all musical events from audio and transcribe them into musical notations is an essential skill for musicians as well as a paramount goal of music machine learning research.
Enabling an automatic conversion between musical audio and symbolic notations, automatic music transcription opens up many new possibilities, including the applications listed in Subsection \ref{ch:introduction}.\ref{sec:applications}.


Due to the complexity and difficulty of creating a completely end-to-end music transcription system, many existing approaches focus on a specific subtask of the problem \cite{casey2008mir}, e.g. extracting onsets and beats, recognizing timbre and instruments, tracking monophonic and polyphonic pitches, or separating audio sources from a mixture.
Each of these subtasks poses interesting goals and applications even without the lofty goal of end-to-end music transcription, and they are classified as subproblems of \emph{music information retrieval} (MIR).
Although this term has existed since 1960s \cite{kassler1966mir}, it was only after the late 1990s when active research on this area has spun off from computer music and computational musicology literature.
During the last two decades, numerous sophisticated and novel approaches for each of these subproblems have been introduced, that have continuously improved the performance in terms of the accuracy in predicting the correct annotations.
This chapter starts by introducing the common concepts and techniques employed in many AMT models, followed by reviews of the state-of-the-art techniques in each area of music transcription.
The purpose of this chapter is to provide a survey over the history of MIR research related to automatic music transcription, as well as to show a clear common pattern over the areas of MIR where the machine learning models have been evolving from simple heuristics based on hand-crafted features to sophisticated deep learning models with millions of parameters.
Many methods employing deep neural networks are referenced in this chapter, and the concepts and the formulation of those models such as convolutional newral networks (CNN) and recurrent neural networks (RNN) are described in detail in Chapter \ref{ch:deeplearning}.


\section{Feature Extraction and Basic Assumptions}

Audio data is huge in volume; a typical audio track contains 44,100 real-numbered samples per second, and sometimes even more.
Therefore, computational methods for extracting musical information from audio usually contains a pipeline of feature extraction stages to reduce the volume and increase the interpretability of input data, as shown in Figure \ref{fig:pipeline}.
The pipeline includes a few techniques widely used in speech processing, as well as many feature extraction stages created for music-specific purposes.


While there are many MIR tasks that operate on the track level, such as music recommendation, tagging, and genre classification, most subtasks of music transcription involve the prediction of labels that are dependent on time, operating either in the sample-level or frame-level.
Frames are created by taking a series of overlapping short-time audio segments, where the length of a segment typically ranges from 10 to 50 milliseconds, and optionally multiplying them by a window function.
Taking discrete Fourier transforms on the frames produces a \emph{short-time Fourier transform} (STFT), and the squared magnitude of an STFT gives a \emph{spectrogram}; 
i.e. for a signal $x[n]$ and a window function $w[n]$:
\begin{eqnarray}
\textbf{STFT}\{x[n]\}(m, \omega) & = & \sum_{n = -\infty}^{\infty} x[n] w[m-n] e^{-j \omega n}, \\
\textbf{Spectrogram}\{x[n]\}(m, \omega) & = & \big | \textbf{STFT}\{x[n]\}(m, \omega) \big |^2.
\end{eqnarray}
Spectrograms give very rich information about the audio; for example, the contour of melodies and the dynamics of music are usually identifiable from the spectrogram image.
Spectrograms are expressive enough to be used as an output of sound synthesis or a source separation algorithm, and the corresponding audio signals can be reconstructed without incurring significant perceptual inconsistencies \cite{griffin1984lim, leroux2010spectrogram}.
However, the dimensionality of a spectrogram is still quite high, making it computationally prohibitive to run many algorithms directly on an STFT or a spectrogram.

\begin{figure}[t]
	\includegraphics[width=\textwidth]{pipeline.pdf}
	\caption{\small The standard pipeline for music feature extraction. An appropriate set of feature extraction methods needs to be heuristically selected depending on the task.}\label{fig:pipeline}
\end{figure}

This necessitated further transformations by the means of filterbanks, such as \emph{Mel-Frequency Cepstral Coefficients} (MFCC) \cite{logan2000mfcc} by applying the Mel filterbank inspired by the human auditory perception and taking the first few DCT components that contain independent factors describing the spectral shape.
\emph{Constant-Q transform} (CQT) \cite{schorkhuber2010cqt} uses a filterbank where the center frequencies of filters have a constant Q factor, which is the ratio between the center frequency and the 3 dB bandwidth of a filter.
By configuring CQT to produce 12 filters per octave, it is possible to obtain the coefficients corresponding to each musical tone, and to fold the representation to produce a \emph{chromagram} \cite{harte2005chromagram}.
Tonnetz \cite{harte2006tonnetz} is a 6-dimensional feature space signifying harmonic relationships, and more sophisticated feature extractors include the summary ACF \cite{tolonen2000multipitch}, Specmurt \cite{saito2008specmurt}, and bispectrums \cite{argenti2011bispectral}.


To extract the beat and tempo information, a heuristic function, such as the first-order difference of the time-domain log energy function or the \emph{spectral flux} that measures the total energy increase over the STFT frequency bins, is applied to formulate a novelty curve.
This curve can then be used to measure energy bursts that are typically present in the onsets of notes \cite{bello2005onset}.
The onset information can be further processed to obtain tempo information via \emph{tempogram} \cite{cemgil2000tempogram} or cyclic tempogram \cite{grosche2010tempogram}.

Meanwhile, many recent approaches have successfully eliminated some or all feature transformation stages in the standard MIR pipeline by training a deep model directly on spectrograms or audio waveforms.
Applications of deep learning arose in virtually all types of MIR tasks, including melody extraction \cite{bittner2017deepsalience}, beat tracking \cite{vogl2017drum}, and genre classification \cite{oramas2017genre}, and outperformed previous feature-based approaches.
Apart from a small number of end-to-end approaches, most deep learning models for music still rely on predefined feature transforms such as STFT or CQT, because those features leverage the prior knowledge that music signals are often harmonically sparse and make it easier for a model to learn meaningful concepts without overfitting, using a smaller number of parameters and hence being achievable in a limited hardware capacity.
However, any feature extraction stage induces a loss of information, and the best-performing model would benefit most from the raw audio data, given enough amount of training data and hardware \cite{pons2018tagging}.


As is the case for feature extraction, musical prior knowledge is often applied to the algorithmic design of models as well, such as the assumption that sudden changes in music is rare and most changes happen gradually.
In the time domain, median filtering \cite{oudre2009chord} is a simple heuristic that can suppress spurious changes, and \textit{Hidden Markov models (HMM)} are widely employed for modeling sequence data such as chord progressions \cite{cho2010chord} as well as to smooth sequence outputs as a post-processing step \cite{khadkevich2009hmm}.
In a higher level, prior knowledge about musical notes can be incorporated more specifically. These approaches include detecting onsets and offsets \cite{benetos2011polyphonic}, modeling attacks and decays \cite{cheng2016attackdecay}, and more generally the temporal evolution of notes \cite{cogliati2015temporal}.
In the frequency domain, the \textit{spectral smoothness principle} states that the spectral envelopes of real sounds tend to be slowly varying as a function of frequency \cite{klapuri2003multiple}.
The principle has been implemented in a number of ways, such as a moving-average filter for iterative source estimation and separation \cite{klapuri2003multiple}, a score function for F0 candidate \cite{yeh2010mffe}, and a low-order autoregressive overtone modeling \cite{emiya2010smoothness}.

In this section, we have reviewed the common approaches for feature extraction and a few basic assumptions, which are inspired by musicological or mathematical knowledge on the musical signal and formulated in conjunction with the algorithmic design of specific models for each subtask.
The following sections provide more in-depth literature reviews on the different subtasks relevant to automatic music transcription, starting from the simplest problem of monophonic picth tracking.


\section{Monophonic Pitch Tracking}\label{sec:monophonic}

Estimating the fundamental frequency (F0) of a monophonic audio signal, also known as pitch tracking or pitch estimation, is one of the simplest but a long-standing topic of research in audio signal processing.
Pitch estimation plays an important role outside of automatic music transcription as well, where monophonic pitch tracking can be used as a method to generate pitch annotations for multi-track datasets \cite{bittner2014medleydb} or as a core component of melody extraction systems \cite{bosch2014melody, mauch2015computer}.

On a side note, pitch is defined as a subjective quality of perceived sounds and does not precisely correspond to the physical property of the fundamental frequency \cite{hartmann1997signals}.
However, apart from a few rare exceptions, pitch can be quantified using fundamental frequency, and thus they are often used interchangeably in the MIR literature outside psychoacoustical studies. 
Since differentiating the physical and the perceptual aspects of pitch is a non-goal, the two terms are interchangeably throughout this thesis as well.

Computational methods for monotonic pitch estimation have been studied for more than a half-century \cite{noll1967cepstrum}, and many reliable methods have been proposed since.
Earlier methods commonly employ a certain candidate-generating function, accompanied by pre- and post-processing stages to produce the pitch curve.
Those functions include the cepstrum \cite{noll1967cepstrum}, the autocorrelation function (ACF) \cite{dubnowski1976acf}, the average magnitude difference function (AMDF) \cite{ross1974amdf}, the normalized cross-correlation function (NCCF) as proposed by RAPT \cite{talkin1995rapt} and PRAAT \cite{boersma1993praat}, and the cumulative mean normalized difference function as proposed by YIN \cite{decheveigne2002yin}. More recent approaches include SWIPE \cite{camacho2008swipe}, which performs template matching with the spectrum of a sawtooth waveform, and 
pYIN \cite{mauch2014pyin}, a probabilistic variant of YIN that uses a Hidden Markov Model (HMM) to decode the most probable sequence of pitch values.
According to a few comparative studies, the state of the art is achieved by YIN-based methods \cite{von2010comparison, babacan2013comparative}, with pYIN being the best performing method to date.

While these methods work well for monophonic audio, they are not enough for general music transcription where multiple concurrent notes and sound sources are present.
To address this situation, the next section reviews the techniques for tracking multiple pitches.


\section{Multiple Fundamental Frequency Estimation}\label{sec:mffe}

Among the subtasks of automatic music transcription, estimating and tracking all pitches from a polyphonic recording poses the most difficult challenges, as apparent from the recent stream of results from MIREX challenges \cite{downie2014mirex}.
The task is commonly referred to as \emph{multiple fundamental frequency estimation} (Multi-F0 estimation, or MFFE) or simply \emph{polyphonic pitch tracking}.
This task is in some sense a superset of other MIR tasks like onset detection, beat tracking, chord recognition, and melody extraction,
since the frequency tracking task has to indicate the onset of every sound, and tracking chords and melodies becomes much easier when the correct annotations for all pitch values are available.


%%% Early approachs including blackboard systems

Early approaches for polyphonic pitch tracking are often based on rather strict assumptions on the types of timbre and the number of polyphony in the audio to be transcribed \cite{moorer1977transcription,piszczalski1977transcription}. Blackboard systems \cite{martin1996blackboard,dixon2000piano} are one of the first methods that enabled polyphonic transcription to work under milder assumptions, by integrating both signal processing and musicological knowledge into hierarchical problem abstraction. Limitations of the blackboard approach include the overall complexity and the exhaustive searches required by the system. Accordingly, later approaches often use statistical models or factorization-based methods to more efficiently capture pitch information.


%%% NMF and its variants

\textit{Non-negative matrix factorization (NMF)} \cite{lee1999nmf,lee2001nmf} refers to an algorithm that finds two matrices that factorize a given matrix where the elements of all three matrices are non-negative.
Starting with \citeA{smaragdis2003nmf}, many successful methods for music transcription have been built based on NMF \cite{benetos2019amt}.
These methods commonly take the approach of factorizing a time-frequency representation $\X \in \R^{F \times T}$ as a product of a dictionary matrix $\mathbf{D} \in \R^{F \times K}$ and an activation matrix $\mathbf{A} \in \R^{K \times T}$, where $K$ is the number of pitch labels to be transcribed, e.g. 88 keys for piano transcription.
This allows for an intuitive interpretation of each matrix, where each column of $\mathbf{D}$ contains a spectral template for a pitch label, and each row of $\mathbf{A}$ contains the activation of the corresponding pitch over time.
Various extensions of factorization-based methods have been proposed to leverage sparsity~\cite{abdallah2004sparse,cont2006realtime,costantini2013nmf}, non-negative matrix division \cite{niedermayer2008division}, $\beta$-divergence \cite{dessein2010beta}, adaptive estimation of harmonic spectra~\cite{vincent2010adaptive,fuentes2013harmonic}, a Bayesian framework for encouraging harmonicity and temporal smoothness \cite{bertin2009nmf,bertin2010nmf,peeling2010factorization}, and modeling of attack and decay sounds~\cite{benetos2013multi,ewert2016admm}.
With carefully designed regularizers and the \textit{alternating direction method of multipliers (ADMM)} for training NMF, \citeA{ewert2016admm,ewert2017admm} achieved close-to-perfect accuracy for piano transcription in a studio setting, i.e. when the exact spectral profile of the piano notes to be transcribed is known in advance.

  

%%% PLSA (aka PLSI or PLCA)

A probabilistic approach closely related to NMF is \textit{probabilistic latent semantic analysis (PLSA)} \cite{hofmann1999plsa}, a simple probabilistic graphical model that factorizes the joint probability distribution of time and frequency into conditional distributions involving a latent semantic factor.
PLSA is is equivalent to NMF when the KL divergence is minimized under $L_1$ normalization \cite{gaussier2005plsa,ding2008equiv}, while the probabilistic framework enables statistical learning and introducing transformation invariances \cite{smaragdis2006latent}.
In \cite{smaragdis2009relative,benetos2012latent}, a shift-invariant extension to PLSA has been applied to multi-instrument MFFE by using a convolution over constant-Q spectrograms.
\citeA{grindlay2010subspace} proposed an extension to PLSA based on the \textit{subspace NMF} algorithm \cite{grindlay2009eigeninstruments} for multi-instrument MFFE, incorporating additional parameters for instrument sources and possible pitch values.


%%% Bridging to general (Bayesian) probabilistic methods

Many probabilistic models for music transcription employ a Bayesian framework, in which the audio signal is modeled using a probability distribution conditional on unobserved variables such as the note frequencies and timing.
The transcription is then performed through Bayesian inference on those parameters, using either \textit{Markov-chain Monte Carlo (MCMC)} or \textit{variational inference}.
Bayesian models for music transcription are usually designed with prior and conditional distributions incorporating musicological and acoustical knowledge on the music and the instruments to be transcribed, such as the de-tuning of partials \cite{davy2003harmonic}.
Existing approaches in directions include
a state-space model for musical harmonics \cite{cemgil2003generative},
a decomposition of audio signals into Gabor atoms \cite{davy2006bayesian}, 
a hierarchical graphical model \cite{pesek2017hierarchical},
and an overtone corpus encoding the harmonic structure \cite{sakaue2013overtone}.
A nonparametric Bayesian model proposed in \cite{yoshii2012nonparametric} employs infinite Gaussian mixtures and noninformative hyperprior distributions, automating the model selection process.
Bayesian models provide a powerful tool connecting the whole generative process of music, but usually suffer from the high computational cost and the complexity of the algorithm.


%%% Simple data-driven classification models

Unlike the aforementioned approaches which incorporate as much prior knowledge on the music as possible, discriminative models employ a simple approach of learning a direct mapping between the audio features and the labels from large training data, making minimal assumptions on the acoustical or musical structure.
This data-driven approach is made possible by the availability of large datasets and increased computational capabilities.
\textit{Support vector machines (SVM)} were commonly used in earlier discriminative approaches, by constructing one-versus-all SVM classifiers on STFT magnitudes \cite{poliner2006discriminative}, on learned features using \textit{deep belief networks (DBN)} \cite{nam2011classification}, or on the result of NMF \cite{weninger2013nmf}.
While being flexible and straightforward to be applied on any features of choice, SVMs have high time and space complexities which limit the size of training dataset and hence the capability of the model.


%%% Neural networks and deep learning


Deep learning~\cite{lecun2015deeplearning} methods for music transcription are increasingly popular%replacing NMF-based methods
~\cite{benetos2019amt}, as larger labeled datasets and more powerful hardware become accessible.
These approaches use neural networks (NN) to produce music transcriptions from the input audio.
An early work~\cite{nam2011classification} used deep belief networks~\cite{hinton2006dbn} to extract audio features which are subsequently fed to pitch-wise SVM-HMM pairs to predict the target piano rolls.
More recent approaches are based on convolutional%neural networks
~\cite{bittner2017deepsalience,kelz2016framewise} and/or recurrent neural networks~\cite{bock2012rnn,sigtia2016endtoend,hawthorne2018onsetsframes}.
\TODO{connect the paragraph with the below:}
Music transcription models based on neural networks are relatively recent phenomena starting since 2012, with a notable exception of \citeA{marolt1999nn,marolt2004connectionist}.
Because the sequential nature of \textit{recurrent neural networks (RNN)} are suitable for modeling the time-domain dependencies in music, many neural transcription models include recurrent connections in their architecture.
RNN architectures proposed for AMT include bidirectional RNNs \cite{bock2012rnn}, recurrent temporal \text{restricted Boltzmann machines} \cite{boulangerlewandowski2012temporal}, an acoustic model combined with an RNN-based music language model \cite{sigtia2015hybrid,sigtia2016endtoend},
a sequence-to-sequence model \cite{ullrich2017seq2seq},
a dual-objective loss for onsets and frames \cite{hawthorne2018onsetsframes},
and a \textit{convolutional-recurrent neural networks (CRNN)} \cite{thome2017crnn}, which scored the top accuracy in the MFFE subtask of the MIREX 2017 competition.

% framewise convolutional layers \cite{kelz2016framewise}
% deep salience \cite{bittner2017deepsalience}
% music language model \cite{wang2018mlm}


%%% 

With the state of the art nearing the perfect accuracies \cite{ewert2017admm} on the MAPS dataset \cite{emiya2010smoothness} which have been the standard evaluation dataset for piano transcription algorithms, more systematic assessments of transcription models have recently been proposed.
These include the study of invariance under data augmentation \cite{thickstun2017invariances}, the entanglement of note representations that may prevent accurate predictions for unseen combinations of notes \cite{kelz2017entanglement}, and a musically inspired evaluation metric that also takes account of voice separation and harmonic analysis \cite{mcleod2018eval}.


%%% generating dataset and (deep) generative models

\citeA{li2017infinite} explored the idea of using on-the-fly synthesized training dataset for piano transcription, using a simple fully-connected neural network operating on the CQT representation.
The idea of using generative models to predict multiple fundamental frequencies is also not new \cite{dubois2005harmonic,cemgil2006generative}, but the authors relied on manually designed generative models for sound generation, which limits the expressibility and the generalizability of the model.
Using deep generative models can be a direction for overcoming these limitations, since the recent neural network approaches for audio generation are capable of producing highly realistic sounds.
Chapter \ref{ch:deeplearning} will review deep generative models for audio in detail.


\section{Source Separation and Music Translation}\label{sec:separation}

Source separation refers to the task of separating sound sources from a mixture signal, and is closely related to automatic music transcription, because it provides a means to separate each instrument sources from multi-instrument music.
This problem is also called as a \emph{cocktail party problem}, based on humans' ability to focus on a single voice at a noisy cocktail party.
Sound source separation has been a popular research topic since the seminal work on \emph{auditory scene analysis} by \citeA{bregman1990asa}, from which stemmed computational auditory scene analaysis (CASA) \cite{brown1994casa}, a problem of using computational models to analyze an auditory scene, identifying the sources and location of all nearby sounds.
In this sense, automatic music transcription and sound source separation are particular aspects of auditory scene analysis \cite{plumbley2002transcription}, and source separation enables similar kinds of applications to AMT, such as music editing, 3D sound rendering, and information retrieval systems.


\emph{Blind source separation} refers to the situation where no information about the sources or the mixing process is known \cite{bell1995blind}, whereas \emph{informed source separation} \cite{vincent2013separation} concerns the case where some level of side information is available, e.g. the presence of a score \cite{ewert2014separation}.
Blind sound source separation has to resort to using purely statistical approaches such as indepenent component analysis \cite{saruwatari2006ica} or robust PCA \cite{huang2012separation}, whereas informed source separation can leverage the knowledge on the musical structure \cite{rafii2013separation, liutkus2012separation} or the timbral differences of the sources \cite{li2007separation,ono2010hpss} for better separation.
Probabilistic models for source separation \cite{ozerov2007separation,leglaive2016prior} have also been developed, and source-filter modeling \cite{heittola2009separation,durrieu2011separation} is a generative approach which separately models a source that creates a sound and a filter that shapes the timbre.
The proposed AMT model is similar to source-filter models in a sense that it describes the generative process of each sound, but also relates to timbre-informed source separation models \cite{miron2018thesis}, because it needs to learn the concept of timbre to produce per-instrument piano rolls.


A closely related problem to source separation is audio translation, which concerns mapping input audio to a corresponding output with some desired properties, such as speech with reduced noise, singing voice separated from music, or the same speech content in the voice of a different speaker.
\citeA{barry2018style} applied the style transfer algorithm \cite{gatys2015style} to an ensemble of STFT, CQT, and Mel spectrograms, to transfer musical styles capturing harmonic, rhythmic, and timbral elements.
The \emph{U-Net} architecture \cite{ronneberger2015unet} uses an encoder-decoder framework with skip connections between the hidden layers at the same level of abstraction to perform image translation, and a singing voice separation model can be trained using this architecture \cite{jansson2017separation}.
The encoder-decoder architecture with skip connections can also be trained with GAN objectives, and a few audio translation models working on spectrograms have been developed; examples include singing voice separation \cite{fan2017svsgan, stoller2017separation}, source separation \cite{subakan2017gan}, and speech enhancement \cite{pascual2017segan, donahue2017segan}.



\section{Machine Learning Models for Music Synthesis}

The recent deep generative have been very successful in synthesizing breathtakingly high-quality audio signals.
We would want the synthesized music and audio signals to capture the long-term dependencies such as beats, measures, and chord progressions that ranges up to a few seconds, while the raw audio signals typically have the order of 10 thousand sameples per second.
This made end-to-end synthesis models more difficult to train than image synthesis and translation models which it usually suffices to capture dependencies ranging a few hundred pixels.
SampleRNN \cite{mehri2016samplernn}, to be discussed in Chapter \ref{ch:deeplearning} in the context of deep autoregressive models, is one of the first successful deep generative models for audio and formed a basis for the techniques used by Lyrebird, an AI startup founded by University of Montr\'{e}al students that provides API for synthesized voice of a specific person, e.g. Barack Obama.
WaveNet \cite{oord2016wavenet}, developed by Google DeepMind, uses a causal architecture using dilated convolutions to generate time-domain audio samples, and is able to produce realistic human voices and piano sounds.
WaveNet learns acoustically meaningful representations including pitch and spectral features \cite{hua2018wavenet}.
There also exist faster approaches using recurrent neural networks to produce vocal and musical audio, as found in \cite{nayebi2015gruv} and \cite{kalingeri2016generation}, albeit with lower quality when compared to WaveNet.
Tacotron \cite{wang2017tacotron, shen2018tacotron} is a fully end-to-end speech synthesizer that works directly on a sequence of characters, which can learn the pronunciation of unseen complex words and different ways of reading the same word according to the phrase semantics and punctuations.
A newer RNN-based model called WaveRNN \cite{kalchbrenner2018wavernn} is capable of generating audio that matches WaveNet in quality, yet with an enough efficiency to be able to run real-time on GPUs or even on mobile phones.
A singing synthesis model \cite{blaauw2017singing} based on the WaveNet architecture is also capable of synthesizing voice parametrically, separating the influence of pitch and timbre in the model.
A music synthesis technique employing a similar approach as the above will be a key component of the overall architecture, allowing the transcription model to generate realistic-sounding music to compare with the input audio.


A problem of producing spectrograms as an output of image translation model is that reconstructing the resulting audio requires the phase information.
While using the phases from the input STFT can produce acceptable results for a translation model, a phase reconstruction algorithm is required for a generative model for audio.
A GAN architecture using one-dimensional convolutions called \emph{WaveGAN} was recently introduced \cite{donahue2018wavegan}, which is capable of generating 1-second audio segments from the latent representations.
The 1-D output does not require phase reconstruction, and it was able to achieve a better perceptual quality than the audio reconstructed using 2-D GANs.
As shown in the paper, training of 1-D GANs is much more susceptible to the choice of the GAN objective than the 2-D GANs, and training GAN for longer audio sequences and extending it as a tool for disentanglement of latent semantic information or a conditional audio synthesis framework remains as a challenge.

\section{Music Language Models for Symbolic Music Generation}

Symbolic music processing refers to the techniques for processing music at a symbolic level, such as in the form of sheet music, MIDI signals, or piano roll representations.
Problems in this domain includes optical music recognition \cite{rebelo2012omr}, algorithmic composition \cite{fernandez2013ai}, and computational music theory \cite{hamanaka2013computational}, while the subject most relevant to music transcription research would be \emph{music language models}.
A music language model is a statistical model, often a generative model, that encodes music theoretic knowledge to describe the structural composition and arrangement of musical elements \cite{patel2010musiclanguage}, similarly to how computational linguists build language models to describe the structure of natural languages.
A well-designed music language model can be an important component for a generative model for music, because it can serve as a prior for latent representations and can be combined with conditional synthesis models or software instruments to produce audio.
As a side note, there are two distinct objectives for symbolic music generation systems --- style imitation and genuine composition \cite{nierhaus2009composition} --- among with this thesis only focuses on the former, aiming at informing the transcription model about music theoretic knowledge rather than imagining a new type of musical art.

The first systematic approach of applying a linguistic theory to music was the \emph{generative theory of tonal music} \cite{lerdahl1983gttm}, which was inspired by Noam Chomsky's generative grammar \cite{chomsky1966generative} and was influential in music theory, music psychology, and cognitive musicology.
Music language models implement this idea using computational methods, typically involving statistical models that are also used in natural language processing.
These include many kinds of approaches for symbolic music generation, such as hidden Markov models \cite{farbood2001markov}, generative grammars \cite{chemilier2001grammar}, cellular automata \cite{burraston2004automata}, and genetic algorithms \cite{miranda2007evolutionary}.
More recently, deep learning models such as a recurrent neural network to build a music language models \cite{sigtia2014lm} has also been proposed.
The latest approaches to generate realistic sound music sequences in the symbolic domain include an application of variational autoencoder \cite{teng2017generating,tikhonov2017generation}, and a generative adversarial network \cite{yang2017midinet}.

\section{Summary}

In this chapter, a broad range of MIR techniques related to automatic music transcription have been discussed, in the fields of monophonic and polyphonic pitch tracking, source separation and music translation, music synthesis, and music language models.
A clear observation in each subtask of AMT is that many recent methods employ deep learning models, and this is because deep models have more flexibility and capacity to learn complex statistical relations of interest.
More details on deep learning is discussed in the next chapter, in order to build solid methodological design for the proposed AMT system using the deep models.
