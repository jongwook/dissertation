%!TEX root = ../dissertation.tex
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------
% the code below specifies where the figures are stored
\graphicspath{{2-deeplearning/figures/}}

\chapter{Deep Learning in a Nutshell}\label{sec:deeplearning}
\label{ch:deeplearning}

Since recently, a family of machine learning research under the buzzword \emph{deep learning} has incurred many groundbreaking changes to the world of artificial intelligence, making the long-waited dream of the \emph{artificial general intelligence} (AGI) \footnote{a loosely defined term referring to human-level intelligence, i.e. an AI system that can solve complex problems in various, even previously-unseen domains with self-understanding and autonomous self-control. \cite{goertzel2007agi}} look not so distant in the future.
The impact of deep learning has been so dramatic that many successful applications of deep learning like DeepMind's AlphaGo outplaying the human Go champion and Google's neural machine translation have became familiar to the general public.
The core idea of using artificial neural networks to process complex information traces back to the earliest days of computing \cite{kleene1951representation}, but it has long been considered less effective than alternative methods, such as support vector machines or probabilistic graphical models.
Since around 2010, it has been increasingly shown that neural networks can substantially outperform those other approaches and have much more capability for further improvements, and that the lower performance of neural networks in the past was merely due to insufficient data, the lack of computational power, and some numerical tricks that have not been employed before.
This finding has opened the era of deep learning, a term coined after the fact that neural networks often employ multiple layers of learned feature transformations, and is continuing to innovate virtually all fields of science and engineering, including, of course, music technology.

This chapter reviews the essential concepts and terminologies of deep learning, from the basic architectures and techniques to the most recent advances in deep generative models.
The purpose of this chapter is to present a historical perspective toward deep generative models and to provide a motivation for building music transcription systems upon them in the proposed research.


\section{Neural Network Architectures}

The key idea of an artificial neural network in the simplest setting is to find an appropriate matrix $W$ to model the relationship between variables $\bm{x}$ and $\bm{y}$, so that
\begin{equation}\label{eqn:perceptron}
	\bm{y} = \sigma(W \bm{x})
\end{equation}
is a good approximation, where $\sigma$ is a nonlinear function like the sigmoid or the hyperbolic tangent.
This model in Equation \ref{eqn:perceptron} is also known as a \emph{perceptron} \cite{rosenblatt1957perceptron}, one of the first artificial neural networks in history.
This computation --- a matrix multiplication followed by a nonlinear activation --- can be applied multiple times, like
\begin{equation}\label{eqn:mlp}
	\bm{y} = \sigma(W_3\sigma(W_2 \sigma(W_1 \bm{x}))),
\end{equation}
which gives the model more expressive power, meaning that it can learn more complex relationship in the data that the previous model could not discern, e.g. the XOR problem \cite{riedmiller1994mlp}.
The model in Equation \ref{eqn:mlp} is called a \emph{multilayer perceptron} (MLP) in a sense that it is a concatenation of perceptrons, and the fact that it contains multiple layers is why these neural networks are called ``deep".


A multilayer perceptron is a special case of feedforward neural networks, which refer to any computational graph that does not contain a cycle.
A popular model under this category is \emph{convolutional neural networks} (CNN), which uses a convolution (a cross-correlation, to be precise) with fixed-size kernels instead of matrix multiplications.
A 2-D convolutional layer takes input arrays $X_c \in \mathbb{R}^{H \times W}$, $c \in \{ 1, \cdots, C \}$, and produces output arrays $Y_d \in \mathbb{R}^{H \times W}$, $d \in \{ 1, \cdots, D \}$.
The kernels $K_{cd} \in \mathbb{R}^{K_1 \times K_2}$, $c \in \{ 1, \cdots, C \}$, $d \in \{ 1, \cdots, D \}$, and the biases $\bm{b} \in \mathbb{R}^D$ are the parameters to be optimized, and the output is calculated as:
\begin{equation}\label{eqn:convnet}
Y_{d}[i,~j] = \sum_{m=1}^{K_1} \sum_{n=1}^{K_2} \sum_{c=1}^C K_{cd}[m,~n] X_c[i+m,~j+n] + b_d
\end{equation}
There are various options to this operation including whether to pad the input or trim the output of the convolution to according to the kernel size, and how much to stride the kernels while moving along the input arrays.
The 2-D convolution is suitable for image data, where the initial $C$ can be the 3 RGB channels of color images, while the similarly-defined 1-D and 3-D convolutions are more often used with time-series and video data respectively.

Using convolutional layers results in a fewer number of parameters to learn in each layer than the equivalent multilayer perceptron, allowing deeper models for the same total number of parameters.
LeNet \cite{lecun1995lenet} for digit classification is what pioneered the technique of using convolutional layers in neural networks, and it is an essential building block of the majority of deep learning methods, including the models that surpassed the human-level accuracy in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) \cite{krizhevsky2012imagenet, simonyan2014vgg, szegedy2015googlenet, he2016resnet}.
A standard practice of building a CNN is to stack a multiple convolutional layers along with pooling layers to obtain a compact feature repesentation, which is fed to a multi-layer perceptron as in Equation \ref{eqn:mlp} to produce output.
The layers of MLP are called fully connected or dense layers, because unlike the convolutional layers, the weight matrix used in the matrix multiplication associates every pair of the input and output features.
\emph{Fully convolutional networks}, which omit the fully connected layers that are typically placed at the last stages of neural networks, do not require a fixed input and output size and are known to perform well for image segmentation \cite{shelhamer2017fcn}.
Using the ability of deep convolutional layers extracting complex semantic information from images, many artistic applications have been developed, such as the transfer of artistic style from one image to another \cite{gatys2015style}, and a captivating transformation of images using neural network weights known as \emph{Deep Dream} \cite{mahendran2016deepdream}.


A network with cyclic connections is called a \emph{recurrent neural network} (RNN), and has been successfully applied to modeling sequence data.
Because it is hard for a recurrent neural network to propagate long-range dependencies through a chain of recurrent connections, specific recurrent units called long short-term memory (LSTM) \cite{hochreiter1997lstm} and gated recurrent unit (GRU) \cite{cho2014seq2seq} are devised to resolve the problem and are considered essential for recurrent neural networks.
A formulation of recurrent neural network called the sequence-to-sequence model \cite{cho2014seq2seq,sutskever2014seq2seq}, which can model a mapping from variable-length input to variable-length output, is well known to be very effective for machine translation, and is deployed in production in Google's translation services \cite{wu2016google}.
An important technique for building recurrent neural networks is \emph{attention} \cite{bahdanau2014attention}, which allows the network to focus on a specific part of a sequence.
The attention mechanism is shown to be effective in tasks including not only machine translation as in the original paper, but also in image description generation \cite{karpathy2017desc}, speech recognition \cite{chorowski2015speech}, and question answering \cite{sukhbaatar2015memory}.

\emph{Reinforcement learning} is a formulation of machine learning where a software agent takes actions in an environment to maximize the reward given according to the actions \cite{sutton2018reinforcement}.
This formulation is inspired by behaviorist psychology and is well-suited for environments that require explorations by the agent, such as robotics and games.
Deep Q-Network (DQN) \cite{mnih2015dqn} is a neural network model designed for reinforcement learning, which has been successfully applied to automatically playing Atari games \cite{mnih2013atari} and the agent playing the game of Go that surpassed the human level \cite{silver2016alphago}.


\section{Performance Optimization Techniques}

The success of deep learning was possible not only because of the architectural design of deeper models and the hardware capable of supporting such models, but also thanks to the numerous elaborate techniques and clever tricks that enabled previously impossible performances.

Training a neural network involves optimization of its parameters, e.g. the weights $W$ in Equation \ref{eqn:perceptron}-\ref{eqn:mlp} and the kernels $K_{cd}$ in Equation \ref{eqn:convnet}, which typically requires the gradient of the loss function, i.e. the partial derivatives with respect to all of the model's parameters.
It is feasible to manually derive the gradient for simple models, but for deep neural networks it is often too complex and error-prone to calculate the derivative by hand.
For this reason, a method called backpropagation \cite{werbos1982backpropagation, rumelhart1986backpropagation} was introduced based on the ideas of automatic differentiation \cite{linnainmaa1970ad} and revived neural network research that had been largely abandoned.
The popularization of \emph{backpropagation} in the 1980s partly contributed to the ending of the first AI winter, leading to the first commercially successful application of neural network in optical digit recognition and speech recognition.
Backpropagation is still a fundamental element of deep learning, and many deep learning frameworks are capable of automatically calculating gradients using backpropagation when a compute graph is given.
This enables the developer to write only the forward calculation and run the backpropagation automatically, greatly improving the productivity.


Once a gradient is known, the standard way of optimizing a neural network is to use a variant of \emph{stochastic gradient descent} (SGD), where the direction of the gradient descent is determined only based on a mini-batch of training data.
Although using only a tiny subset of training data makes the gradient unstable, in practice, stochastic gradient descent converges faster than batch gradient descent using the same amount of training samples.
Adding momentum in the gradient descent optimizer has shown to be effective for finding the convergence even faster, and many schemes for applying the momentum have been introduced, such as Adagrad \cite{duchi2011adagrad}, RMSprop \cite{hinton2012rmsprop}, Adadelta \cite{zeiler2012adadelta}, and Adam \cite{kingma2015adam}.
While Adam is by far the most popular choice of optimizer, a few modification to Adam's algorithm have been proposed, including Eve \cite{koushik2016eve} using feedback from the objective function, Nadam \cite{dozat2016nadam} incorporating Nesterov's accelerated gradient descent, and AMSgrad \cite{reddi2018amsgrad} fixing a failure case of Adam where it does not converge to the optimum even in a simple convex optimization problem.

Historically, the sigmoid and the hyperbolic tangent function have been popular choices for the nonlinearity, but it is surprisingly shown \cite{nair2010relu} that the \emph{rectified linear units} (ReLU),
\begin{equation}
	f(x) = \max \{ x, 0 \},
\end{equation}
generally improves the accuracy of deep learning models.
It is also known that neural networks with ReLU activations converge faster, and more robust to the vanishing gradient problem.
A number of ReLU variants, including leaky ReLU \cite{xu2015leakyrelu}, parametric ReLU (PReLU) \cite{he2015prelu}, SReLU \cite{jin2015srelu}, have been devised and shown to be effective in some cases.


As with any other machine learning methods, overfitting is a problem to overcome for deep learning models as well.
While directly adding a L1 or L2 regularization term of weights is possible, a few cleverer tricks for preventing overfitting have been devised and widely employed, and they are treated as regularization methods in a wider sense.
\emph{Dropout} \cite{srivastava2014dropout} is a simple yet powerful regularization method that turns off a random subset of activations during the training process.
Because the network has to learn how to make accurate predictions using only a random subset of its components, the training becomes more robust and less susceptible to overfitting.
\emph{Batch normalization} \cite{ioffe2015batchnorm} is a method to reduce what the covariance shift of activations, by performing normalization for each training mini-batch so that the activations of each layer have zero mean and unit variance, and is also known to improve the generalizability of the trained model.
Despite being relatively new, dropout and batch normalization are drop-in methods that can be added to most deep architectures with almost no changes to code and yet significantly improve the performance, and are thus included almost by default in the majority of newer deep models.
\emph{Scaled exponential linear units} (SELU) \cite{klambauer2017selu} use a special activation function that induces a self-normalizing property over layers, making the activations have zero mean and unit variance without using batch normalization explicitly.


Additionally, because typical neural networks contain thousands to millions of parameters to train, a proper initialization of the weights prior to training is important.
In early days of deep learning, unsupervised pre-training of weights \cite{bengio2007greedy,erhan2010pretraining} was considered necessary, but recently it is shown that a simple random initialization of weights is sufficient with the current computational power of the hardware.
A widely practiced way of initializing the weights without unsupervised pre-training is to sample from a Gaussian or uniform distribution, scaled according to the number of input and output nodes \cite{glorot2010initialization,he2015prelu}.


\section{Toward Deep Generative Models}

Statistical models that describe how data is generated are called \emph{generative models} and provide means of generating samples of data, either by directly modeling the data distribution or through a sampling procedure specified by the model which implicitly defines the probability distribution.
This is in contrast with \emph{discriminative models}, which can only predict the labels corresponding to the given data samples.

\subsection{Traditional Models}

Classic examples of generative models include \emph{na\"{i}ve Bayes classifiers} \cite{maron1961naive} which model a conditional distribution of each feature assuming they are conditionally independent given the label and use Bayes' theorem to predict the labels.
\emph{Gaussian mixture models} (GMM) \cite{everitt1981mixture} approximates the data distribution with a mixture of multivariate gaussian distributions.

While these simple models work effectively to a certain degree with a well-crafted set of features, it is desirable to have generative models that can capture more intricate geometry of the data distribution.
\emph{Probabilistic graphical models} (PGM) specify the structural dependencies between random variables using graphs, with nodes representing random variables and connections between them representing their dependencies.
The graphs can have directed or undirected connections to formulate the joint probability distributions of the variables.
\emph{Hidden Markov models} (HMM) \cite{rabiner1989hmm} and \emph{latent Dirichlet allocation} (LDA) \cite{blei2003lda} are special cases of directed probabilistic graphical models, also called \emph{Bayesian networks}, and are widely used for sequence modeling and topic modeling, respectively.
Undirected graphical models are also called \emph{Markov random fields} (MRF), and have many applications in image processing, typically by having connections between nodes corresponding to adjacent pixels.
Undirected graphical models where every pair of nodes has a connection are called \emph{Boltzmann machines} and are capable of learning internal representations of data.

\subsection{Early Deep Generative Models and Autoregressive Models}

\emph{Restricted Boltzmann machines} (RBM) are simplified variants of Boltzmann machines consisting of two layers of nodes with only interlayer connections, and unlike Boltzmann machines, there exists a relatively efficient algorithm \cite{hinton2005cd} for training restricted Boltzmann machines.
\emph{Deep belief networks} (DBN) \cite{hinton2006dbn} are composed of multiple layers of restricted Boltzmann machines, which can be trained using greedy layer-wise optimization, and are capable of classifying hand-written digits as well as conditionally generating them.
Although deep belief networks are one of the first successful deep learning applications and gave many architectural and algorithmic insights to the development of deep learning in the subsequent years, the algorithms for training DBNs are not as scalable as those for other discriminative models like stochastic gradient descent, and eventually faded away in favor of the discriminative models that runs more effectively with a larger scale of data.

An alternative method to generate data samples using a neural network is to produce one element (i.e. one audio sample or one pixel) at a time by feeding the previous elements to the network.
This approach is called an autoregressive model, and many architectures based on this idea including NADE \cite{larochelle2011nade}, DARN \cite{gregor2013darn}, RIDE \cite{theis2015ride}, DRAW \cite{gregor2015draw}, PixelCNN/PixelRNN \cite{oord2016pixel}, SampleRNN \cite{mehri2016samplernn}, WaveNet \cite{oord2016wavenet}, and WaveRNN \cite{kalchbrenner2018wavernn} are proposed and shown to be capable of generating image and audio samples.
However, in addition to being inevitably slow having to repetitively run the model for every element, a drawback of autoregressive models is the difficulty of interpreting the representation, because the autoregressive model only encodes the local dependency of one sample on the adjacent elements and does not provide a compact latent representation corresponding to the global structure.

\subsection{Variational Autoencoders}

A straightforward method to obtain a compact latent representation from unlabeled data is to build an encoder that transforms the input data into a smaller latent dimension, followed by a decoder that maps it back to the original data.
This architecture is called an \emph{autoencoder} \cite{bengio2009deeplearning}, and being a deep extension to principal component analysis (PCA), it is capable of learning a nonlinear mapping for dimensionality reduction.
The autoencoder architecture are shown to be effective at encoding the latent representation of data, through a few successful variants including sparse autoencoder \cite{ng2011sparse} which produces a sparse representation of the input data, denoising autoencoder \cite{vincent2008denoising} which is capable of reducing noise or recover a redacted portion of an image, and contractive autoencoder \cite{rifai2011contractive} which adds a regularization term to make the model robust to slight variations of input values.


Autoencoders are not generative models in a strict sense, because, while its decoder part can produce data samples from their latent representations, it lacks the ability to randomly sample the points in the latent dimensions that corresponds to the data distribution.
\emph{Variational autoencoders} (VAE) \cite{kingma2013vae} fix this problem by restricting the posterior latent distribution to be Gaussian.
This is achieved by variational inference, reformulating the evidence lower bound (ELBO) of the data log-likelihood as:
\begin{equation}\label{eqn:vae}
\log p(\bm{x}) \ge \mathcal{L}(p_\theta, q_\phi) = \mathbb{E}_{q_\phi(\bm{z}|\bm{x})} [\log p_\theta(\bm{x}|\bm{z})] - \mathrm{KL}(q_\phi(\bm{z}|\bm{x}) || p(\bm{z})),
\end{equation}
where $q_\phi(\bm{z}|\bm{x})$ models the encoder and $p_\theta(\bm{x}|\bm{z})$ models the decoder, and both are parameterized using neural networks which provides a flexible and differentiable family of functions.
Note that maximizing $\mathcal{L}$ will maximize the first term of RHS, the log likelihood of the reconstructed data, and minimize the second term, the KL divergence between the encoded data distribution and the prior, serving as a regularizer that induces the posterior to be Gaussian.
The Gaussian prior gives the KL divergence a closed-form solution making it straightfoward to derive the derivative according to $\phi$, whereas the first term contains an expectation over a distribution depending on $\phi$, which disallows moving the gradient operator into the expectation and makes the stochastic gradient descent and backpropagation impossible.
A reparameterization trick is used to address this issue, by setting $\bm{z} = g({\epsilon}, \bm{x}) = \bm{\mu}_\phi (\bm{x}) + {\epsilon} \cdot \bm{\sigma}_\phi(\bm{x})$ where $\epsilon \sim \mathcal{N}(0, 1)$, which gives:
\begin{equation}\label{eqn:reparam}
\nabla_\phi \mathbb{E}_{q_\phi(\bm{z}|\bm{x})} [\log p_\theta(\bm{x}|\bm{z})] = \nabla_\phi \mathbb{E}_{\epsilon} [\log p_\theta(\bm{x}|g(\epsilon, \bm{x}))] = 
\mathbb{E}_{\epsilon} [\nabla_\phi \log p_\theta(\bm{x}|g(\epsilon, \bm{x}))],
\end{equation}
making the stochastic gradient ascent and backpropagation on the ELBO possible.


The biggest drawback of variational autoencoders, however, is the blurriness in reconstructed images, that may come from the inexactness of the Gaussian assumption and the variational lower bound used by the model \cite{doersch2016tutorial}.
There have been many attempts to overcome this by allowing more flexible prior distributions \cite{rezende2015flow} as well as better latent representations \cite{kingma2016iaf}.
VQ-VAE \cite{oord2017vqvae} uses discrete prior and posterior distributions, and is able to generate less blurry images and perform speaker conversion using raw audio.
Variational autoencoders are powerful deep generative models with the advantages of having a single objective function to be optimized and thus having a stable training scheme.
Despite being one of the most successful types of deep generative models to date, it remains to be seen if variational autoencoders can be extended to become a building block of an automatic music transcription system.


\section{Generative Adversarial Networks}\label{sec:gan}

\emph{Generative adversarial networks} (GAN) \cite{goodfellow2014gan} are a family of deep generative models that have become extremely popular.
Unlike other deep neural network models that use optimization to find the weights minimizing the loss function, GANs try to find a Nash equilibrium between its two components, the generator and discriminator.
Given the training data $\bm{x}\sim p_{\mathrm{data}}$ and the prior of latent vectors $\bm{z} \sim p_{\bm{z}}$ which typically is a multivariate Gaussian distribution, GAN performs the following minimax game:
\begin{equation}\label{eqn:gan}
	\min_{G} \max_{D} \Big[ \mathbb{E}_{\bm{x} \sim p_{\mathrm{data}}} {\log D(\bm{x})} + \mathbb{E}_{\bm{z} \sim p_z} \log \left ( 1 - D(G(\bm{z})) \right ) \Big],
\end{equation}
where the generator $G$ learns to transform a noise vector $\bm{z}$ into a data point that can fool the discriminator as if it is a real data sample, while the discriminator $D$ tries to correctly distinguish the output of generator $G(\bm{z})$ from the real data $\bm{x}$.
Because the second expectation has a near-zero gradient where $D(G(\bm{z})) \approx 0$, i.e. the discriminator classifies the generated samples as fake, the authors suggests using a non-saturating loss for training the generator:
\begin{equation}\label{eqn:nsgan}
\max_{G} \mathbb{E}_{\bm{z} \sim p_{\bm{z}}} \log D(G(\bm{z})),
\end{equation}
which maximizes the log-likelihood of the discriminator classifying the generated samples as real.


\subsection{Evolution of the GAN Architecture}

The original formulation of GAN uses neural networks, which can only be applied to simple datasets of up to 32$\times$32 images such as MNIST, CIFAR-10, and Toronto Face Dataset.
LAPGAN \cite{denton2015lapgan} is the first GAN formulation to generate 64$\times$64 images, which builds upon a Laplacian pyramid of convolutional layers that conditionally generates an image that is twice larger, gradually building 64$\times$64 images from 4$\times$4 samples.
DCGAN \cite{radford2015dcgan} provides a simpler method of training convolutional GANs by following a list of architectural choices, making adversarial training of 64$\times$64 images possible only using one generator and one discriminator.
Together with Improved GAN \cite{salimans2016improved} which proposed now-standard tricks such as feature matching, minibatch discrimination, historical averaging, and one-sided label smoothing to generate 128$\times$128 images, the DCGAN architecture is employed by virtually all subsequent GAN applications.
For photo-realistic images synthesis in a higher resolution, StackGAN \cite{zhang2017stackgan2} uses a multi-stage GAN architecture to synthesize 256$\times$256 images, and progressive growing of GANs \cite{karras2017pggan} uses a training scheme that gradually switches to larger GANs to generated photo-realistic images of size 1024$\times$1024.
The GAN architecture is also applicable to 3-D \cite{wu2016gan} and 1-D \cite{donahue2018wavegan} synthesis.


\subsection{The GAN Zoo}

GANs are notoriously difficult to train \cite{arjovsky2017principled}; its convergence is unstable because of the minimax nature of its formulation, and the generator network may simply memorize and output just a few samples of training data, an undesirable phenomena called mode collapsing.
A plethora of variations of GAN have been proposed to mitigate this problem, aiming to stabilize the training and/or improve the perceptual quality of generated samples.
A common approach among them is to devise a different loss function or to add a regularization term to the loss function that penalizes mode collapsing.

$f$-GAN \cite{nowozin2016fgan} is a generalization of the original GAN using a family of $f$-divergences in addition to the original GAN's formulation using Jensen-Shannon divergence.
Wasserstein GAN (WGAN) \cite{arjovsky2017wgan} minimizes the Wasserstein distance between the model and real distribution, and was later extended to WGAN-GP \cite{gulrajani2017wgan} which uses a gradient penalty that does not require weight clipping as in Wasserstein GAN.
Based on WGAN's observation that Lipschitz continuity of GAN is beneficial, spectral normalization \cite{miyato2018spectral} is another technique to impose Lipschitz continuity that is more stable and provides higher diversity in generated images than gradient penalty.
Wasserstein distance is not an $f$-divergence but a special case of integral probability metrics (IPM), which also includes maximum mean discrepancy (MMD) distance, on which MMD GAN \cite{li2017mmdgan} and Distributional Adversarial Networks (DAN) \cite{li2017dan} are based.
Fisher GAN \cite{mroueh2017fishergan} and Sobolev GAN \cite{mroueh2018gan} are also IPM-based GAN models.

Least-Square GAN (LSGAN) \cite{mao2017lsgan} uses least-square losses instead, and also can be trained with gradient penalty \cite{mao2017effectiveness} which achieves a training stability similar to WGAN-GP's.
Energy-Based GAN (EBGAN) \cite{zhao2017ebgan} views the discriminator as an energy function that puts low energy near the data manifold, and uses an autoencoder to model the discriminator. Boundary Equilibrium GAN (BEGAN) \cite{berthelot2017began} extends the EBGAN architecture using Wasserstein distance to balance the generator and discriminator during training.
CoulombGAN \cite{unterthiner2017coulomb} formulates the GAN setup in terms of a potential field of charged particles, and DRAGAN \cite{kodali2017gan} poses GAN training as a regret minimization problem in online learning.
Unrolled GAN \cite{metz2016unrolled} uses a surrogate objective function for the more stable generator updates, which approximates the optimal discriminator in the generator's perspective.
While there are many more GAN formulations claiming to be superior than others, a large-scale empirical study \cite{lucic2017gan} suggested that the none of the popular variants actually outperforms the original GAN, provided that the hyperparameters are sufficiently optimized.


\subsection{Conditional Generation}

It is usually desirable to generate samples according to certain conditions, e.g. generating images for a specific digit.
Conditional GAN (cGAN) \cite{mirza2014conditional} is an architecture where the generator can use the class label as well as the noise input to produce samples.
Auxiliary Classifier GAN (AC-GAN) \cite{odena2016acgan} is an extension to cGAN in which the discriminator can also serve as a classifier.
InfoGAN \cite{chen2016infogan} can perform conditional generation in a completely unsupervised manner, i.e. without using class labels during training, by maximizing the mutual information between a subset of the latent variables and the observations.

sAll of the above architectures use conditional latent components concatenated to the other feature components, which makes training a conditional GAN with a large number of classes difficult.
\cite{miyato2018cgan} overcomes this by performing projections on the feature space of the discriminator, successfully demonstrating conditional image synthesis on the 1,000 classes of images of the ILSVRC dataset \cite{russakovsky2015imagenet}.


\subsection{GANs with Encoder}

While the generator can produce data samples from latent vectors, the default GAN formulation does not provide means to obtain the latent vector corresponding to the given data sample.
This task is usually referred to as inference in GAN literature, and many architectures have been proposed to achieve this.
Adversarially learned inference (ALI) \cite{dumoulin2017ali} and Bidirectional GAN (BiGAN) \cite{donahue2016bigan} both refers to the same architecture that jointly trains an encoder that calculates the inverse of generator, by training discriminator to operate on pairs of the data samples and the corresponding latent vectors:
\begin{equation}\label{eqn:bigan}
\min_{G,~E} \max_{D} \Big[ \mathbb{E}_{\bm{x} \sim p_{\mathrm{data}}} {\log D(E(\bm{x}), \bm{x})} + \mathbb{E}_{\bm{z} \sim p_{\bm{z}}} \log \left ( 1 - D(z, G(\bm{z})) \right ) \Big].
\end{equation}

ALICE \cite{li2017alice} resolves the non-identifiability problem present in ALI that , by adding a conditional entropy regularization term, and bidirectional conditional GAN (BCGAN) \cite{jaiswal2017bcgan} combines BiGAN with cGAN to perform both inference and conditional synthesis.
Mode regularized GAN (MRGAN) \cite{che2016mrgan} has a similar setup of jointly training an encoder network and uses a regularization term on the generator loss that discourages mode collapse:
\begin{equation}\label{eqn:mrgan}
\min_{G,~E} \Big[ - \mathbb{E}_{\bm{z} \sim p_{\bm{z}}} [ \log  D(G(\bm{z})) ]
+ \mathbb{E}_{\bm{x} \sim p_{\mathrm{data}}} [ \lambda_1 d(x, G(E(\bm{x}))) + \lambda_2 \log D(G(E(\bm{x}))) ] \Big],
\end{equation}
where $d$ is a distance metric defined in the data space.
Adversarial generator-encoder (AGE) networks \cite{ulyanov2017age} achieve a comparable quality to other GANs using only two components, the generator and the encoder, without the discriminator.


\subsection{Fusing GAN with Variational Autoencoder}

The idea of combining the autoencoder architecture and adversarial training gave birth to new kinds of deep generative models.
Adversarial Autoencoders (AAE) \cite{makhzani2015aae} jointly train a standard autoencoder and an adversarial network that regularizes the posterior distribution to be Gaussian.
VAE-GAN \cite{larsen2015vaegan} trains a concatenation of VAE and GAN using the sum of their loss functions, and Adversarial Variational Bayes (AVB) \cite{mescheder2017adversarial} similarly extends on variational autoencoder using an auxiliary discriminator network to perform adversarial training, which brings the variational lower bound tighter to the maximum likelihood.
$\alpha$-GAN \cite{rosca2017alphagan} is yet another method to combine the VAE and GAN loss, which adds additional discriminator on the $\bm{z}$ domain to distinguish the encoded data samples from the Gaussian prior.


\subsection{Appilcations of GAN}

Many image-to-image translation models have been developed using GANs. Pix2pix \cite{isola2016pix2pix} is trained on pairs of cross-domain images, and learns to translate images between the domains, e.g. satellite images to corresponding maps, day photos to night photos, and edges of images to the original.
DiscoGAN and CycleGAN \cite{kim2017discogan, zhu2017cyclegan} are capable of performing similar tasks, but does not require paired training samples, greatly expanding the ranges of datasets that can be used for training.
StarGAN \cite{choi2017stargan} learns to translate between more than two domains.

GAN has been successfully applied to many other tasks, including image super-resolution (AffGAN) \cite{sonderby2016amortised}, text-to-image synthesis (StackGAN) \cite{zhang2017stackgan,zhang2017stackgan2}, text generation (Boundary-seeking GAN, BGAN) \cite{hjelm2018bsgan}, and speech enhancement (SEGAN) \cite{pascual2017segan}.


\subsection{Evaluation of Generated Samples}

Unlike discriminative models that can be evaluated using well-defined metrics, it is not as straightforward to evaluate the performance of generative models.
Structural similarity (SSIM) \cite{wang2004ssim} and its multi-scale extension MS-SSIM \cite{wang2003msssim} can measure the similarity between two images using luminance, contrast, and structure information, and are commonly used for measuring intra-class diversity \cite{odena2016acgan}.
This metric can be useful for assessing the mode collapsing problem, but is less useful if the dataset is already diverse \cite{fedus2018equilibrium}.

Another popular method for evaluating the perceptual quality of generated images is the Inception score \cite{salimans2016improved}, based on the image classification model under the same name \cite{szegedy2015inception} which is a 1000-class image classifier trained on the ILSVRC dataset \cite{russakovsky2015imagenet}.
Assuming that meaningful images would have a low-entropy conditional label distribution, and a diverse set of generated images would have a high-entropy marginal label distribution, the Inception score can be obtained by feeding generated images to the Inception classifier and calculating:
\begin{equation}\label{eqn:inception}
\exp \left ( \mathbb{E}_{\bm{x}} \Big[ \textrm{KL} \left ( p(y|\bm{x}) || p(y) \right ) \Big] \right ).
\end{equation}
For the domains other than natural images, Inception score can be calculated using a different classifier; in \cite{donahue2018wavegan} for example, an Inception score using an audio classifier is used to evaluate the quality of generated audio excerpts.
While Inception scores correlate well with human perception, a drawback of is that the distribution of real data is not considered in the calculation, for a metric measuring how realistic the generated samples are.

Fréchet Inception distance (FID) \cite{heusel2017ttur} address this problem and provides a distance metric between the data distribution and model distribution.
FID is defined using the activations of a coding layer of the Inception-v3 model, \texttt{pool\_3} to be specific, as the Fréchet distance between multivariate Gaussian approximations of the two distributions:
\begin{equation}\label{eqn:fid}
d^2 = \left \lVert \bm{\mu}_1 - \bm{\mu}_2 \right \rVert^2 + \mathrm{Tr} \left ( C_1 + C_2 - 2 ( C_1 C_2 )^{1/2} \right ),
\end{equation}
where $\bm{\mu}_i$ and $C_i$ are the mean vector and the covariance matrix of the coding layer for each distribution.

\subsection{Theories on GAN Convergence}

\TODO{review:}

\cite{arjovsky2017principled}, \cite{kodali2017gan}, \cite{nagarajan2017local}, \cite{daskalakis2018gan}, \cite{mohamed2016implicit}, \cite{fedus2018equilibrium}, \cite{arora2017gan,arora2018gan}, \cite{mescheder2017gan,mescheder2018convergence}

\section{Summary}

\TODO{summarize}
