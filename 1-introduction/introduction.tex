%!TEX root = ../dissertation.tex
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------
% the code below specifies where the figures are stored
\graphicspath{{1/figures/}}

\chapter{Introduction}
\label{ch:introduction}

Automatic music transcription refers to an automated method that can identify all musical events in the input audio and convert them into musical notations.
The nature of music transcription is multifold; to create a complete transcription, one has to identify all instruments, beats, dynamics, and the pitch traces for every instrument present in the music, and it is still far from achieving the human-level accuracy despite decades of research in each of these subtasks.
The need for study arises naturally, not only because this is an intriguing problem in the interdiscipline of music and technology that has remained unsolved for decades, but also because the solution to this problem can provide practical benefits to many applications including, but not limited to, music recommender systems, music search engine, and compositional aid software.
The task of automatic music transcription shares many common values with other machine learning tasks, such as image segmentation, machine translation, and speech recognition, in a sense that the core task is to build an intelligent system that can extract and process semantics that are conveyed in complex signals.
This is the essence of artificial intelligence (AI)
--- a system that perceives its environment and takes actions that maximizes the utility \cite{russell2009ai} --- 
where the signals coming from the environment is complex multimedia data and understanding the semantics of them is a necessity to perform well.
Therefore, the problem of automatic music transcription is not just an intriguing task in music technology, but will also be a key component of the AI-enabled future society, constituting a musical brain of AI.


This thesis aims to design and develop improved methods for automatic music transcription with deeper computational understanding of musical semantics, leveraging the recent technological breakthroughs in the area of deep learning and big data.
The idea more specifically focuses on deep generative models and learning from training data generated by various sources including software instruments, so that machine learning algorithms can benefit from an effectively infinite source of labeled training data.
Another motivation for using generated audio data and generative models is the fact that synthesized music is more prevalent and perceptually more familiar to people than synthesized texts or pictures, suggesting that synthesized and generated audio will be able to more accurately model the distribution of real audio data to be transcribed.
This generative approach also aligns with how real musicians transcribe music, where they match their knowledge of how the instruments will sound when played in a certain combination of rhythms and melodies, with given audio.
Therefore it is reasonable to claim that machines should also be able to perform in a similar way, provided that a proper representation of knowledge about the music and instruments is available.
To validate this claim as a feasible research direction, this proposal provides a literature review covering the previous approaches for automatic music transcription and their limitations, as well as a survey of recent computational techniques that are considered to be essential in realizing the research goals.


One of the most exciting phenomena that have been happening in the field of machine learning during the past five years is the advent of deep learning and its tremendous success in computer vision, natural language processing, and many other fields.
The success was made possible by the unprecedented scale of high-performance computing resources that became available, which made intelligent systems capable of processing machine learning algorithms and data pipelines that were infeasible to be realized just until a few years ago.
At the same time, a number of new models and techniques have been devised to drastically improve the effectiveness and flexibility of existing algorithms.
Because it is expected that the techniques of deep learning will play the most important role in this study, this proposal starts with a general introduction to deep learning in Section \ref{sec:deeplearning}, which provides a survey of the essential building blocks of deep learning with an emphasis on deep generative models.


% Due to the difficulty and complexity of the problem, automatic music transcription has been tackled by researchers by dividing the problem into several separate subtasks, which include rhythmic, timbral, harmonic, and symbolic analyses, as well as audio source separation which aims to separate the individual tracks and perform transcription on each track.
Despite being not tremendously successful, there have been numerous research projects toward automatic music transcription as a whole or targeting a subtask of it, that stemmed from musicology, audio signal processing, and machine learning perspectives.
Section \ref{sec:review} provides a review of the standard methods and the current trends of automatic music transcription research.
Section \ref{sec:other} will present the overall pipeline that describes the specific methods for the proposed automatic music transcription research, which incorporates deep learning and music signal processing methods as well as other important components including data augmentation, software instruments, audio synthesis, and music language models.

Subsequently, some results of the preliminary experiments in this direction of research, including a monotonic pitch estimator based on time-domain convolutional neural network and a generative adversarial network trained on log-magnitude spectrograms of instrument note samples, are introduced in Section \ref{sec:prelim}.
These experiments are based on a few publicly and commercially available datasets including RWC Music Database \cite{goto2003rwc}, MedleyDB \cite{bittner2014medleydb}, and Vienna Symphonic Library of orchestral sounds as studied in \cite{humphrey2011nlse}.
In future studies, the NSynth Dataset published recently by Google's Magenta project \cite{engel2017nsynth} is also planned to be used, as the dataset contains additional kinds of instruments and comes with more accurate annotations.
Lastly, Section \ref{sec:summary} concludes the proposal by summarizing the core idea for the thesis.


\section{Scope of this Study}
\label{sec:scope}

Souffle chupa chups croissant donut. Muffin cotton candy cookie marzipan chupa chups. Jelly-o gummi bears topping caramels pudding. Marzipan applicake jujubes souffle sweet roll. Lemon drops dessert fruitcake carrot cake cotton candy lollipop tiramisu. Gummi bears oat cake bear claw liquorice tootsie roll jelly cookie. Lemon drops croissant applicake. Toffee applicake pie carrot cake. Wafer dragee souffle toffee. Powder tart apple pie pie sweet cotton candy sesame snaps.

\section{Motivation}

Icing toffee gummi bears bear claw caramels chocolate bar apple pie. Apple pie biscuit jelly jelly. Jelly beans tiramisu gingerbread gummi bears. Souffle topping bonbon chupa chups pie fruitcake. Souffle topping muffin jelly beans gummies liquorice tiramisu. Gummi bears tiramisu danish. Liquorice dessert chocolate powder macaroon gummies apple pie croissant. Topping jelly-o gingerbread unerdwear.com bonbon sugar plum candy canes. Croissant gummies cupcake gummi bears sesame snaps macaroon biscuit. Sweet roll liquorice apple pie sweet roll.

\section{Dissertation Outline}\label{sec:do}
\label{sec:outline}

\thesection, \thesubsection, \thechapter, \ref{sec:do}

\arabic{chapter}, \arabic{section}, \arabic{subsection}

\begin{description}
	
\item Chapter \ref{ch:review} Tiramisu wafer wafer icing fruitcake powder brownie macaroon dessert. 

\item Chapter \ref{ch:methods} concludes this thesis. Candy gingerbread chupa chups carrot cake danish. 
\end{description}

\section{Contributions}
The primary contributions of this dissertation are listed below:

\begin{itemize}
  \onehalfspacing
\item Carrot cake macaroon brownie chupa chups powder sesame snaps bear claw souffle biscuit. 
\item Sweet roll chocolate chocolate cake. 
\end{itemize}

\section{Associated Publications by the Author}

This thesis covers much of the work presented in the publications listed below:

\subsection{Peer-Reviewed Articles}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\vspace{1em}
\begin{itemize}
\onehalfspacing
\item Sugar plum jelly beans cookie tootsie roll jelly-o.
\item Tootsie roll sugar plum cotton candy pastry chocolate cake pudding oat cake gummi bears. 
\end{itemize}

\subsection{Peer-Reviewed Conference Papers}
\vspace{1em}
\begin{itemize}
\onehalfspacing
\item Cheesecake pudding marzipan gingerbread cheesecake oat cake applicake.
\item  Dragee marzipan unerdwear.com powder icing croissant pastry. 
\item  Dessert macaroon sweet roll macaroon wafer topping croissant. 

\end{itemize}

