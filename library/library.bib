Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{maron1961naive,
author = {Maron, M. E.},
doi = {10.1145/321075.321084},
file = {:C\:/Users/jongwook/Dropbox/References/Automatic Indexing An Experimental Inquiry.pdf:pdf},
isbn = {0004-5411},
issn = {00045411},
journal = {Journal of the ACM},
number = {3},
pages = {404--417},
title = {{Automatic Indexing: An Experimental Inquiry}},
url = {http://portal.acm.org/citation.cfm?doid=321075.321084},
volume = {8},
year = {1961}
}
@article{goto2003rwc,
author = {Goto, Masataka and Hashiguchi, Hiroki and Nishimura, Takuichi and Oka, Ryuichi},
file = {:C\:/Users/jongwook/Dropbox/References/RWC Music Database Music Genre Database and Musical Instrument Sound Database.pdf:pdf},
journal = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
keywords = {Dataset},
mendeley-tags = {Dataset},
publisher = {Johns Hopkins University},
title = {{RWC Music Database: Music Genre Database and Musical Instrument Sound Database}},
year = {2003}
}
@article{mescheder2017adversarial,
abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
archivePrefix = {arXiv},
arxivId = {1701.04722},
author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
eprint = {1701.04722},
file = {:C\:/Users/jongwook/Dropbox/References/Adversarial Variational Bayes Unifying Variational Autoencoders and Generative Adversarial Networks.pdf:pdf},
issn = {1938-7228},
journal = {arXiv preprint arXiv:1701.04722},
keywords = {GAN,VAE},
mendeley-tags = {GAN,VAE},
title = {{Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1701.04722},
year = {2017}
}
@article{hinton2005cd,
author = {Carreira-Perpi{\~{n}}{\'{a}}n, Miguel A and Hinton, Geoffrey E.},
doi = {10.3389/conf.neuro.10.2009.14.121},
file = {:C\:/Users/jongwook/Dropbox/References/On Contrastive Divergence Learning.pdf:pdf},
isbn = {0818681322},
issn = {16625188},
journal = {Artificial Intelligence and Statistics},
pages = {17},
title = {{On Contrastive Divergence Learning}},
year = {2005}
}
@article{doersch2016tutorial,
author = {Doersch, Carl},
file = {:C\:/Users/jongwook/Dropbox/References/Tutorial on Variational Autoencoders.pdf:pdf},
journal = {arXiv preprint arXiv:1606.05908},
keywords = {VAE},
mendeley-tags = {VAE},
title = {{Tutorial on Variational Autoencoders}},
year = {2016}
}
@inproceedings{oord2016pixel,
abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
archivePrefix = {arXiv},
arxivId = {1601.06759},
author = {van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1601.06759},
file = {:C\:/Users/jongwook/Dropbox/References/Pixel Recurrent Neural Networks.pdf:pdf},
isbn = {9781510829008},
title = {{Pixel Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1601.06759},
year = {2016}
}
@book{cook2002synthesis,
author = {Cook, Perry R},
file = {:C\:/Users/jongwook/Dropbox/References/Real Sound Synthesis for Interactive Applications.pdf:pdf},
keywords = {Synthesis},
mendeley-tags = {Synthesis},
publisher = {CRC Press},
title = {{Real Sound Synthesis for Interactive Applications}},
year = {2002}
}
@inproceedings{ping2017deepvoice3,
archivePrefix = {arXiv},
arxivId = {1710.07654},
author = {Ping, Wei and Peng, Kainan and Gibiansky, Andrew and Arik, Sercan O. and Kannan, Ajay and Narang, Sharan and Raiman, Jonathan and Miller, John},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1710.07654},
file = {:C\:/Users/jongwook/Dropbox/References/Deep Voice 3 2000-Speaker Neural Text-to-Speech.pdf:pdf},
title = {{Deep Voice 3: 2000-Speaker Neural Text-to-Speech}},
url = {http://arxiv.org/abs/1710.07654},
year = {2017}
}
@article{silver2016alphago,
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Others},
file = {:C\:/Users/jongwook/Dropbox/References/Mastering the Game of Go with Deep Neural Networks and Tree Search.pdf:pdf},
journal = {Nature},
keywords = {RL},
mendeley-tags = {RL},
number = {7587},
pages = {484--489},
publisher = {Nature Publishing Group},
title = {{Mastering the Game of Go with Deep Neural Networks and Tree Search}},
volume = {529},
year = {2016}
}
@inproceedings{bittner2015contour,
author = {Bittner, Rachel M and Salamon, Justin and Essid, Slim and Bello, Juan P},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Melody Extraction by Contour Classification.pdf:pdf},
isbn = {978-84-606-8853-2},
pages = {500--506},
title = {{Melody Extraction by Contour Classification}},
year = {2015}
}
@inproceedings{perraudin2013griffinlim,
author = {Perraudin, Nathana{\"{e}}l and Balazs, Peter and S{\o}ndergaard, Peter L},
booktitle = {Proceedings of the {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics {(WASPAA)}},
file = {:C\:/Users/jongwook/Dropbox/References/A Fast Griffin-Lim Algorithm.pdf:pdf},
isbn = {9781479909728},
title = {{A Fast Griffin-Lim Algorithm}},
year = {2013}
}
@article{larochelle2011nade,
abstract = {We describe a new approach for modeling the distribution of high-dimensional vectors of discrete variables. This model is inspired by the restricted Boltzmann machine (RBM), which has been shown to be a powerful model of such distributions. However, an RBM typically does not provide a tractable distribution estimator, since evaluating the probability it assigns to some given observation requires the computation of the so-called partition function, which itself is intractable for RBMs of even moderate size. Our model circumvents this difficulty by decomposing the joint distribution of observations into tractable conditional distributions and modeling each conditional using a non-linear function similar to a conditional of an RBM. Our model can also be interpreted as an autoencoder wired such that its output can be used to assign valid probabilities to observations. We show that this new model outperforms other multivariate binary distribution estimators on several datasets and performs similarly to a large (but intractable) RBM.},
author = {Larochelle, Hugo and Murray, Iain},
file = {:C\:/Users/jongwook/Dropbox/References/The Neural Autoregressive Distribution Estimator.pdf:pdf},
issn = {15324435},
journal = {Proceedings of the International Conference on Machine Learning {(ICML)}},
pages = {29--37},
title = {{The Neural Autoregressive Distribution Estimator}},
volume = {15},
year = {2011}
}
@misc{lerdahl1983gttm,
author = {Lerdahl, Fred and Jackendoff, Ray},
file = {:C\:/Users/jongwook/Dropbox/References/Generative Theory of Tonal Music.pdf:pdf},
publisher = {MIT Press},
title = {{Generative Theory of Tonal Music}},
year = {1983}
}
@inproceedings{cogliati2017metric,
abstract = {Automatic music transcription aims at transcribing musical performances into music notation. However, most existing transcription systems only focus on parametric transcrip-tion, i.e., they output a symbolic representation in absolute terms, showing frequency and absolute time (e.g., a piano-roll representation), but not in musical terms, with spelling distinctions (e.g., A versus G) and quantized meter. Re-cent attempts at producing full music notation output have been hindered by the lack of an objective metric to mea-sure the adherence of the results to the ground truth mu-sic score, and had to rely on time-consuming human eval-uation by music theorists. In this paper, we propose an edit distance, similar to the Levenshtein Distance used for measuring the difference between two sequences, typically strings of characters. The metric treats a music score as a sequence of sets of musical objects, ordered by their on-sets. The metric reports the differences between two music scores based on twelve aspects: barlines, clefs, key signa-tures, time signatures, notes, note spelling, note durations, stem directions, groupings, rests, rest duration, and staff assignment. We also apply a linear regression model to the metric in order to predict human evaluations on a dataset of short music excerpts automatically transcribed into music notation.},
author = {Cogliati, Andrea and Duan, Zhiyao},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/A Metric for Music Notation Transcription Accuracy.pdf:pdf},
keywords = {Metric},
mendeley-tags = {Metric},
pages = {407--413},
title = {{A Metric for Music Notation Transcription Accuracy}},
year = {2017}
}
@article{engel2017nsynth,
abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
archivePrefix = {arXiv},
arxivId = {1704.01279},
author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
eprint = {1704.01279},
file = {:C\:/Users/jongwook/Dropbox/References/Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders.pdf:pdf},
journal = {arXiv preprint arXiv:1704.01279},
keywords = {Synthesis},
mendeley-tags = {Synthesis},
title = {{Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders}},
url = {http://arxiv.org/abs/1704.01279},
year = {2017}
}
@article{antoniou2017dagan,
abstract = {Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation \cite{krizhevsky2012imagenet} alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13\% increase in accuracy in the low-data regime experiments in Omniglot (from 69\% to 82\%), EMNIST (73.9\% to 76\%) and VGG-Face (4.5\% to 12\%); in Matching Networks for Omniglot we observe an increase of 0.5\% (from 96.9\% to 97.4\%) and an increase of 1.8\% in EMNIST (from 59.5\% to 61.3\%).},
archivePrefix = {arXiv},
arxivId = {1711.04340},
author = {Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
eprint = {1711.04340},
file = {:C\:/Users/jongwook/Dropbox/References/Data Augmentation Generative Adversarial Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1711.04340},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {1--13},
title = {{Data Augmentation Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1711.04340},
year = {2017}
}
@article{lee2017introspective,
abstract = {We present Wasserstein introspective neural networks (WINN) that are both a generator and a discriminator within a single model. WINN provides a significant improvement over the recent introspective neural networks (INN) method by enhancing INN's generative modeling capability. WINN has three interesting properties: (1) A mathematical connection between the formulation of Wasserstein generative adversarial networks (WGAN) and the INN algorithm is made; (2) The explicit adoption of the Wasserstein distance into INN results in a large enhancement to INN, achieving compelling results even with a single classifier --- e.g., providing a 20 times reduction in model size over INN within texture modeling; (3) When applied to supervised classification, WINN also gives rise to greater robustness with an $88\%$ reduction of errors against adversarial examples --- improved over the result of $39\%$ by an INN-family algorithm. In the experiments, we report encouraging results on unsupervised learning problems including texture, face, and object modeling, as well as a supervised classification task against adversarial attack.},
archivePrefix = {arXiv},
arxivId = {1711.08875},
author = {Lee, Kwonjoon and Xu, Weijian and Fan, Fan and Tu, Zhuowen},
eprint = {1711.08875},
file = {:C\:/Users/jongwook/Dropbox/References/Wasserstein Introspective Neural Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1711.08875},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Wasserstein Introspective Neural Networks}},
url = {http://arxiv.org/abs/1711.08875},
year = {2017}
}
@inproceedings{lim2017chord,
author = {Lim, Hyungui and Rhyu, Seungyeon and Lee, Kyogu},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Chord Generation From Symbolic Melody Using BLSTM Networks.pdf:pdf},
keywords = {LSTM,Symbolic},
mendeley-tags = {LSTM,Symbolic},
pages = {621--627},
title = {{Chord Generation From Symbolic Melody Using BLSTM Networks}},
year = {2017}
}
@article{achille2018information,
archivePrefix = {arXiv},
arxivId = {1611.01353},
author = {Achille, Alessandro and Soatto, Stefano},
doi = {10.1109/TPAMI.2017.2784440},
eprint = {1611.01353},
file = {:C\:/Users/jongwook/Dropbox/References/Information Dropout Learning Optimal Representations Through Noisy Computation.pdf:pdf},
issn = {0162-8828},
journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
title = {{Information Dropout: Learning Optimal Representations Through Noisy Computation}},
url = {http://arxiv.org/abs/1611.01353},
year = {2018}
}
@inproceedings{salimans2016improved,
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
file = {:C\:/Users/jongwook/Dropbox/References/Improved Techniques for Training GANs.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {2226--2234},
title = {{Improved Techniques for Training GANs}},
year = {2016}
}
@inproceedings{humphrey2012rethinking,
author = {Humphrey, Eric J. and Bello, Juan P.},
booktitle = {Proceedings of the International Conference on Machine Learning and Applications and Workshops {(ICMLA)}},
doi = {10.1109/ICMLA.2012.220},
file = {:C\:/Users/jongwook/Dropbox/References/Rethinking Automatic Chord Recognition with Convolutional Neural Networks.pdf:pdf},
isbn = {9780769549132},
keywords = {automatic music transcription,chord recognition,convolutional neural nets},
pages = {357--362},
title = {{Rethinking Automatic Chord Recognition with Convolutional Neural Networks}},
volume = {2},
year = {2012}
}
@article{fernandez2013ai,
abstract = {Algorithmic Composition;Survey;},
author = {Fern{\'{a}}ndez, Jose D and Vico, Francisco},
file = {:C\:/Users/jongwook/Dropbox/References/AI Methods in Algorithmic Composition A Comprehensive Survey.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {513--582},
title = {{AI Methods in Algorithmic Composition: A Comprehensive Survey}},
volume = {48},
year = {2013}
}
@book{sutton2018reinforcement,
author = {Sutton, Richard S and Barto, Andrew G},
file = {:C\:/Users/jongwook/Dropbox/References/Reinforcement Learning An Introduction (2nd Edition).pdf:pdf},
keywords = {RL},
mendeley-tags = {RL},
publisher = {MIT press Cambridge},
title = {{Reinforcement Learning: An Introduction (2nd Edition)}},
year = {2018}
}
@inproceedings{larsen2015vaegan,
abstract = {We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.},
archivePrefix = {arXiv},
arxivId = {1512.09300},
author = {Larsen, Anders Boesen Lindbo and S{\o}nderby, S{\o}ren Kaae and Larochelle, Hugo and Winther, Ole},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1512.09300},
file = {:C\:/Users/jongwook/Dropbox/References/Autoencoding Beyond Pixels using a Learned Similarity Metric.pdf:pdf},
isbn = {9781510829008},
keywords = {GAN,VAE},
mendeley-tags = {GAN,VAE},
title = {{Autoencoding Beyond Pixels using a Learned Similarity Metric}},
url = {http://arxiv.org/abs/1512.09300},
year = {2016}
}
@article{zhang2017stackgan2,
abstract = {Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) aimed at generating high-resolution photorealistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches primitive shape and colors of the object based on given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and discriminators in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.},
archivePrefix = {arXiv},
arxivId = {1710.10916},
author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
doi = {10.1109/ICCV.2017.629},
eprint = {1710.10916},
file = {:C\:/Users/jongwook/Dropbox/References/StackGAN Realistic Image Synthesis with Stacked Generative Adversarial Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1710.10916},
keywords = {GAN},
mendeley-tags = {GAN},
pmid = {202927},
title = {{StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1710.10916},
year = {2017}
}
@article{samuel1959ml,
author = {Samuel, Arthur L},
doi = {10.1147/rd.33.0210},
file = {:C\:/Users/jongwook/Dropbox/References/Some Studies in Machine Learning Using the Game of Checkers.pdf:pdf},
issn = {0018-8646},
journal = {IBM Journal of Research and Development},
month = {jul},
number = {3},
pages = {210--229},
title = {{Some Studies in Machine Learning Using the Game of Checkers}},
volume = {3},
year = {1959}
}
@inproceedings{nowozin2016fgan,
archivePrefix = {arXiv},
arxivId = {arXiv:1606.00709},
author = {Nowozin, Sebastian and Cseke, Botond},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {arXiv:1606.00709},
file = {:C\:/Users/jongwook/Dropbox/References/f-GAN Training Generative Neural Samplers using Variational Divergence Minimization.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization}},
year = {2016}
}
@article{donahue2017segan,
abstract = {We investigate the effectiveness of generative adversarial networks (GANs) for speech enhancement, in the context of improving noise robustness of automatic speech recognition (ASR) systems. Prior work demonstrates that GANs can effectively suppress additive noise in raw waveform speech signals, improving perceptual quality metrics; however this technique was not justified in the context of ASR. In this work, we conduct a detailed study to measure the effectiveness of GANs in enhancing speech contaminated by both additive and reverberant noise. Motivated by recent advances in image processing, we propose operating GANs on log-Mel filterbank spectra instead of waveforms, which requires less computation and is more robust to reverberant noise. While GAN enhancement improves the performance of a clean-trained ASR system on noisy speech, it falls short of the performance achieved by conventional multi-style training (MTR). By appending the GAN-enhanced features to the noisy inputs and retraining, we achieve a 7% WER improvement relative to the MTR system.},
archivePrefix = {arXiv},
arxivId = {1711.05747},
author = {Donahue, Chris and Li, Bo and Prabhavalkar, Rohit},
eprint = {1711.05747},
file = {:C\:/Users/jongwook/Dropbox/References/Exploring Speech Enhancement with Generative Adversarial Networks for Robust Speech Recognition.pdf:pdf},
journal = {arXiv preprint arXiv:1711.05747},
keywords = {GAN,Speech},
mendeley-tags = {GAN,Speech},
title = {{Exploring Speech Enhancement with Generative Adversarial Networks for Robust Speech Recognition}},
url = {http://arxiv.org/abs/1711.05747},
year = {2017}
}
@inproceedings{salamon2017analysis,
author = {Salamon, Justin and Bittner, Rachel M. and Bonada, Jordi and Bosch, Juan J. and Gomez, Emilia and juan pablo Bello},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/An AnalysisSynthesis Framework for Automatic F0 Annotation of Multitrack Datasets.pdf:pdf},
keywords = {Synthesis},
mendeley-tags = {Synthesis},
pages = {71--78},
title = {{An Analysis/Synthesis Framework for Automatic F0 Annotation of Multitrack Datasets}},
year = {2017}
}
@inproceedings{lee2017gp,
abstract = {A deep fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP) in the limit of infinite network width. This correspondence enables exact Bayesian inference for neural networks on regression tasks by means of straightforward matrix computations. For single hidden-layer networks, the covariance function of this GP has long been known. Recently, kernel functions for multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified the correspondence between using these kernels as the covariance function for a GP and performing fully Bayesian prediction with a deep neural network. In this work, we derive this correspondence and develop a computationally efficient pipeline to compute the covariance functions. We then use the resulting GP to perform Bayesian inference for deep neural networks on MNIST and CIFAR-10. We find that the GP-based predictions are competitive and can outperform neural networks trained with stochastic gradient descent. We observe that the trained neural network accuracy approaches that of the corresponding GP-based computation with increasing layer width, and that the GP uncertainty is strongly correlated with prediction error. We connect our observations to the recent development of signal propagation in random neural networks.},
archivePrefix = {arXiv},
arxivId = {1711.00165},
author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1711.00165},
file = {:C\:/Users/jongwook/Dropbox/References/Deep Neural Networks as Gaussian Processes.pdf:pdf},
pages = {1--14},
title = {{Deep Neural Networks as Gaussian Processes}},
url = {http://arxiv.org/abs/1711.00165},
year = {2018}
}
@article{sturm2013classification,
author = {Sturm, Bob L.},
doi = {10.1007/s10844-013-0250-y},
file = {:C\:/Users/jongwook/Dropbox/References/Classification Accuracy is Not Enough.pdf:pdf},
issn = {09259902},
journal = {Journal of Intelligent Information Systems},
keywords = {Classification,Evaluation,Genre,Music},
number = {3},
pages = {371--406},
title = {{Classification Accuracy is Not Enough: On the Evaluation of Music Genre Recognition Systems}},
volume = {41},
year = {2013}
}
@inproceedings{daskalakis2018gan,
abstract = {We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous no-regret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.},
archivePrefix = {arXiv},
arxivId = {1711.00141},
author = {Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1711.00141},
file = {:C\:/Users/jongwook/Dropbox/References/Training GANs with Optimism.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Training GANs with Optimism}},
url = {http://arxiv.org/abs/1711.00141},
year = {2018}
}
@article{cemgil2006generative,
author = {Cemgil, Ali Taylan and Kappen, Hilbert J and Barber, David},
file = {:C\:/Users/jongwook/Dropbox/References/A Generative Model for Music Transcription.pdf:pdf},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Transcription},
mendeley-tags = {Transcription},
number = {2},
pages = {679--694},
publisher = {IEEE},
title = {{A Generative Model for Music Transcription}},
volume = {14},
year = {2006}
}
@inproceedings{downie2014mirex,
author = {Downie, J. Stephen and Hu, Xiao and Lee, Jin Ha and Choi, Kahyun and Cunningham, Sally Jo and Hao, Yun},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Ten Years of MIREX Reflections, Challenges, and Opportunities.pdf:pdf},
keywords = {MIR,MIREX},
mendeley-tags = {MIR,MIREX},
number = {Ismir},
organization = {ISMIR},
pages = {657--662},
title = {{Ten Years of MIREX: Reflections, Challenges, and Opportunities}},
year = {2014}
}
@article{kalchbrenner2018audio,
archivePrefix = {arXiv},
arxivId = {1802.08435},
author = {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and {Van Den Oord}, A{\"{a}}ron and Dieleman, Sander and Kavukcuoglu, Koray},
eprint = {1802.08435},
file = {:C\:/Users/jongwook/Dropbox/References/Efficient Neural Audio Synthesis.pdf:pdf},
journal = {arXiv preprint arXiv:1802.08435},
title = {{Efficient Neural Audio Synthesis}},
year = {2018}
}
@article{bengio2013representation,
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
file = {:C\:/Users/jongwook/Dropbox/References/Representation Learning A Review and New Perspectives.pdf:pdf},
journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Survey},
mendeley-tags = {Survey},
number = {8},
pages = {1798--1828},
publisher = {IEEE},
title = {{Representation Learning: A Review and New Perspectives}},
volume = {35},
year = {2013}
}
@inproceedings{lee2001nmf,
archivePrefix = {arXiv},
arxivId = {arXiv:cs/0408058v1},
author = {Lee, Daniel D. and Seung, H. Sebastian},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {10.1109/IJCNN.2008.4634046},
eprint = {0408058v1},
file = {:C\:/Users/jongwook/Dropbox/References/Algorithms for Non-Negative Matrix Factorization.pdf:pdf},
isbn = {9781424418206},
issn = {10987576},
pages = {556--562},
pmid = {10548103},
primaryClass = {arXiv:cs},
title = {{Algorithms for Non-Negative Matrix Factorization}},
url = {http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization},
year = {2001}
}
@article{stoller2017separation,
abstract = {The state of the art in music source separation employs neural networks trained in a supervised fashion on multi-track databases to estimate the sources from a given mixture. With only few datasets available, often extensive data augmentation is used to combat overfitting. Mixing random tracks, however, can even reduce separation performance as instruments in real music are strongly correlated. The key concept in our approach is that source estimates of an optimal separator should be indistinguishable from real source signals. Based on this idea, we drive the separator towards outputs deemed as realistic by discriminator networks that are trained to tell apart real from separator samples. This way, we can also use unpaired source and mixture recordings without the drawbacks of creating unrealistic music mixtures. Our framework is widely applicable as it does not assume a specific network architecture or number of sources. To our knowledge, this is the first adoption of adversarial training for music source separation. In a prototype experiment for singing voice separation, separation performance increases with our approach compared to purely supervised training.},
archivePrefix = {arXiv},
arxivId = {1711.00048},
author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
eprint = {1711.00048},
file = {:C\:/Users/jongwook/Dropbox/References/Adversarial Semi-Supervised Audio Source Separation applied to Singing Voice Extraction.pdf:pdf},
journal = {arXiv preprint arXiv:1711.00048},
keywords = {GAN,Source Separation},
mendeley-tags = {GAN,Source Separation},
title = {{Adversarial Semi-Supervised Audio Source Separation applied to Singing Voice Extraction}},
url = {http://arxiv.org/abs/1711.00048},
year = {2017}
}
@book{rowe2003musicianship,
author = {Rowe, Robert},
doi = {Book Review},
file = {:C\:/Users/jongwook/Dropbox/References/Machine Musicianship.pdf:pdf},
isbn = {9780262681490},
issn = {09298215},
pmid = {3862},
title = {{Machine Musicianship}},
year = {2003}
}
@inproceedings{dubois2005harmonic,
author = {Dubois, Corentin and Davy, Manuel},
booktitle = {{IEEE/SP} Workshop on Statistical Signal Processing},
file = {:C\:/Users/jongwook/Dropbox/References/Harmonic Tracking Using Sequential Monte Carlo.pdf:pdf},
organization = {IEEE},
pages = {1292--1297},
title = {{Harmonic Tracking Using Sequential Monte Carlo}},
year = {2005}
}
@article{sigtia2016endtoend,
abstract = {{We present a neural network model for polyphonic music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language mode}. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony or the number or type of instruments. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We investigate various neural network architectures for the acoustic models and compare their performance to two popular state-of-the-art acoustic models. We also present an efficient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications. We evaluate the model's performance on the MAPS dataset and show that the proposed model outperforms state-of-the-art transcription systems.},
archivePrefix = {arXiv},
arxivId = {1508.01774},
author = {Sigtia, Siddharth and Benetos, Emmanouil and Dixon, Simon},
doi = {10.1109/TASLP.2016.2533858},
eprint = {1508.01774},
file = {:C\:/Users/jongwook/Dropbox/References/An End-to-End Neural Network for Polyphonic Piano Music Transcription.pdf:pdf},
issn = {2329-9290},
journal = {{IEEE/ACM} Transactions on Audio, Speech, and Language Processing},
keywords = {End-to-End,Transcription},
mendeley-tags = {End-to-End,Transcription},
number = {5},
pages = {927--939},
publisher = {IEEE Press},
title = {{An End-to-End Neural Network for Polyphonic Piano Music Transcription}},
url = {http://arxiv.org/abs/1508.01774},
volume = {24},
year = {2016}
}
@inproceedings{unterthiner2017coulomb,
abstract = {Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on a variety of image datasets. On LSUN and celebA, Coulomb GANs set a new state of the art and produce a previously unseen variety of different samples.},
archivePrefix = {arXiv},
arxivId = {1708.08819},
author = {Unterthiner, Thomas and Nessler, Bernhard and Seward, Calvin and Klambauer, G{\"{u}}nter and Heusel, Martin and Ramsauer, Hubert and Hochreiter, Sepp},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1708.08819},
file = {:C\:/Users/jongwook/Dropbox/References/Coulomb GANs Provably Optimal Nash Equilibria via Potential Fields.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields}},
url = {http://arxiv.org/abs/1708.08819},
year = {2017}
}
@book{chomsky1966generative,
author = {Chomsky, Noam},
file = {:C\:/Users/jongwook/Dropbox/References/Topics in the Theory of Generative Grammar.pdf:pdf},
publisher = {Mouton},
title = {{Topics in the Theory of Generative Grammar}},
year = {1966}
}
@inproceedings{rifai2011contractive,
author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
file = {:C\:/Users/jongwook/Dropbox/References/Contractive Auto-Encoders Explicit Invariance During Feature Extraction.pdf:pdf},
keywords = {Auto-Encoders},
mendeley-tags = {Auto-Encoders},
pages = {833--840},
title = {{Contractive Auto-Encoders: Explicit Invariance During Feature Extraction}},
year = {2011}
}
@article{burda2016vae,
archivePrefix = {arXiv},
arxivId = {1509.00519},
author = {Burda, Yuri and Crosse, Roger and Salakhutdinov, Ruslan},
eprint = {1509.00519},
file = {:C\:/Users/jongwook/Dropbox/References/Importance weighted autoencoders.pdf:pdf},
journal = {arXiv preprint arXiv:1509.00519},
keywords = {VAE},
mendeley-tags = {VAE},
title = {{Importance weighted autoencoders}},
url = {https://arxiv.org/abs/1509.00519},
year = {2016}
}
@inproceedings{sigtia2014lm,
abstract = {In this paper, we investigate the use of Music Language Models (MLMs) for improving Automatic Music Transcription performance. The MLMs are trained on sequences of symbolic polyphonic music from the Nottingham dataset. We train Recurrent Neural Network (RNN)-based models, as they are capable of capturing complex temporal structure present in symbolic music data. Similar to the function of language models in automatic speech recognition, we use the MLMs to generate a prior probability for the occurrence of a sequence. The acoustic AMT model is based on probabilistic latent component analysis, and prior information from the MLM is incorporated into the transcription framework using Dirichlet priors. We test our hybrid models on a dataset of multiple-instrument polyphonic music and report a significant 3% improvement in terms of F-measure, when compared to using an acoustic-only model.},
author = {Sigtia, Siddharth and Benetos, Emmanouil and Cherla, Srikanth and Weyde, Tillman and d'Avila Garcez, a. and Dixon, Simon},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/An RNN-based Music Language Model for Improving Automatic Music Transcription.pdf:pdf},
keywords = {M Music and Books on Music,QA75 Electronic computers. Computer science},
number = {Ismir},
pages = {53--58},
title = {{An RNN-based Music Language Model for Improving Automatic Music Transcription}},
url = {http://openaccess.city.ac.uk/4529/8/ISMIR2014.pdf},
year = {2014}
}
@inproceedings{ness2009tag,
author = {Ness, Steven R and Theocharis, Anthony and Tzanetakis, George and Martins, Luis Gustavo},
booktitle = {Proceedings of the {ACM} International Conference on Multimedia},
doi = {10.1145/1631272.1631393},
file = {:C\:/Users/jongwook/Dropbox/References/Improving Automatic Music Tag Annotation using Stacked Generalization of Probabilistic SVM Outputs.pdf:pdf},
isbn = {9781605586083},
keywords = {folksonomies,music information retrieval,music recommendation,sound analysis,tags},
pages = {705--708},
title = {{Improving Automatic Music Tag Annotation using Stacked Generalization of Probabilistic SVM Outputs}},
url = {http://dl.acm.org/citation.cfm?id=1631393%5Cnhttp://portal.acm.org/citation.cfm?doid=1631272.1631393},
year = {2009}
}
@inproceedings{shi2017pianorolls,
abstract = {Reproducing piano rolls are among the early music storage mediums, preserving fine details of a piano or organ per- formance on a continuous roll of paper with holes punched onto them. While early acoustic recordings suffer from poor quality sound, reproducing piano rolls benefit from the fidelity of a live piano for playback, and capture all features of a performance in what amounts to an early dig- ital data format. However, due to limited availability of well maintained playback instruments and the condition of fragile paper, rolls have remained elusive and generally inaccessible for study. This paper proposes methods for modeling and digitizing reproducing piano rolls. Starting with an optical scan, we convert the rawimage data into the MIDI file format by applying histogram-based image pro- cessing and building computational models of the musical expressions encoded on the rolls. Our evaluations show that MIDI emulations from our computational models are accurate on note level and proximate the musical expres- sions when compared with original playback recordings.},
author = {Shi, Zhengshan and Arul, Kumaran and Smith, Julius O},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Modeling and Digitizing Reproducing Piano Rolls.pdf:pdf},
keywords = {Symbolic},
mendeley-tags = {Symbolic},
pages = {197--203},
title = {{Modeling and Digitizing Reproducing Piano Rolls}},
year = {2017}
}
@article{linnainmaa1970ad,
author = {Linnainmaa, Seppo},
journal = {Master's Thesis (in Finnish), University of Helsinki},
pages = {6--7},
title = {{The Representation of the Cumulative Rounding Error of an Algorithm as a Taylor Expansion of the Local Rounding Errors}},
year = {1970}
}
@inproceedings{sukhbaatar2015memory,
archivePrefix = {arXiv},
arxivId = {1503.08895},
author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {v5},
eprint = {1503.08895},
file = {:C\:/Users/jongwook/Dropbox/References/End-To-End Memory Networks.pdf:pdf},
isbn = {1551-6709},
issn = {10495258},
pmid = {9377276},
title = {{End-To-End Memory Networks}},
url = {http://arxiv.org/abs/1503.08895},
year = {2015}
}
@inproceedings{hansen2016codebook,
author = {Hansen, Martin Weiss and Jensen, Jesper Rindom and Christensen, Mads Graesboll},
booktitle = {Proceedings of the European Signal Processing Conference {(EUSIPCO)}},
file = {:C\:/Users/jongwook/Dropbox/References/Estimation of Multiple Pitches in Stereophonic Mixtures Using a Codebook-Based Approach.pdf:pdf},
isbn = {9780992862657},
keywords = {Multi-F0},
mendeley-tags = {Multi-F0},
pages = {983--987},
title = {{Estimation of Multiple Pitches in Stereophonic Mixtures Using a Codebook-Based Approach}},
year = {2016}
}
@inproceedings{fedus2018equilibrium,
abstract = {Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players' parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.},
archivePrefix = {arXiv},
arxivId = {1710.08446},
author = {Fedus, William and Rosca, Mihaela and Lakshminarayanan, Balaji and Dai, Andrew M. and Mohamed, Shakir and Goodfellow, Ian},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1710.08446},
file = {:C\:/Users/jongwook/Dropbox/References/Many Paths to Equilibrium GANs Do Not Need to Decrease a Divergence At Every Step.pdf:pdf},
title = {{Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step}},
url = {http://arxiv.org/abs/1710.08446},
year = {2018}
}
@inproceedings{nair2010relu,
author = {Nair, Vinod and Hinton, Geoffrey E},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
file = {:C\:/Users/jongwook/Dropbox/References/Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:pdf},
keywords = {Activations},
mendeley-tags = {Activations},
pages = {807--814},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{thickstun2017invariances,
archivePrefix = {arXiv},
arxivId = {1711.04845},
author = {Thickstun, John and Harchaoui, Zaid and Foster, Dean and Kakade, Sham M.},
eprint = {1711.04845},
file = {:C\:/Users/jongwook/Dropbox/References/Invariances and Data Augmentation for Supervised Music Transcription.pdf:pdf},
journal = {arXiv preprint arXiv:1711.04845},
title = {{Invariances and Data Augmentation for Supervised Music Transcription}},
url = {http://arxiv.org/abs/1711.04845},
year = {2017}
}
@inproceedings{vogl2017drum,
abstract = {Existing systems for automatic transcription of drum tracks from polyphonic music focus on detecting drum in-strument onsets but lack consideration of additional meta information like bar boundaries, tempo, and meter. We ad-dress this limitation by proposing a system which has the capability to detect drum instrument onsets along with the corresponding beats and downbeats. In this design, the sys-tem has the means to utilize information on the rhythmical structure of a song which is closely related to the desired drum transcript. To this end, we introduce and compare different architectures for this task, i.e., recurrent, convo-lutional, and recurrent-convolutional neural networks. We evaluate our systems on two well-known data sets and an additional new data set containing both drum and beat annotations. We show that convolutional and recurrent-convolutional neural networks perform better than state-of-the-art methods and that learning beats jointly with drums can be beneficial for the task of drum detection.},
author = {Vogl, Richard and Dorfer, Matthias and Widmer, Gerhard and Knees, Peter},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Drum Transcription via Joint Beat and Drum Modeling using Convolutional Recurrent Neural Networks.pdf:pdf},
pages = {150--157},
title = {{Drum Transcription via Joint Beat and Drum Modeling using Convolutional Recurrent Neural Networks}},
year = {2017}
}
@article{maaten2008tsne,
author = {van der Maaten, Laurens and Hinton, Geoffrey},
file = {:C\:/Users/jongwook/Dropbox/References/Visualizing Data using t-SNE.pdf:pdf},
journal = {Journal of Machine Learning Research},
number = {2008},
pages = {2579--2605},
title = {{Visualizing Data using t-SNE}},
volume = {9},
year = {2008}
}
@inproceedings{johnson2016loss,
abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks us- ing a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features ex- tracted from pretrained networks. We combine the benefits of both ap- proaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Com- pared to the optimization-based method, our network gives similar quali- tative results but is three orders of magnitude faster.We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results. Keywords:},
author = {Johnson, Justin and Alahi, Alexandre and Fei-fei, Li},
booktitle = {Proceedings of the European Conference on Computer Vision},
file = {:C\:/Users/jongwook/Dropbox/References/Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf:pdf},
keywords = {Style Transfer,Super-Resolution,deep learning,style transfer,super-resolution},
mendeley-tags = {Style Transfer,Super-Resolution},
title = {{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}},
year = {2016}
}
@inproceedings{bosch2014melody,
author = {Bosch, Juan J. and G{\'{o}}mez, Emilia},
booktitle = {Proceedings of the Conference on Interdisciplinary Musicology},
file = {:C\:/Users/jongwook/Dropbox/References/Melody Extraction in Symphonic Classical Music a Comprehensive Study of Mutual Agreement Between Humans and Algorithms.pdf:pdf},
title = {{Melody Extraction in Symphonic Classical Music: a Comparative Study of Mutual Agreement Between Humans and Algorithms}},
year = {2014}
}
@article{noll1967cepstrum,
author = {Noll, A Michael},
doi = {10.1121/1.1910339},
file = {:C\:/Users/jongwook/Dropbox/References/Cepstrum Pitch Determination.pdf:pdf},
isbn = {0001-4966 (Print) 0001-4966 (Linking)},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {2},
pages = {293--309},
pmid = {6040805},
title = {{Cepstrum Pitch Determination}},
url = {http://scitation.aip.org/content/asa/journal/jasa/41/2/10.1121/1.1910339},
volume = {41},
year = {1967}
}
@book{everitt1981mixture,
author = {Everitt, B. S. and Hand, D. J.},
file = {:C\:/Users/jongwook/Dropbox/References/Finite Mixture Distributions.pdf:pdf},
publisher = {Chapman and Hall},
title = {{Finite Mixture Distributions}},
year = {1981}
}
@article{ulyanov2017age,
abstract = {We present a new autoencoder-type architecture that is trainable in an unsupervised mode, sustains both generation and inference, and has the quality of conditional and unconditional samples boosted by adversarial learning. Unlike previous hybrids of autoencoders and adversarial networks, the adversarial game in our approach is set up directly between the encoder and the generator, and no external mappings are trained in the process of learning. The game objective compares the divergences of each of the real and the generated data distributions with the prior distribution in the latent space. We show that direct generator-vs-encoder game leads to a tight coupling of the two components, resulting in samples and reconstructions of a comparable quality to some recently-proposed more complex architectures.},
archivePrefix = {arXiv},
arxivId = {1704.02304},
author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
eprint = {1704.02304},
file = {:C\:/Users/jongwook/Dropbox/References/It Takes (Only) Two Adversarial Generator-Encoder Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1704.02304},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{It Takes (Only) Two: Adversarial Generator-Encoder Networks}},
url = {http://arxiv.org/abs/1704.02304},
year = {2017}
}
@article{klapuri2003multiple,
author = {Klapuri, Anssi P},
file = {:C\:/Users/jongwook/Dropbox/References/Multiple Fundamental Frequency Estimation Based on Harmonicity and Spectral Smoothness.pdf:pdf},
journal = {{IEEE} Transactions on Speech and Audio Processing},
number = {6},
pages = {804--816},
publisher = {IEEE},
title = {{Multiple Fundamental Frequency Estimation Based on Harmonicity and Spectral Smoothness}},
volume = {11},
year = {2003}
}
@article{shao2017riemannian,
abstract = {Deep generative models learn a mapping from a low dimensional latent space to a high-dimensional data space. Under certain regularity conditions, these models parameterize nonlinear manifolds in the data space. In this paper, we investigate the Riemannian geometry of these generated manifolds. First, we develop efficient algorithms for computing geodesic curves, which provide an intrinsic notion of distance between points on the manifold. Second, we develop an algorithm for parallel translation of a tangent vector along a path on the manifold. We show how parallel translation can be used to generate analogies, i.e., to transport a change in one data point into a semantically similar change of another data point. Our experiments on real image data show that the manifolds learned by deep generative models, while nonlinear, are surprisingly close to zero curvature. The practical implication is that linear paths in the latent space closely approximate geodesics on the generated manifold. However, further investigation into this phenomenon is warranted, to identify if there are other architectures or datasets where curvature plays a more prominent role. We believe that exploring the Riemannian geometry of deep generative models, using the tools developed in this paper, will be an important step in understanding the high-dimensional, nonlinear spaces these models learn.},
archivePrefix = {arXiv},
arxivId = {1711.08014},
author = {Shao, Hang and Kumar, Abhishek and Fletcher, P. Thomas},
eprint = {1711.08014},
file = {:C\:/Users/jongwook/Dropbox/References/The Riemannian Geometry of Deep Generative Models.pdf:pdf},
journal = {arXiv preprint arXiv:1711.08014},
keywords = {Riemannian},
mendeley-tags = {Riemannian},
title = {{The Riemannian Geometry of Deep Generative Models}},
url = {http://arxiv.org/abs/1711.08014},
year = {2017}
}
@inproceedings{kingma2016iaf,
archivePrefix = {arXiv},
arxivId = {1606.04934},
author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1606.04934},
file = {:C\:/Users/jongwook/Dropbox/References/Improving Variational Inference with Inverse Autoregressive Flow.pdf:pdf},
isbn = {9781611970685},
issn = {10495258},
keywords = {VAE},
mendeley-tags = {VAE},
title = {{Improving Variational Inference with Inverse Autoregressive Flow}},
url = {http://arxiv.org/abs/1606.04934},
year = {2016}
}
@inproceedings{bittner2014medleydb,
author = {Bittner, Rachel M and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan Pablo},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/MedleyDB A Multitrack Dataset for Annotation-Intensive MIR Research.pdf:pdf},
keywords = {Dataset},
mendeley-tags = {Dataset},
pages = {155--160},
title = {{MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research.}},
volume = {14},
year = {2014}
}
@inproceedings{zhao2018arae,
abstract = {While autoencoders are a key technique in representation learning for continuous structures, such as images or wave forms, developing general-purpose autoencoders for discrete structures, such as text sequence or discretized images, has proven to be more challenging. In particular, discrete inputs make it more difficult to learn a smooth encoder that preserves the complex local relationships in the input space. In this work, we propose an adversarially regularized autoencoder (ARAE) with the goal of learning more robust discrete-space representations. ARAE jointly trains both a rich discrete-space encoder, such as an RNN, and a simpler continuous space generator function, while using generative adversarial network (GAN) training to constrain the distributions to be similar. This method yields a smoother contracted code space that maps similar inputs to nearby codes, and also an implicit latent variable GAN model for generation. Experiments on text and discretized images demonstrate that the GAN model produces clean interpolations and captures the multimodality of the original space, and that the autoencoder produces improve- ments in semi-supervised learning as well as state-of-the-art results in unaligned text style transfer task using only a shared continuous-space representation.},
archivePrefix = {arXiv},
arxivId = {1706.04223},
author = {Zhao, Junbo and Kim, Yoon and Zhang, Kelly and Rush, Alexander M. and LeCun, Yann and Junbo and Zhao and Kim, Yoon and Zhang, Kelly and Rush, Alexander M. and LeCun, Yann},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1706.04223},
file = {:C\:/Users/jongwook/Dropbox/References/Adversarially Regularized Autoencoders.pdf:pdf},
pages = {1--18},
title = {{Adversarially Regularized Autoencoders}},
url = {http://arxiv.org/abs/1706.04223},
year = {2017}
}
@inproceedings{liu2017hyperspherical,
abstract = {Convolution as inner product has been the founding basis of convolutional neural networks (CNNs) and the key to end-to-end visual representation learning. Benefiting from deeper architectures, recent CNNs have demonstrated increasingly strong representation abilities. Despite such improvement, the increased depth and larger parameter space have also led to challenges in properly training a network. In light of such challenges, we propose hyperspherical convolution (SphereConv), a novel learning framework that gives angular representations on hyperspheres. We introduce SphereNet, deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks. In particular, SphereNet adopts SphereConv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under SphereConv. We show that SphereNet can effectively encode discriminative representation and alleviate training difficulty, leading to easier optimization, faster convergence and comparable (even better) classification accuracy over convolutional counterparts. We also provide some theoretical insights for the advantages of learning on hyperspheres. In addition, we introduce the learnable SphereConv, i.e., a natural improvement over prefixed SphereConv, and SphereNorm, i.e., hyperspherical learning as a normalization method. Experiments have verified our conclusions.},
archivePrefix = {arXiv},
arxivId = {1711.03189},
author = {Liu, Weiyang and Zhang, Yan-Ming and Li, Xingguo and Yu, Zhiding and Dai, Bo and Zhao, Tuo and Song, Le},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1711.03189},
file = {:C\:/Users/jongwook/Dropbox/References/Deep Hyperspherical Learning.pdf:pdf},
title = {{Deep Hyperspherical Learning}},
url = {http://arxiv.org/abs/1711.03189},
year = {2017}
}
@article{arjovsky2017wgan,
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
file = {:C\:/Users/jongwook/Dropbox/References/Wasserstein GAN.pdf:pdf},
journal = {arXiv preprint arXiv:1701.07875},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Wasserstein GAN}},
year = {2017}
}
@article{su2009cf,
abstract = {As one of the most successful approaches to building recommender systems, collaborative filtering ( CF ) uses the known preferences of a group of users to make recommendations or predictions of the unknown preferences for other users. In this paper, we first introduce CF tasks and their main challenges, such as data sparsity, scalability, synonymy, gray sheep, shilling attacks, privacy protection, etc., and their possible solutions. We then present three main categories of CF techniques: memory-based, model-based, and hybrid CF algorithms (that combine CF with other recommendation techniques), with examples for representative algorithms of each category, and analysis of their predictive performance and their ability to address the challenges. From basic techniques to the state-of-the-art, we attempt to present a comprehensive survey for CF techniques, which can be served as a roadmap for research and practice in this area.},
archivePrefix = {arXiv},
arxivId = {421425},
author = {Su, Xiaoyuan and Khoshgoftaar, Taghi M.},
doi = {10.1155/2009/421425},
eprint = {421425},
file = {:C\:/Users/jongwook/Dropbox/References/A Survey of Collaborative Filtering Techniques.pdf:pdf},
isbn = {1687-7470},
issn = {1687-7470},
journal = {Advances in Artificial Intelligence},
keywords = {Collaborative Filtering,Recommender Systems},
mendeley-tags = {Collaborative Filtering,Recommender Systems},
pages = {1--19},
pmid = {11867525},
title = {{A Survey of Collaborative Filtering Techniques}},
url = {http://www.hindawi.com/journals/aai/2009/421425/},
volume = {2009},
year = {2009}
}
@article{donahue2018wavegan,
abstract = {While Generative Adversarial Networks (GANs) have seen wide success at the problem of synthesizing realistic images, they have seen little application to the problem of unsupervised audio generation. Unlike for images, a barrier to success is that the best discriminative representations for audio tend to be non-invertible, and thus cannot be used to synthesize listenable outputs. In this paper, we introduce WaveGAN, a first attempt at applying GANs to raw audio synthesis in an unsupervised setting. Our experiments on speech demonstrate that WaveGAN can produce intelligible words from a small vocabulary of human speech, as well as synthesize audio from other domains such as bird vocalizations, drums, and piano. Qualitatively, we find that human judges prefer the generated examples from WaveGAN over those from a method which naively apply GANs on image-like audio feature representations.},
archivePrefix = {arXiv},
arxivId = {1802.04208},
author = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
eprint = {1802.04208},
file = {:C\:/Users/jongwook/Dropbox/References/Synthesizing Audio with Generative Adversarial Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1802.04208},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Synthesizing Audio with Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1802.04208},
year = {2018}
}
@inproceedings{mohamed2016implicit,
abstract = {Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.},
archivePrefix = {arXiv},
arxivId = {1610.03483},
author = {Mohamed, Shakir and Lakshminarayanan, Balaji},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1610.03483},
file = {:C\:/Users/jongwook/Dropbox/References/Learning in Implicit Generative Models.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Learning in Implicit Generative Models}},
url = {http://arxiv.org/abs/1610.03483},
year = {2017}
}
@inproceedings{pachet2017sampling,
abstract = {Recently, machine-learning techniques have been success-fully used for the generation of complex artifacts such as music or text. However, these techniques are still unable to capture and generate artifacts that are convincingly struc-tured. In particular, musical sequences do not exhibit pat-tern structure, as typically found in human composed mu-sic. We present an approach to generate structured se-quences, based on a mechanism for sampling efficiently variations of musical sequences. Given an input sequence and a statistical model, this mechanism uses belief propa-gation to sample a set of sequences whose distance to the input sequence is approximately within specified bounds. This mechanism uses local fields to bias the generation. We show experimentally that sampled sequences are in-deed closely correlated to the standard musical similarity function defined by Mongeau and Sankoff. We then show how this mechanism can be used to implement composi-tion strategies that enforce arbitrary structure on a musical lead sheet generation problem. We illustrate our approach with a convincingly structured generated lead sheet in the style of the Beatles.},
author = {Pachet, Fran{\c{c}}ois and Papadopoulos, Alexandre and Roy, Pierre},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Sampling Variations of Sequences for Structured Music Generation.pdf:pdf},
keywords = {Symbolic},
mendeley-tags = {Symbolic},
title = {{Sampling Variations of Sequences for Structured Music Generation}},
url = {https://ismir2017.smcnus.org/wp-content/uploads/2017/10/50_Paper.pdf},
year = {2017}
}
@article{erhan2010pretraining,
author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
file = {:C\:/Users/jongwook/Dropbox/References/Why Does Unsupervised Pre-training Help Deep Learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
number = {Feb},
pages = {625--660},
title = {{Why Does Unsupervised Pre-training Help Deep Learning?}},
volume = {11},
year = {2010}
}
@inproceedings{logan2000mfcc,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Logan, Beth},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
doi = {10.1.1.11.9216},
eprint = {arXiv:1011.1669v3},
file = {:C\:/Users/jongwook/Dropbox/References/Mel Frequency Cepstral Coefficients for Music Modeling.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
title = {{Mel Frequency Cepstral Coefficients for Music Modeling}},
url = {http://ismir2000.ismir.net/papers/logan_paper.pdf},
year = {2000}
}
@article{louppe2017nondifferentiable,
abstract = {Complex computer simulators are increasingly used across fields of science as generative models tying parameters of an underlying theory to experimental observations. Inference in this setup is often difficult, as simulators rarely admit a tractable density or likelihood function. We introduce Adversarial Variational Optimization (AVO), a likelihood-free inference algorithm for fitting a non-differentiable generative model incorporating ideas from empirical Bayes and variational inference. We adapt the training procedure of generative adversarial networks by replacing the differentiable generative network with a domain-specific simulator. We solve the resulting non-differentiable minimax problem by minimizing variational upper bounds of the two adversarial objectives. Effectively, the procedure results in learning a proposal distribution over simulator parameters, such that the corresponding marginal distribution of the generated data matches the observations. We present results of the method with simulators producing both discrete and continuous data.},
archivePrefix = {arXiv},
arxivId = {1707.07113},
author = {Louppe, Gilles and Cranmer, Kyle},
eprint = {1707.07113},
file = {:C\:/Users/jongwook/Dropbox/References/Adversarial Variational Optimization of Non-Differentiable Simulators.pdf:pdf},
journal = {arXiv preprint arXiv:1707.07113},
keywords = {Variational},
mendeley-tags = {Variational},
title = {{Adversarial Variational Optimization of Non-Differentiable Simulators}},
url = {http://arxiv.org/abs/1707.07113},
year = {2017}
}
@article{pesek2017hierarchical,
author = {Pesek, Matev{\v{z}} and Leonardis, Ale{\v{s}} and Marolt, Matija},
doi = {10.1371/journal.pone.0169411},
file = {:C\:/Users/jongwook/Dropbox/References/Robust Real-Time Music Transcription with a Compositional Hierarchical Model.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {1},
title = {{Robust Real-Time Music Transcription with a Compositional Hierarchical Model}},
volume = {12},
year = {2017}
}
@incollection{hamanaka2013computational,
author = {Hamanaka, Masatoshi and Hirata, Keiji and Tojo, Satoshi},
booktitle = {Guide to Computing for Expressive Music Performance},
doi = {10.1007/978-1-4471-4123-5},
file = {:C\:/Users/jongwook/Dropbox/References/Computational Music Theory and Its Applications to Expressive Performance and Composition.pdf:pdf},
isbn = {978-1-4471-4122-8},
pages = {205--234},
title = {{Computational Music Theory and Its Applications to Expressive Performance and Composition}},
url = {http://link.springer.com/10.1007/978-1-4471-4123-5},
year = {2013}
}
@incollection{talkin1995rapt,
author = {Talkin, David and Kleijn, W. B. and Paliwal, K. K.},
booktitle = {Speech Coding and Synthesis},
chapter = {14},
file = {:C\:/Users/jongwook/Dropbox/References/A Robust Algorithm for Pitch Tracking.pdf:pdf},
isbn = {978-0444821690},
pages = {495--518},
publisher = {Elsevier Science},
title = {{A Robust Algorithm for Pitch Tracking (RAPT)}},
year = {1995}
}
@inproceedings{rezende2015flow,
archivePrefix = {arXiv},
arxivId = {1505.05770},
author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1505.05770},
file = {:C\:/Users/jongwook/Dropbox/References/Variational Inference with Normalizing Flows.pdf:pdf},
isbn = {1505.05770},
issn = {1938-7228},
title = {{Variational Inference with Normalizing Flows}},
url = {http://arxiv.org/abs/1505.05770},
volume = {37},
year = {2015}
}
@article{oord2016wavenet,
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
file = {:C\:/Users/jongwook/Dropbox/References/WaveNet A Generative Model for Raw Audio.pdf:pdf},
journal = {arXiv preprint arXiv:1609.03499},
title = {{WaveNet: A Generative Model for Raw Audio}},
year = {2016}
}
@article{duchi2011adagrad,
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
file = {:C\:/Users/jongwook/Dropbox/References/Adaptive subgradient methods for online learning and stochastic optimization.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Optimizer},
mendeley-tags = {Optimizer},
number = {Jul},
pages = {2121--2159},
title = {{Adaptive subgradient methods for online learning and stochastic optimization}},
volume = {12},
year = {2011}
}
@article{wu2016google,
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and {Jeff Klingner} and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Lukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
file = {:C\:/Users/jongwook/Dropbox/References/Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
journal = {arXiv preprint arXiv:1609.08144},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
year = {2016}
}
@inproceedings{choi2017transfer,
abstract = {In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pre-trained convnet feature, a concatenated feature vector using the activations of feature maps of multiple layers in a trained convolutional network. We show how this convnet feature can serve as general-purpose music representation. In the experiments, a convnet is trained for music tagging and then transferred to other music-related classification and regression tasks. The convnet feature outperforms the baseline MFCC feature in all the considered tasks and several previous approaches that are aggregating MFCCs as well as low- and high-level music features.},
archivePrefix = {arXiv},
arxivId = {1703.09179},
author = {Choi, Keunwoo and Fazekas, Gy{\"{o}}rgy and Sandler, Mark and Cho, Kyunghyun},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
eprint = {1703.09179},
file = {:C\:/Users/jongwook/Dropbox/References/Transfer Learning for Music Classification and Regression Tasks.pdf:pdf},
pages = {141--149},
title = {{Transfer Learning for Music Classification and Regression Tasks}},
url = {http://arxiv.org/abs/1703.09179},
year = {2017}
}
@inproceedings{kim2018crepe,
archivePrefix = {arXiv},
arxivId = {1802.06182},
author = {Kim, Jong Wook and Salamon, Justin and Li, Peter and Bello, Juan Pablo},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
eprint = {1802.06182},
file = {:C\:/Users/jongwook/Dropbox/References/CREPE A Convolutional Representation for Pitch Estimation.pdf:pdf},
title = {{CREPE: A Convolutional Representation for Pitch Estimation}},
url = {http://arxiv.org/abs/1802.06182},
year = {2018}
}
@inproceedings{bonada2008wideband,
abstract = {In this paper we propose a method to estimate and transform harmonic components in wide-band conditions, out of a single period of the analyzed signal. This method allows estimating harmonic parameters with higher temporal resolution than typical Short Time Fourier Transform (STFT) based methods. We also discuss transformations and synthesis strategies in such context, focusing on the human voice.},
author = {Bonada, Jordi},
booktitle = {Proceedings of the International Conference on Digital Audio Effects {(DAFx)}},
file = {:C\:/Users/jongwook/Dropbox/References/Wide-Band Harmonic Sinusoidal Modeling.pdf:pdf},
isbn = {9789512295173},
title = {{Wide-Band Harmonic Sinusoidal Modeling}},
url = {http://mtg.upf.es/files/publications/WBHSM.pdf},
year = {2008}
}
@inproceedings{humphrey2011nlse,
author = {Humphrey, Eric J and Glennon, Aron P and Bello, Juan Pablo},
booktitle = {Proceedings of the International Conference on Machine Learning and Applications and Workshops {(ICMLA)}},
file = {:C\:/Users/jongwook/Dropbox/References/Non-Linear Semantic Embedding for Organizing Large Instrument Sample Libraries.pdf:pdf},
organization = {IEEE},
pages = {142--147},
title = {{Non-Linear Semantic Embedding for Organizing Large Instrument Sample Libraries}},
volume = {2},
year = {2011}
}
@article{subakan2017gan,
abstract = {Generative source separation methods such as non-negative matrix factorization (NMF) or auto-encoders, rely on the assumption of an output probability density. Generative Adversarial Networks (GANs) can learn data distributions without needing a parametric assumption on the output density. We show on a speech source separation experiment that, a multi-layer perceptron trained with a Wasserstein-GAN formulation outperforms NMF, auto-encoders trained with maximum likelihood, and variational auto-encoders in terms of source to distortion ratio.},
archivePrefix = {arXiv},
arxivId = {1710.10779},
author = {Subakan, Cem and Smaragdis, Paris},
eprint = {1710.10779},
file = {:C\:/Users/jongwook/Dropbox/References/Generative Adversarial Source Separation.pdf:pdf},
journal = {arXiv preprint arXiv:1710.10779},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Generative Adversarial Source Separation}},
url = {http://arxiv.org/abs/1710.10779},
year = {2017}
}
@article{mnih2014nvil,
archivePrefix = {arXiv},
arxivId = {arXiv:1402.0030v2},
author = {Mnih, A. and Gregor, Karol},
eprint = {arXiv:1402.0030v2},
file = {:C\:/Users/jongwook/Dropbox/References/Neural Variational Inference and Learning in Belief Networks.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of the International Conference on Machine Learning {(ICML)}},
keywords = {belief networks,deep learning,variational inference},
title = {{Neural Variational Inference and Learning in Belief Networks}},
volume = {32},
year = {2014}
}
@article{tzanetakis2002genre,
archivePrefix = {arXiv},
arxivId = {62},
author = {Tzanetakis, George and Cook, Perry},
doi = {10.1109/TSA.2002.800560},
eprint = {62},
file = {:C\:/Users/jongwook/Dropbox/References/Musical Genre Classification of Audio Signals.pdf:pdf},
isbn = {1063-6676},
issn = {10636676},
journal = {{IEEE} Transactions on Speech and Audio Processing},
keywords = {Audio classification,Beat analysis,Feature extraction,Musical genre classification,Wavelets},
number = {5},
pages = {293--302},
pmid = {7359731},
title = {{Musical Genre Classification of Audio Signals}},
volume = {10},
year = {2002}
}
@article{decheveigne2002yin,
abstract = {An algorithm is presented for the estimation of the fundamental frequency (F0) of speech or musical sounds. It is based on the well-known autocorrelation method with a number of modifications that combine to prevent errors. The algorithm has several desirable features. Error rates are about three times lower than the best competing methods, as evaluated over a database of speech recorded together with a laryngograph signal. There is no upper limit on the frequency search range, so the algorithm is suited for high-pitched voices and music. The algorithm is relatively simple and may be implemented efficiently and with low latency, and it involves few parameters that must be tuned. It is based on a signal model (periodic signal) that may be extended in several ways to handle various forms of aperiodicity that occur in particular applications. Finally, interesting parallels may be drawn with models of auditory processing.},
author = {de Cheveign{\'{e}}, Alain and Kawahara, Hideki},
doi = {10.1121/1.1458024},
file = {:C\:/Users/jongwook/Dropbox/References/YIN, a Fundamental Frequency Estimator for Speech and Music.pdf:pdf},
isbn = {0001-4966 (Print)},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
keywords = {F0,YIN},
mendeley-tags = {F0,YIN},
number = {4},
pages = {1917--1930},
pmid = {12002874},
title = {{YIN, a Fundamental Frequency Estimator for Speech and Music}},
url = {http://asa.scitation.org/doi/10.1121/1.1458024},
volume = {111},
year = {2002}
}
@article{brahma2016disentanglement,
author = {Brahma, Pratik Prabhanjan and Wu, Dapeng and She, Yiyuan},
doi = {10.1109/TNNLS.2015.2496947},
file = {:C\:/Users/jongwook/Dropbox/References/Why Deep Learning Works A Manifold Disentanglement Perspective.pdf:pdf},
issn = {21622388},
journal = {{IEEE} Transactions on Neural Networks and Learning Systems},
keywords = {Deep learning,disentanglement,manifold learning,unsupervised feature transformation},
number = {10},
pages = {1997--2008},
title = {{Why Deep Learning Works: A Manifold Disentanglement Perspective}},
volume = {27},
year = {2016}
}
@article{kassler1966mir,
author = {Kassler, Michael},
doi = {10.2307/832213},
file = {:C\:/Users/jongwook/Dropbox/References/Toward Musical Information Retrieval.pdf:pdf},
isbn = {00316016},
issn = {00316016},
journal = {Perspectives of New Music},
keywords = {MIR},
mendeley-tags = {MIR},
number = {2},
pages = {59},
publisher = {JSTOR},
title = {{Toward Musical Information Retrieval}},
volume = {4},
year = {1966}
}
@inproceedings{michelsanti2017gan,
abstract = {Improving speech system performance in noisy environments remains a challenging task, and speech enhancement (SE) is one of the effective techniques to solve the problem. Motivated by the promising results of generative adversarial networks (GANs) in a variety of image processing tasks, we explore the potential of conditional GANs (cGANs) for SE, and in particular, we make use of the image processing framework proposed by Isola et al. [1] to learn a mapping from the spectrogram of noisy speech to an enhanced counterpart. The SE cGAN consists of two networks, trained in an adversarial manner: a generator that tries to enhance the input noisy spectrogram, and a discriminator that tries to distinguish between enhanced spectrograms provided by the generator and clean ones from the database using the noisy spectrogram as a condition. We evaluate the performance of the cGAN method in terms of perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI), and equal error rate (EER) of speaker verification (an example application). Experimental results show that the cGAN method overall outperforms the classical short-time spectral amplitude minimum mean square error (STSA-MMSE) SE algorithm, and is comparable to a deep neural network-based SE approach (DNN-SE).},
archivePrefix = {arXiv},
arxivId = {1709.01703},
author = {Michelsanti, Daniel and Tan, Zheng Hua},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association {(INTERSPEECH)}},
doi = {10.21437/Interspeech.2017-1620},
eprint = {1709.01703},
file = {:C\:/Users/jongwook/Dropbox/References/Conditional Generative Adversarial Networks for Speech Enhancement and Noise-Robust Speaker Verification.pdf:pdf},
issn = {19909772},
keywords = {GAN,Generative Adversarial Networks,Speaker Verification,Speech Enhancement},
mendeley-tags = {GAN},
title = {{Conditional Generative Adversarial Networks for Speech Enhancement and Noise-Robust Speaker Verification}},
year = {2017}
}
@inproceedings{miyato2018cgan,
author = {Miyato, Takeru and Koyama, Masanori},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:C\:/Users/jongwook/Dropbox/References/cGANs with Projection Discriminator.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{cGANs with Projection Discriminator}},
year = {2018}
}
@article{oudre2009chord,
archivePrefix = {arXiv},
arxivId = {1605.07008},
author = {Oudre, Laurent and Grenier, Yves and F{\'{e}}votte, C{\'{e}}dric},
doi = {10.1109/MLSP.2016.7738895},
eprint = {1605.07008},
file = {:C\:/Users/jongwook/Dropbox/References/Template-Based Chord Recognition Influence of the Chord Types.pdf:pdf},
isbn = {9780981353708},
issn = {21610371},
journal = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
pages = {153--158},
title = {{Template-Based Chord Recognition: Influence of the Chord Types}},
year = {2009}
}
@article{kumar2018ecommercegan,
abstract = {E-commerce companies such as Amazon, Alibaba and Flipkart process billions of orders every year. However, these orders represent only a small fraction of all plausible orders. Exploring the space of all plausible orders could help us better understand the relationships between the various entities in an e-commerce ecosystem, namely the customers and the products they purchase. In this paper, we propose a Generative Adversarial Network (GAN) for orders made in e-commerce websites. Once trained, the generator in the GAN could generate any number of plausible orders. Our contributions include: (a) creating a dense and low-dimensional representation of e-commerce orders, (b) train an ecommerceGAN (ecGAN) with real orders to show the feasibility of the proposed paradigm, and (c) train an ecommerce-conditional-GAN (ec^2GAN) to generate the plausible orders involving a particular product. We propose several qualitative methods to evaluate ecGAN and demonstrate its effectiveness. The ec^2GAN is used for various kinds of characterization of possible orders involving a product that has just been introduced into the e-commerce system. The proposed approach ec^2GAN performs significantly better than the baseline in most of the scenarios.},
archivePrefix = {arXiv},
arxivId = {1801.03244},
author = {Kumar, Ashutosh and Biswas, Arijit and Sanyal, Subhajit},
eprint = {1801.03244},
file = {:C\:/Users/jongwook/Dropbox/References/eCommerceGAN A Generative Adversarial Network for E-commerce.pdf:pdf},
isbn = {1234567245},
journal = {arXiv preprint arXiv:1801.03244},
keywords = {GAN,acm reference format,deep learning,e-commerce,generative adversarial networks,order embedding,product recommendation},
mendeley-tags = {GAN},
title = {{eCommerceGAN: A Generative Adversarial Network for E-commerce}},
url = {http://arxiv.org/abs/1801.03244},
year = {2018}
}
@article{koren2008cf,
abstract = {Recommender systems provide userswith personalized suggestions for products or services. These systems often rely on Collaborat- ing Filtering (CF), where past transactions are analyzed in order to establish connections between users and products. The two more successful approaches to CF are latent factor models, which di- rectly profile both users and products, and neighborhood models, which analyze similarities between products or users. In this work we introduce some innovations to both approaches. The factor and neighborhood models can now be smoothly merged, thereby build- ing a more accurate combined model. Further accuracy improve- ments are achieved by extending themodels to exploit both explicit and implicit feedback by the users. The methods are tested on the Netflix data. Results are better than those previously published on that dataset. In addition, we suggest a newevaluationmetric,which highlights the differences among methods, based on their perfor- mance at a top-K recommendation task. Categories},
archivePrefix = {arXiv},
arxivId = {62},
author = {Koren, Yehuda},
doi = {10.1145/1401890.1401944},
eprint = {62},
file = {:C\:/Users/jongwook/Dropbox/References/Factorization Meets the Neighborhood a Multifaceted Collaborative Filtering Model.pdf:pdf},
isbn = {1605581933},
issn = {1605581933},
journal = {Proceedings of the {ACM SIGKDD} International Conference on Knowledge Discovery and Data Mining},
keywords = {collaborative filtering,recommender systems},
pages = {426--434},
pmid = {2428061},
title = {{Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model}},
url = {ACM},
year = {2008}
}
@inproceedings{gulrajani2017wgan,
abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
archivePrefix = {arXiv},
arxivId = {1704.00028},
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {10.1016/j.aqpro.2013.07.003},
eprint = {1704.00028},
file = {:C\:/Users/jongwook/Dropbox/References/Improved Training of Wasserstein GANs.pdf:pdf},
isbn = {0030-8870},
issn = {00308870},
pages = {5769--5779},
pmid = {24439530},
title = {{Improved Training of Wasserstein GANs}},
url = {http://arxiv.org/abs/1704.00028},
volume = {30},
year = {2017}
}
@inproceedings{teng2017generating,
abstract = {We present a hybrid neural network and rule-based system that generates pop music. Music produced by pure rule-based systems often sounds mechanical. Music produced by machine learning sounds better, but still lacks hierarchical temporal structure. We restore temporal hierarchy by augmenting machine learning with a temporal production grammar, which generates the music's overall structure and chord progressions. A compatible melody is then generated by a conditional variational recurrent autoencoder. The autoencoder is trained with eight-measure segments from a corpus of 10,000 MIDI files, each of which has had its melody track and chord progressions identified heuristically. The autoencoder maps melody into a multi-dimensional feature space, conditioned by the underlying chord progression. A melody is then generated by feeding a random sample from that space to the autoencoder's decoder, along with the chord progression generated by the grammar. The autoencoder can make musically plausible variations on an existing melody, suitable for recurring motifs. It can also reharmonize a melody to a new chord progression, keeping the rhythm and contour. The generated music compares favorably with that generated by other academic and commercial software designed for the music-as-a-service industry.},
archivePrefix = {arXiv},
arxivId = {1710.02280},
author = {Teng, Yifei and Zhao, An and Goudeseune, Camille},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
eprint = {1710.02280},
file = {:C\:/Users/jongwook/Dropbox/References/Generating Nontrivial Melodies for Music as a Service.pdf:pdf},
keywords = {Auto-Encoders,Symbolic},
mendeley-tags = {Auto-Encoders,Symbolic},
title = {{Generating Nontrivial Melodies for Music as a Service}},
url = {http://arxiv.org/abs/1710.02280},
year = {2017}
}
@article{simonyan2014vgg,
author = {Simonyan, Karen and Zisserman, Andrew},
file = {:C\:/Users/jongwook/Dropbox/References/Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
journal = {arXiv preprint arXiv:1409.1556},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
year = {2014}
}
@inproceedings{oramas2017genre,
abstract = {Music genres allow to categorize musical items that share common characteristics. Although these categories are not mutually exclusive, most related research is traditionally focused on classifying tracks into a single class. Furthermore, these categories (e.g., Pop, Rock) tend to be too broad for certain applications. In this work we aim to expand this task by categorizing musical items into multiple and fine-grained labels, using three different data modalities: audio, text, and images. To this end we present MuMu, a new dataset of more than 31k albums classified into 250 genre classes. For every album we have collected the cover image, text reviews, and audio tracks. Additionally, we propose an approach for multi-label genre classification based on the combination of feature embeddings learned with state-of-the-art deep learning methodologies. Experiments show major differences between modalities, which not only introduce new baselines for multi-label genre classification, but also suggest that combining them yields improved results.},
archivePrefix = {arXiv},
arxivId = {1707.04916},
author = {Oramas, Sergio and Nieto, Oriol and Barbieri, Francesco and Serra, Xavier},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
eprint = {1707.04916},
file = {:C\:/Users/jongwook/Dropbox/References/Multi-Label Music Genre Classification from Audio, Text, and Images Using Deep Features.pdf:pdf},
keywords = {Genre Classification},
mendeley-tags = {Genre Classification},
pages = {23--30},
title = {{Multi-Label Music Genre Classification from Audio, Text, and Images Using Deep Features}},
url = {http://arxiv.org/abs/1707.04916},
year = {2017}
}
@article{bello2005onset,
abstract = {Note onset detection and localization is useful in a number of analysis and indexing techniques for musical signals. The usual way to detect onsets is to look for “transient” regions in the signal, a notion that leads to many definitions: a sudden burst of energy, a change in the short-time spectrum of the signal or in the statistical properties, etc. The goal of this paper is to review, categorize, and compare some of the most commonly used techniques for onset detection, and to present possible enhancements. We discuss methods based on the use of explicitly predefined signal features: the signal's amplitude envelope, spectral magnitudes and phases, time-frequency representations; and methods based on probabilistic signal models: model-based change point detection, surprise signals, etc. Using a choice of test cases, we provide some guidelines for choosing the appropriate method for a given application.},
author = {Bello, Juan Pablo and Daudet, Laurent and Abdallah, Samer and Duxbury, Chris and Davies, Mike and Sandler, Mark B.},
doi = {10.1109/TSA.2005.851998},
file = {:C\:/Users/jongwook/Dropbox/References/A Tutorial on Onset Detection in Music Signals.pdf:pdf},
isbn = {1063-6676},
issn = {10636676},
journal = {{IEEE} Transactions on Speech and Audio Processing},
keywords = {Attack transcients,Audio,MIR,Note segmentation,Novelty detection,Onset},
mendeley-tags = {MIR,Onset},
number = {5},
pages = {1035--1046},
pmid = {1000106150},
title = {{A Tutorial on Onset Detection in Music Signals}},
volume = {13},
year = {2005}
}
@article{raczynski2013dynamic,
author = {Raczy{\'{n}}ski, Stanis{\l}aw A. and Vincent, Emmanuel and Sagayama, Shigeki},
doi = {10.1109/TASL.2013.2258012},
file = {:C\:/Users/jongwook/Dropbox/References/Dynamic Bayesian Networks for Symbolic Polyphonic Pitch Modeling.pdf:pdf},
issn = {1558-7916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
number = {9},
pages = {1830--1840},
publisher = {IEEE},
title = {{Dynamic Bayesian Networks for Symbolic Polyphonic Pitch Modeling}},
volume = {21},
year = {2013}
}
@inproceedings{lecun1995lenet,
abstract = {This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also rejection, training time, recognition time, and memory requirements.},
author = {LeCun, Yann and Jackel, Larry D. and Bottou, Leon and Brunot, A. and Cortes, Corinna and Denker, J. S. and Drucker, Harris and Guyon, I. and Muller, U. A. and Sackinger, Eduard and Simard, P. and Vapnik, V.},
booktitle = {Proceedings of the International Conference on Artificial Neural Networks},
file = {:C\:/Users/jongwook/Dropbox/References/Comparison of Learning Algorithms for Handwritten Digit Recognition.pdf:pdf},
pages = {53--60},
title = {{Comparison of Learning Algorithms for Handwritten Digit Recognition}},
volume = {60},
year = {1995}
}
@inproceedings{mroueh2018gan,
abstract = {We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM. The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure $\mu$. We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis. The Dominant measure $\mu$ plays a crucial role as it defines the support on which conditional CDFs are compared. Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cram\'er statistics to high dimensional distributions. We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs). We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation. Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization.},
author = {Mroueh, Youssef and Li, Chun-Liang and Sercu, Tom and Raj, Anant and Cheng, Yu},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:C\:/Users/jongwook/Dropbox/References/Sobolev GAN.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Sobolev GAN}},
year = {2018}
}
@article{gao2017nmf,
author = {Gao, Lufei and Su, Li and Yang, Yi Hsuan and Lee, T.},
doi = {10.1109/ICASSP.2017.7952164},
file = {:C\:/Users/jongwook/Dropbox/References/Polyphonic Piano Note Transcription with Non-Negative Matrix Factorization of Differential Spectrogram.pdf:pdf},
isbn = {9781509041176},
issn = {15206149},
journal = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}},
keywords = {Music information retrieval,differential spectrogram,non-negative matrix factorization,spectral flux},
title = {{Polyphonic Piano Note Transcription with Non-Negative Matrix Factorization of Differential Spectrogram}},
year = {2017}
}
@inproceedings{he2016resnet,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}},
file = {:C\:/Users/jongwook/Dropbox/References/Deep Residual Learning for Image Recognition.pdf:pdf},
pages = {770--778},
title = {{Deep Residual Learning for Image Recognition}},
year = {2016}
}
@article{kodali2017gan,
abstract = {We propose studying GAN training dynamics as regret minimization, which is in contrast to the popular view that there is consistent minimization of a divergence between real and generated distributions. We analyze the convergence of GAN training from this new point of view to understand why mode collapse happens. We hypothesize the existence of undesirable local equilibria in this non-convex game to be responsible for mode collapse. We observe that these local equilibria often exhibit sharp gradients of the discriminator function around some real data points. We demonstrate that these degenerate local equilibria can be avoided with a gradient penalty scheme called DRAGAN. We show that DRAGAN enables faster training, achieves improved stability with fewer mode collapses, and leads to generator networks with better modeling performance across a variety of architectures and objective functions.},
archivePrefix = {arXiv},
arxivId = {1705.07215},
author = {Kodali, Naveen and Abernethy, Jacob and Hays, James and Kira, Zsolt},
eprint = {1705.07215},
file = {:C\:/Users/jongwook/Dropbox/References/On Convergence and Stability of GANs.pdf:pdf},
isbn = {978 0 340 99716 1},
journal = {arXiv preprint arXiv:1705.07215},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{On Convergence and Stability of GANs}},
url = {http://arxiv.org/abs/1705.07215},
year = {2017}
}
@inproceedings{mauch2014pyin,
author = {Mauch, Matthias and Dixon, Simon},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:C\:/Users/jongwook/Dropbox/References/pYIN A Fundamental Frequency Estimator Using Probabilistic Threshold Distributions.pdf:pdf},
keywords = {F0},
mendeley-tags = {F0},
organization = {IEEE},
pages = {659--663},
title = {{pYIN: A Fundamental Frequency Estimator Using Probabilistic Threshold Distributions}},
year = {2014}
}
@inproceedings{mescheder2017gan,
abstract = {In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.},
archivePrefix = {arXiv},
arxivId = {1705.10461},
author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1705.10461},
file = {:C\:/Users/jongwook/Dropbox/References/The Numerics of GANs.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{The Numerics of GANs}},
url = {http://arxiv.org/abs/1705.10461},
year = {2017}
}
@article{lin2017pacgan,
abstract = {Generative adversarial networks (GANs) are innovative techniques for learning generative models of complex data distributions from samples. Despite remarkable recent improvements in generating realistic images, one of their major shortcomings is the fact that in practice, they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon, known as mode collapse, has been the main focus of several recent advances in GANs. Yet there is little understanding of why mode collapse happens and why existing approaches are able to mitigate mode collapse. We propose a principled approach to handling mode collapse, which we call packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We borrow analysis tools from binary hypothesis testing---in particular the seminal result of Blackwell [Bla53]---to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggests that packing provides significant improvements in practice as well.},
archivePrefix = {arXiv},
arxivId = {1712.04086},
author = {Lin, Zinan and Khetan, Ashish and Fanti, Giulia and Oh, Sewoong},
eprint = {1712.04086},
file = {:C\:/Users/jongwook/Dropbox/References/PacGAN The power of two samples in generative adversarial networks.pdf:pdf},
journal = {arXiv preprint arXiv:1712.04086},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{PacGAN: The power of two samples in generative adversarial networks}},
url = {http://arxiv.org/abs/1712.04086},
year = {2017}
}
@inproceedings{arora2018gan,
author = {Arora, Sanjeev and Zhang, Yi},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:C\:/Users/jongwook/Dropbox/References/Do GANs Learn the Distribution Some Theory and Empirics.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Do GANs Learn the Distribution? Some Theory and Empirics}},
year = {2018}
}
@article{chen2017gcn,
abstract = {It is a usual practice to ignore any structural information underlying classes in multi-class classification. In this paper, we propose a graph convolutional network (GCN) augmented neural network classifier to exploit a known, underlying graph structure of labels. The proposed approach resembles an (approximate) inference procedure in, for instance, a conditional random field (CRF), however without losing any modelling flexibility. The proposed method can easily scale up to thousands of labels. We evaluate the proposed approach on the problems of document classification and object recognition and report both accuracies and graph-theoretic metrics that correspond to the consistency of the model's prediction. The experiment results reveal that the proposed model outperforms a baseline method which ignores the graph structures of a label space.},
archivePrefix = {arXiv},
arxivId = {1710.04908},
author = {Chen, Meihao and Lin, Zhuoru and Cho, Kyunghyun},
eprint = {1710.04908},
file = {:C\:/Users/jongwook/Dropbox/References/Graph Convolutional Networks for Classification with a Structured Label Space.pdf:pdf},
journal = {arXiv preprint arXiv:1710.04908},
keywords = {GCN},
mendeley-tags = {GCN},
title = {{Graph Convolutional Networks for Classification with a Structured Label Space}},
url = {http://arxiv.org/abs/1710.04908},
year = {2017}
}
@inproceedings{sainath2015cldnn,
abstract = {Learning an acoustic model directly from the raw waveform has been an active area of research. However, waveform-based models have not yet matched the performance of log-mel trained neural networks. We will show that raw wave-form features match the performance of log-mel filterbank ener-gies when used with a state-of-the-art CLDNN acoustic model trained on over 2,000 hours of speech. Specifically, we will show the benefit of the CLDNN, namely the time convolution layer in reducing temporal variations, the frequency convolution layer for preserving locality and reducing frequency variations, as well as the LSTM layers for temporal modeling. In addition, by stacking raw waveform features with log-mel features, we achieve a 3% relative reduction in word error rate.},
author = {Sainath, Tara N. and Weiss, Ron J. and Senior, Andrew and Wilson, Kevin W. and Vinyals, Oriol},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association {(INTERSPEECH)}},
doi = {10.1.1.706.7430},
file = {:C\:/Users/jongwook/Dropbox/References/Learning the Speech Front-End with Raw Waveform CLDNNs.pdf:pdf},
issn = {19909772},
keywords = {Raw,Speech},
mendeley-tags = {Raw,Speech},
title = {{Learning the Speech Front-End with Raw Waveform CLDNNs}},
year = {2015}
}
@article{kingma2013vae,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
author = {Kingma, Diederik P and Welling, Max},
file = {:C\:/Users/jongwook/Dropbox/References/Auto-Encoding Variational Bayes.pdf:pdf},
journal = {arXiv preprint arXiv:1312.6114},
keywords = {VAE},
mendeley-tags = {VAE},
title = {{Auto-Encoding Variational Bayes}},
year = {2013}
}
@inproceedings{wei2018wgan,
author = {Wei, Xiang and Liu, Zixia and Wang, Liqiang and Gong, Boqing},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:C\:/Users/jongwook/Dropbox/References/Improving the Improved Training of Wasserstein GANs a Consistency Term and Its Dual Effect.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Improving the Improved Training of Wasserstein GANs a Consistency Term and Its Dual Effect}},
year = {2018}
}
@article{shepard1964circularity,
abstract = {A special set of computer‐generated complex tones is shown to lead to a complete breakdown of transitivity in judgments of relative pitch. Indeed, the tones can be represented as equally spaced points around a circle in such a way that the clockwise neighbor of each tone is judged higher in pitch while the counterclockwise neighbor is judged lower in pitch. Diametrically opposed tones—though clearly different in pitch—are quite ambiguous as to the direction of the difference. The results demonstrate the operation of a “proximity principle” for the continuum of frequency and suggest that perceived pitch cannot be adequately represented by a purely rectilinear scale.},
author = {Shepard, Roger N.},
doi = {10.1121/1.1919362},
file = {:C\:/Users/jongwook/Dropbox/References/Circularity in Judgments of Relative Pitch.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {12},
pages = {2346--2353},
publisher = {ASA},
title = {{Circularity in Judgments of Relative Pitch}},
url = {http://asa.scitation.org/doi/10.1121/1.1919362},
volume = {36},
year = {1964}
}
@article{ross1974amdf,
author = {Ross, Myron J. and Shaffer, Harry L. and Cohen, Andrew and Freudberg, Richard and Manley, Harold J.},
doi = {10.1109/TASSP.1974.1162598},
file = {:C\:/Users/jongwook/Dropbox/References/Average Magnitude Difference Function Pitch Extractor.pdf:pdf},
issn = {00963518},
journal = {{IEEE} Transactions on Acoustics, Speech, and Signal Processing},
number = {5},
pages = {353--362},
title = {{Average Magnitude Difference Function Pitch Extractor}},
volume = {ASSP-22},
year = {1974}
}
@article{hinton2006dbn,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
doi = {10.1162/neco.2006.18.7.1527},
eprint = {1111.6189v1},
file = {:C\:/Users/jongwook/Dropbox/References/A Fast Learning Algorithm for Deep Belief Nets.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
number = {7},
pages = {1527--1554},
pmid = {16764513},
title = {{A Fast Learning Algorithm for Deep Belief Nets}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.2006.18.7.1527},
volume = {18},
year = {2006}
}
@inproceedings{ioffe2015batchnorm,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:C\:/Users/jongwook/Dropbox/References/Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
number = {2},
pages = {448--456},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
year = {2015}
}
@article{bengio2009deeplearning,
author = {Bengio, Yoshua},
file = {:C\:/Users/jongwook/Dropbox/References/Learning Deep Architectures for AI.pdf:pdf},
journal = {Foundations and Trends in Machine Learning},
keywords = {Survey},
mendeley-tags = {Survey},
number = {1},
pages = {1--127},
publisher = {Now Publishers, Inc.},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@techreport{kleene1951representation,
author = {Kleene, Stephen Cole},
file = {:C\:/Users/jongwook/Dropbox/References/Representation of Events in Nerve Nets and Finite Automata.pdf:pdf},
institution = {DTIC Document},
title = {{Representation of Events in Nerve Nets and Finite Automata}},
year = {1951}
}
@inproceedings{harte2006tonnetz,
author = {Harte, Christopher and Sandler, Mark and Gasser, Martin},
booktitle = {Proceedings of the {ACM} workshop on Audio and Music Computing Multimedia},
doi = {10.1145/1178723.1178727},
file = {:C\:/Users/jongwook/Dropbox/References/Detecting Harmonic Change in Musical Audio.pdf:pdf},
isbn = {1595935010},
keywords = {audio,harmonic,music,pitch space,segmentation},
pages = {21},
title = {{Detecting Harmonic Change in Musical Audio}},
url = {http://portal.acm.org/citation.cfm?doid=1178723.1178727},
year = {2006}
}
@article{srivastava2014dropout,
author = {Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:C\:/Users/jongwook/Dropbox/References/Dropout a Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
journal = {Journal of Machine Learning Research},
number = {1},
pages = {1929--1958},
title = {{Dropout: a Simple Way to Prevent Neural Networks from Overfitting.}},
volume = {15},
year = {2014}
}
@article{ranganath2013bbvi,
archivePrefix = {arXiv},
arxivId = {1401.0118},
author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
eprint = {1401.0118},
file = {:C\:/Users/jongwook/Dropbox/References/Black Box Variational Inference.pdf:pdf},
isbn = {1401.0118},
issn = {0002-9513},
journal = {Artificial Intelligence and Statistics},
title = {{Black Box Variational Inference}},
url = {http://arxiv.org/abs/1401.0118%0Ahttp://www.ncbi.nlm.nih.gov/pubmed/814822%0Ahttp://arxiv.org/abs/1401.0118},
year = {2014}
}
@inproceedings{jansen2017audio,
abstract = {Our goal is to learn semantically structured audio representations without relying on categorically labeled data. We consider several class-agnostic semantic con-straints that are inherent to non-speech audio: (i) sound categories are invariant to additive noise and translations in time, (ii) mixtures of two sound events inherit the categories of the constituents, and (iii) the categories of events in close temporal proximity in a single recording are likely to be the same or related. We apply these constraints to sample training data for triplet-loss embedding models using a large unlabeled dataset of YouTube soundtracks. The resulting low-dimensional repre-sentations provide both greatly improved query-by-example retrieval performance and reduced labeled data and model complexity requirements for supervised sound classification.},
author = {Jansen, Aren and Plakal, Manoj and Pandya, Ratheet and Ellis, Daniel P W and Hershey, Shawn and Liu, Jiayang and Moore, R Channing and Saurous, Rif A},
booktitle = {{NIPS} Workshop on Machine Learning for Audio Signal Processing {(ML4Audio)}},
file = {:C\:/Users/jongwook/Dropbox/References/Towards Learning Semantic Audio Representations from Unlabeled Data.pdf:pdf},
title = {{Towards Learning Semantic Audio Representations from Unlabeled Data}},
url = {http://media.aau.dk/smc/wp-content/uploads/2017/12/ML4AudioNIPS17_paper_18.pdf},
year = {2017}
}
@inproceedings{kelz2017entanglement,
author = {Nelissen, Geoffrey and N{\'{e}}lis, Vincent},
booktitle = {Proceedings of the {AES} Conference on Semantic Audio},
file = {:C\:/Users/jongwook/Dropbox/References/An Experimental Analysis of the Entanglement Problem in Neural-Network-based Music Transcription Systems.pdf:pdf},
title = {{An Experimental Analysis of the Entanglement Problem in Neural-Network-based Music Transcription Systems}},
year = {2017}
}
@article{fan2017svsgan,
abstract = {Separating two sources from an audio mixture is an important task with many applications. It is a challenging problem since only one signal channel is available for analysis. In this paper, we propose a novel framework for singing voice separation using the generative adversarial network (GAN) with a time-frequency masking function. The mixture spectra is considered to be a distribution and is mapped to the clean spectra which is also considered a distribtution. The approximation of distributions between mixture spectra and clean spectra is performed during the adversarial training process. In contrast with current deep learning approaches for source separation, the parameters of the proposed framework are first initialized in a supervised setting and then optimized by the training procedure of GAN in an unsupervised setting. Experimental results on three datasets (MIR-1K, iKala and DSD100) show that performance can be improved by the proposed framework consisting of conventional networks.},
archivePrefix = {arXiv},
arxivId = {1710.11428},
author = {Fan, Zhe-Cheng and Lai, Yen-Lin and Jang, Jyh-Shing Roger},
eprint = {1710.11428},
file = {:C\:/Users/jongwook/Dropbox/References/SVSGAN Singing Voice Separation via Generative Adversarial Network.pdf:pdf},
journal = {arXiv preprint arXiv:1710.11428},
keywords = {GAN,Source Separation},
mendeley-tags = {GAN,Source Separation},
title = {{SVSGAN: Singing Voice Separation via Generative Adversarial Network}},
url = {http://arxiv.org/abs/1710.11428},
year = {2017}
}
@article{gatys2015style,
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
file = {:C\:/Users/jongwook/Dropbox/References/A Neural Algorithm of Artistic Style.pdf:pdf},
journal = {arXiv preprint arXiv:1508.06576},
keywords = {Style Transfer},
mendeley-tags = {Style Transfer},
title = {{A Neural Algorithm of Artistic Style}},
year = {2015}
}
@inproceedings{hawthorne2018piano,
archivePrefix = {arXiv},
arxivId = {1710.11153},
author = {Hawthorne, Curtis and Elsen, Erich and Song, Jialin and Roberts, Adam and Simon, Ian and Raffel, Colin and Engel, Jesse and Oore, Sageev and Eck, Douglas},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
eprint = {1710.11153},
file = {:C\:/Users/jongwook/Dropbox/References/Onsets and Frames Dual-Objective Piano Transcription.pdf:pdf},
title = {{Onsets and Frames: Dual-Objective Piano Transcription}},
url = {http://arxiv.org/abs/1710.11153},
year = {2018}
}
@article{blei2003lda,
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {:C\:/Users/jongwook/Dropbox/References/Latent Dirichlet Allocation.pdf:pdf},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}
@inproceedings{bock2011enhanced,
author = {B{\"{o}}ck, Sebastian and Schedl, Markus},
booktitle = {Proceedings of the International Conference on Digital Audio Effects {(DAFx)}},
file = {:C\:/Users/jongwook/Dropbox/References/Enhanced Beat Tracking With Context-Aware Neural Networks.pdf:pdf},
isbn = {9782954035109},
title = {{Enhanced Beat Tracking with Context-Aware Neural Networks}},
url = {http://www.cp.jku.at/research/papers/Boeck_Schedl_DAFx_2011.pdf},
year = {2011}
}
@inproceedings{sabour2017capsules,
abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
archivePrefix = {arXiv},
arxivId = {1710.09829},
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1710.09829},
file = {:C\:/Users/jongwook/Dropbox/References/Dynamic Routing Between Capsules.pdf:pdf},
keywords = {Capsule},
mendeley-tags = {Capsule},
title = {{Dynamic Routing Between Capsules}},
url = {http://arxiv.org/abs/1710.09829},
year = {2017}
}
@article{koushik2016eve,
author = {Koushik, Jayanth and Hayashi, Hiroaki},
file = {:C\:/Users/jongwook/Dropbox/References/Improving Stochastic Gradient Descent with Feedback.pdf:pdf},
journal = {arXiv preprint arXiv:1611.01505},
keywords = {Optimizer},
mendeley-tags = {Optimizer},
title = {{Improving Stochastic Gradient Descent with Feedback}},
year = {2016}
}
@article{rumelhart1986backpropagation,
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams., Ronald J.},
file = {:C\:/Users/jongwook/Dropbox/References/Learning Representations by Back-Propagating Errors.pdf:pdf},
journal = {Nature},
number = {6088},
pages = {533--538},
title = {{Learning Representations by Back-Propagating Errors}},
volume = {323},
year = {1986}
}
@inproceedings{zuo2015crnn,
abstract = {In existing convolutional neural networks (CNNs), both convolution and pooling are locally performed for image regions separately, no contextual dependencies between different image regions have been taken into consideration. Such dependencies represent useful spatial structure information in images. Whereas recurrent neural networks (RNNs) are designed for learning contextual dependencies among sequential data by using the recurrent (feedback) connections. In this work, we propose the convolutional recurrent neural network (C-RNN), which learns the spatial dependencies between image regions to enhance the discriminative power of image representation. The C-RNN is trained in an end-to-end manner from raw pixel images. CNN layers are firstly processed to generate middle level features. RNN layer is then learned to encode spatial dependencies. The C-RNN can learn better image representation, especially for images with obvious spatial contextual dependencies. Our method achieves competitive performance on ILSVRC 2012, SUN 397, and MIT indoor.},
author = {Zuo, Zhen and Shuai, Bing and Wang, Gang and Liu, Xiao and Wang, Xingxing and Wang, Bing and Chen, Yushi},
booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition Workshops {(CVPRW)}},
doi = {10.1109/CVPRW.2015.7301268},
file = {:C\:/Users/jongwook/Dropbox/References/Convolutional Recurrent Neural Networks Learning Spatial Dependencies for Image Representation.pdf:pdf},
isbn = {978-1-4673-6759-2},
issn = {21607516},
keywords = {CNN,CRNN,RNN},
mendeley-tags = {CNN,CRNN,RNN},
pages = {18--26},
title = {{Convolutional Recurrent Neural Networks: Learning Spatial Dependencies for Image Representation}},
url = {http://ieeexplore.ieee.org/document/7301268/},
year = {2015}
}
@article{bello2011structure,
abstract = {This paper presents a novel method for measuring the structural similarity between music recordings. It uses recurrence plot analysis to characterize patterns of repetition in the feature se- quence, and the normalized compression distance, a practical ap- proximation of the joint Kolmogorov complexity, to measure the pairwise similarity between the plots. By measuring the distance between intermediate representations of signal structure, the pro- posed method departs from common approaches to music struc- ture analysis which assume a block-based model of music, and thus concentrate on segmenting and clustering sections. The approach ensures that global structure is consistently and robustly character- ized in the presence of tempo, instrumentation, and key changes, while the used metric provides a simple to compute, versatile and robust alternative to common approaches in music similarity re- search. Finally, experimental results demonstrate success at char- acterizing similarity, while contributing an optimal parameteriza- tion of the proposed approach.},
author = {Bello, Juan P.},
file = {:C\:/Users/jongwook/Dropbox/References/Measuring Structural Similarity in Music.pdf:pdf},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Structure},
mendeley-tags = {Structure},
number = {7},
pages = {2013--2025},
title = {{Measuring Structural Similarity in Music}},
volume = {19},
year = {2011}
}
@article{hinton2012rmsprop,
author = {Hinton, Geoffrey},
file = {:C\:/Users/jongwook/Dropbox/References/rmsprop Divide the Gradient by a Running Average of Its Recent Magnitude.pdf:pdf},
journal = {Coursera: Neural Networks for Machine Learning},
title = {{rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude}},
year = {2012}
}
@inproceedings{bittner2017deepsalience,
abstract = {Estimating fundamental frequencies in polyphonic music remains a notoriously difficult task in Music Information Retrieval. While other tasks, such as beat tracking and chord recognition have seen improvement with the appli-cation of deep learning models, little work has been done to apply deep learning methods to fundamental frequency related tasks including multi-f 0 and melody tracking, pri-marily due to the scarce availability of labeled data. In this work, we describe a fully convolutional neural network for learning salience representations for estimating fundamen-tal frequencies, trained using a large, semi-automatically generated f 0 dataset. We demonstrate the effectiveness of our model for learning salience representations for both multi-f 0 and melody tracking in polyphonic audio, and show that our models achieve state-of-the-art performance on several multi-f 0 and melody datasets. We conclude with directions for future research.},
author = {Bittner, Rachel M. and Mcfee, Brian and Salamon, Justin and Li, Peter and Bello, Juan P.},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Deep Salience Representations for F0 Estimation in Polyphonic Music.pdf:pdf},
pages = {23--27},
title = {{Deep Salience Representations for F0 Estimation in Polyphonic Music}},
url = {https://bmcfee.github.io/papers/ismir2017_salience.pdf},
year = {2017}
}
@inproceedings{boersma1993praat,
author = {Boersma, Paul},
booktitle = {Proceedings of the Institute of Phonetic Sciences},
doi = {10.1371/journal.pone.0069107},
file = {:C\:/Users/jongwook/Dropbox/References/Accurate Short-Term Analysis of the Fundamental Frequency and the Harmonics-to-Noise Ratio of a Sampled Sound.pdf:pdf},
pages = {97--110},
title = {{Accurate Short-Term Analysis of the Fundamental Frequency and the Harmonics-To-Noise Ratio of a Sampled Sound}},
volume = {17},
year = {1993}
}
@inproceedings{miyato2018spectral,
author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:C\:/Users/jongwook/Dropbox/References/Spectral Normalization for Generative Adversarial Networks.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Spectral Normalization for Generative Adversarial Networks}},
year = {2018}
}
@misc{lecun1998mnist,
author = {LeCun, Yann and Cortes, Corinna and Burges, Christopher J C},
title = {{The MNIST Database of Handwritten Digits}},
url = {http://yann.lecun.com/exdb/mnist/index.html},
year = {1998}
}
@article{bordes2017draw,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.01722},
author = {Wang, Dilin and Liu, Qiang},
eprint = {arXiv:1611.01722},
file = {:C\:/Users/jongwook/Dropbox/References/Learning to Draw Samples With Application to Amortized MLE for Generative Adversarial Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1611.01722},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning}},
year = {2016}
}
@inproceedings{gregor2013darn,
archivePrefix = {arXiv},
arxivId = {1310.8499},
author = {Gregor, Karol and Danihelka, Ivo and Mnih, Andriy and Blundell, Charles and Wierstra, Daan},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1310.8499},
file = {:C\:/Users/jongwook/Dropbox/References/Deep AutoRegressive Networks.pdf:pdf},
isbn = {9781634393973},
title = {{Deep AutoRegressive Networks}},
url = {http://arxiv.org/abs/1310.8499},
year = {2014}
}
@inproceedings{ronneberger2015unet,
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention {(MICCAI)}},
editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M and Frangi, Alejandro F},
file = {:C\:/Users/jongwook/Dropbox/References/U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:pdf},
isbn = {978-3-319-24574-4},
pages = {234--241},
publisher = {Springer International Publishing},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
year = {2015}
}
@inproceedings{miron2017sourceseparation,
abstract = {Score information has been shown to improve music source separation when included into non-negative matrix factorization (NMF) frameworks. Recently, deep learning approaches have outperformed NMF methods in terms of separation quality and processing time, and there is scope to extend them with score information. In this paper, we propose a score-informed separation system for classical music that is based on deep learning. We propose a method to derive training features from audio files and the corre-sponding coarsely aligned scores for a set of classical mu-sic pieces. Additionally, we introduce a convolutional neu-ral network architecture (CNN) with the goal of estimating time-frequency masks for source separation. Our system is trained with synthetic renditions derived from the original scores and can be used to separate real-life performances based on the same scores, provided a coarse audio-to-score alignment. The proposed system achieves better perfor-mance (SDR and SIR) and is less computationally inten-sive than a score-informed NMF system on a dataset com-prising Bach chorales.},
author = {Miron, Marius and Janer, Jordi and G{\'{o}}mez, Emilia},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Monaural Score-Informed Source Separation for Classical Music Using Convolutional Neural Networks.pdf:pdf},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {55--62},
title = {{Monaural Score-Informed Source Separation for Classical Music Using Convolutional Neural Networks}},
url = {https://ismir2017.smcnus.org/wp-content/uploads/2017/10/51_Paper.pdf},
year = {2017}
}
@book{hartmann1997signals,
author = {Hartmann, William M},
file = {:C\:/Users/jongwook/Dropbox/References/Signals, Sound, and Sensation.pdf:pdf},
isbn = {1563962837},
publisher = {Springer},
title = {{Signals, Sound, and Sensation}},
year = {1997}
}
@article{goodfellow2016gan,
abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
archivePrefix = {arXiv},
arxivId = {1701.00160},
author = {Goodfellow, Ian},
doi = {10.1001/jamainternmed.2016.8245},
eprint = {1701.00160},
file = {:C\:/Users/jongwook/Dropbox/References/NIPS 2016 Tutorial Generative Adversarial Networks.pdf:pdf},
isbn = {1581138285},
issn = {0253-0465},
journal = {arXiv preprint arXiv:1701.00160},
pmid = {15040217},
title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1701.00160},
year = {2016}
}
@article{mehri2016samplernn,
author = {Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
file = {:C\:/Users/jongwook/Dropbox/References/SampleRNN An Unconditional End-to-End Neural Audio Generation Model.pdf:pdf},
journal = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
keywords = {RNN},
mendeley-tags = {RNN},
title = {{SampleRNN: An Unconditional End-to-End Neural Audio Generation Model}},
year = {2017}
}
@article{salamon2014melody,
author = {Salamon, Justin and G{\'{o}}mez, Emilia and Ellis, Daniel P W and Richard, Ga{\"{e}}l},
file = {:C\:/Users/jongwook/Dropbox/References/Melody Extraction from Polyphonic Music Signals Approaches, Applications, and Challenges.pdf:pdf},
journal = {{IEEE} Signal Processing Magazine},
number = {2},
pages = {118--134},
title = {{Melody Extraction from Polyphonic Music Signals: Approaches, Applications, and Challenges}},
volume = {31},
year = {2014}
}
@inproceedings{denton2015lapgan,
abstract = {In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach (Goodfellow et al.). Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.},
archivePrefix = {arXiv},
arxivId = {1506.05751},
author = {Denton, Emily L and Chintala, Soumith and Szlam, Arthur and Fergus, Rob and Others},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1506.05751},
file = {:C\:/Users/jongwook/Dropbox/References/Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks.pdf:pdf},
isbn = {1505.05770},
issn = {10495258},
keywords = {GAN,LAPGAN},
mendeley-tags = {GAN,LAPGAN},
pages = {1486--1494},
title = {{Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks}},
url = {http://arxiv.org/abs/1506.05751},
year = {2015}
}
@inproceedings{sonderby2016amortised,
abstract = {Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, MAP inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.},
archivePrefix = {arXiv},
arxivId = {1610.04490},
author = {S{\o}nderby, Casper Kaae and Caballero, Jose and Theis, Lucas and Shi, Wenzhe and Husz{\'{a}}r, Ferenc},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
doi = {10.1007/s00138-014-0623-4},
eprint = {1610.04490},
file = {:C\:/Users/jongwook/Dropbox/References/Amortised MAP Inference for Image Super-resolution.pdf:pdf},
isbn = {9783901608353},
issn = {14321769},
keywords = {GAN,Super-Resolution},
mendeley-tags = {GAN,Super-Resolution},
title = {{Amortised MAP Inference for Image Super-resolution}},
url = {http://arxiv.org/abs/1610.04490},
year = {2017}
}
@inproceedings{karpathy2017desc,
archivePrefix = {arXiv},
arxivId = {1412.2306},
author = {Karpathy, Andrej and Fei-Fei, Li},
booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}},
doi = {10.1109/TPAMI.2016.2598339},
eprint = {1412.2306},
file = {:C\:/Users/jongwook/Dropbox/References/Deep Visual-Semantic Alignments for Generating Image Descriptions.pdf:pdf},
isbn = {9781467369640},
issn = {01628828},
keywords = {Image captioning,deep neural networks,language model,recurrent neural network,visual-semantic embeddings},
pmid = {16873662},
title = {{Deep Visual-Semantic Alignments for Generating Image Descriptions}},
year = {2017}
}
@article{chen2018tcvae,
abstract = {We decompose the evidence lower bound to show the existence of a term measuring the total cor-relation between latent variables. We use this to motivate our $\beta$-TCVAE (Total Correlation Varia-tional Autoencoder), a refinement of the state-of-the-art $\beta$-VAE objective for learning disentangled representations, requiring no additional hyperpa-rameters during training. We further propose a principled classifier-free measure of disentangle-ment called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the latent variables model is trained using our framework.},
archivePrefix = {arXiv},
arxivId = {1802.04942},
author = {Chen, Tian Qi and Li, Xuechen and Grosse, Roger and Duvenaud, David},
eprint = {1802.04942},
file = {:C\:/Users/jongwook/Dropbox/References/Isolating Sources of Disentanglement in Variational Autoencoders.pdf:pdf},
journal = {arXiv preprint arXiv:1802.04942},
keywords = {VAE},
mendeley-tags = {VAE},
title = {{Isolating Sources of Disentanglement in Variational Autoencoders}},
url = {https://arxiv.org/pdf/1802.04942.pdf},
year = {2018}
}
@inproceedings{theis2015ride,
abstract = {Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.},
archivePrefix = {arXiv},
arxivId = {1506.03478},
author = {Theis, Lucas and Bethge, Matthias},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1506.03478},
file = {:C\:/Users/jongwook/Dropbox/References/Generative Image Modeling Using Spatial LSTMs.pdf:pdf},
issn = {10495258},
title = {{Generative Image Modeling Using Spatial LSTMs}},
url = {http://arxiv.org/abs/1506.03478},
year = {2015}
}
@inproceedings{li2017vibrato,
author = {Li, Bochen and Dinesh, Karthik and Sharma, Gaurav and Duan, Zhiyao},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Video-Based Vibrato Detection and Analysis for Polyphonic String Music.pdf:pdf},
pages = {123--130},
title = {{Video-Based Vibrato Detection and Analysis for Polyphonic String Music}},
year = {2017}
}
@inproceedings{pons2017timbre,
abstract = {The focus of this work is to study how to efficiently tailor Convolutional Neural Networks (CNNs) towards learning timbre representations from log-mel magnitude spectrograms. We first review the trends when designing CNN architectures. Through this literature overview we discuss which are the crucial points to consider for efficiently learning timbre representations using CNNs. From this discussion we propose a design strategy meant to capture the relevant time-frequency contexts for learning timbre, which permits using domain knowledge for designing architectures. In addition, one of our main goals is to design efficient CNN architectures -- what reduces the risk of these models to over-fit, since CNNs' number of parameters is minimized. Several architectures based on the design principles we propose are successfully assessed for different research tasks related to timbre: singing voice phoneme classification, musical instrument recognition and music auto-tagging.},
archivePrefix = {arXiv},
arxivId = {1703.06697},
author = {Pons, Jordi and Slizovskaia, Olga and Gong, Rong and G{\'{o}}mez, Emilia and Serra, Xavier},
booktitle = {Proceedings of the European Signal Processing Conference {(EUSIPCO)}},
eprint = {1703.06697},
file = {:C\:/Users/jongwook/Dropbox/References/Timbre Analysis of Music Audio Signals with Convolutional Neural Networks.pdf:pdf},
isbn = {9780992862671},
keywords = {CNN,Timbre},
mendeley-tags = {CNN,Timbre},
title = {{Timbre Analysis of Music Audio Signals with Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1703.06697},
year = {2017}
}
@article{makhzani2015aae,
abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
archivePrefix = {arXiv},
arxivId = {1511.05644},
author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
eprint = {1511.05644},
file = {:C\:/Users/jongwook/Dropbox/References/Adversarial Autoencoders.pdf:pdf},
isbn = {9781509008063},
journal = {arXiv preprint arXiv:1511.05644},
title = {{Adversarial Autoencoders}},
url = {http://arxiv.org/abs/1511.05644},
year = {2015}
}
@misc{chollet2015keras,
author = {Chollet, Fran{\c{c}}ois},
title = {{Keras: The Python Deep Learning Library}},
url = {https://keras.io}
}
@article{ng2011sparse,
author = {Ng, Andrew},
file = {:C\:/Users/jongwook/Dropbox/References/Sparse Autoencoder.pdf:pdf},
journal = {Stanford {CS294a} Lecture notes},
keywords = {Auto-Encoders},
mendeley-tags = {Auto-Encoders},
title = {{Sparse Autoencoder}},
year = {2011}
}
@article{saito2017gan,
abstract = {A method for statistical parametric speech synthesis incorporating generative adversarial networks (GANs) is proposed. Although powerful deep neural networks (DNNs) techniques can be applied to artificially synthesize speech waveform, the synthetic speech quality is low compared with that of natural speech. One of the issues causing the quality degradation is an over-smoothing effect often observed in the generated speech parameters. A GAN introduced in this paper consists of two neural networks: a discriminator to distinguish natural and generated samples, and a generator to deceive the discriminator. In the proposed framework incorporating the GANs, the discriminator is trained to distinguish natural and generated speech parameters, while the acoustic models are trained to minimize the weighted sum of the conventional minimum generation loss and an adversarial loss for deceiving the discriminator. Since the objective of the GANs is to minimize the divergence (i.e., distribution difference) between the natural and generated speech parameters, the proposed method effectively alleviates the over-smoothing effect on the generated speech parameters. We evaluated the effectiveness for text-to-speech and voice conversion, and found that the proposed method can generate more natural spectral parameters and $F_0$ than conventional minimum generation error training algorithm regardless its hyper-parameter settings. Furthermore, we investigated the effect of the divergence of various GANs, and found that a Wasserstein GAN minimizing the Earth-Mover's distance works the best in terms of improving synthetic speech quality.},
archivePrefix = {arXiv},
arxivId = {1709.08041},
author = {Saito, Yuki and Takamichi, Shinnosuke and Saruwatari, Hiroshi},
doi = {10.1109/TASLP.2017.2761547},
eprint = {1709.08041},
file = {:C\:/Users/jongwook/Dropbox/References/Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks.pdf:pdf},
issn = {2329-9290},
journal = {{IEEE/ACM} Transactions on Audio, Speech, and Language Processing},
keywords = {GAN,Speech},
mendeley-tags = {GAN,Speech},
number = {1},
pages = {84--96},
title = {{Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1709.08041},
volume = {26},
year = {2017}
}
@inproceedings{mauch2015computer,
author = {Mauch, Matthias and Cannam, Chris and Bittner, Rachel and Fazekas, George and Salamon, Justin and Dai, Jiajie and Bello, Juan and Dixon, Simon},
booktitle = {Proceedings of the First International Conference on Technologies for Music Notation and Representation},
doi = {10.1121/1.4881915},
file = {:C\:/Users/jongwook/Dropbox/References/Computer-Aided Melody Note Transcription Using the Tony Software Accuracy and Efficiency.pdf:pdf},
pages = {8},
title = {{Computer-Aided Melody Note Transcription Using the Tony Software: Accuracy and Efficiency}},
url = {https://code.soundsoftware.ac.uk/projects/tony/},
year = {2015}
}
@book{russell2009ai,
author = {Russell, Stuart J and Norvig, Peter},
file = {:C\:/Users/jongwook/Dropbox/References/Artificial Intelligence a Modern Approach (3rd Edition).pdf:pdf},
keywords = {AI},
mendeley-tags = {AI},
publisher = {Pearson},
title = {{Artificial Intelligence: a Modern Approach (3rd Edition)}},
year = {2009}
}
@inproceedings{mao2017lsgan,
abstract = {Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.},
archivePrefix = {arXiv},
arxivId = {1611.04076},
author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
booktitle = {Proceedings of the {IEEE} International Conference on Computer Vision {(ICCV)}},
doi = {10.1109/ICCV.2017.304},
eprint = {1611.04076},
file = {:C\:/Users/jongwook/Dropbox/References/Least Squares Generative Adversarial Networks.pdf:pdf},
isbn = {978-1-5386-1032-9},
issn = {15505499},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Least Squares Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1611.04076},
year = {2017}
}
@inproceedings{scholz2016mirex,
abstract = {In the last 20 years, Music Information Retrieval (MIR) has been an expanding research field, and the MIREX competition has become the main evaluation venue in MIR field. Analyzing recent results for various tasks of MIREX (MIR Evaluation eXchange), we observed that the evolution of task solutions follows two different patterns: for some tasks, the results apparently hit stagnation, whereas for others, they seem getting better over time. In this paper, (a) we compile the MIREX results of the last 6 years, (b) we propose a configurable quantitative index for evolution trend measurement of MIREX tasks, and (c) we discuss possible explanations or hypotheses for the stagnation phenomena hitting some of them. This paper hopes to incite a debate in the MIR research community about the progress in the field and how to adequately measure evolution trends.},
author = {Scholz, Ricardo and Ramalho, Geber and Cabral, Giordano},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Cross Task Study on {MIREX} Recent Results An Index for Evolution Measurement and Some Stagnation Hypotheses.pdf:pdf},
pages = {372--378},
title = {{Cross Task Study on {MIREX} Recent Results: An Index for Evolution Measurement and Some Stagnation Hypotheses}},
url = {https://www.researchgate.net/profile/Ricardo_Scholz3/publication/303558629_Cross_Task_Study_on_MIREX_Recent_Results_an_Index_for_Evolution_Measurement_and_Some_Stagnation_Hypotheses/links/57ecf3a108ae93b7fa95aec3.pdf https://wp.nyu.edu/ismir2016/wp-conten},
year = {2016}
}
@inproceedings{huang2017counterpoint,
author = {Huang, Cheng-Zhi Anna and Cooijmans, Tim and Roberts, Adam and Courville, Aaron and Eck, Douglas},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Counterpoint by Convolution.pdf:pdf},
keywords = {Machine learning models of music typically break u,and explore the use of blocked Gibbs sampling as a,based on both log-likelihood and human evaluation.,compos- ing a piece of music in a single pass from,human composers write music in a nonlinear fashion,of- ten revisiting choices previously made. In ord,scribbling motifs here and there,we find that Gibbs sampling greatly improves sampl,we show that even the cheap approximate blocked Gi,we train a convolutional neural network to complet,which allows more direct ancestral sampling. Howev,which we demonstrate to be due to some conditional},
pages = {211--218},
title = {{Counterpoint by Convolution}},
year = {2017}
}
@article{belghazi2018hali,
archivePrefix = {arXiv},
arxivId = {1802.01071},
author = {Belghazi, Mohamed Ishmael and Rajeswar, Sai and Mastropietro, Olivier and Rostamzadeh, Negar and Mitrovic, Jovana and Courville, Aaron},
eprint = {1802.01071},
file = {:C\:/Users/jongwook/Dropbox/References/Hierarchical Adversarially Learned Inference.pdf:pdf},
journal = {arXiv preprint arXiv:1802.01071},
title = {{Hierarchical Adversarially Learned Inference}},
url = {http://arxiv.org/abs/1802.01071},
year = {2018}
}
@inproceedings{goto2002rwc,
author = {Goto, Masataka and Hashiguchi, Hiroki and Nishimura, Takuichi and Oka, Ryuichi},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/RWC Music Database Popular, Classical, and Jazz Music Databases.pdf:pdf},
isbn = {2844261663},
keywords = {Dataset},
mendeley-tags = {Dataset},
pages = {287--288},
title = {{RWC Music Database: Popular, Classical and Jazz Music Databases}},
year = {2002}
}
@inproceedings{liu2016cogan,
abstract = {We propose the coupled generative adversarial network (CoGAN) framework for generating pairs of corresponding images in two different domains. It consists of a pair of generative adversarial networks, each responsible for generating images in one domain. We show that by enforcing a simple weight-sharing constraint, the CoGAN learns to generate pairs of corresponding images without existence of any pairs of corresponding images in the two domains in the training set. In other words, the CoGAN learns a joint distribution of images in the two domains from images drawn separately from the marginal distributions of the individual domains. This is in contrast to the existing multi-modal generative models, which require corresponding images for training. We apply the CoGAN to several pair image generation tasks. For each task, the GoGAN learns to generate convincing pairs of corresponding images. We further demonstrate the applications of the CoGAN framework for the domain adaptation and cross-domain image generation tasks.},
archivePrefix = {arXiv},
arxivId = {1606.07536},
author = {Liu, Ming-Yu and Tuzel, Oncel},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {arXiv:1606.07536},
eprint = {1606.07536},
file = {:C\:/Users/jongwook/Dropbox/References/Coupled Generative Adversarial Networks.pdf:pdf},
isbn = {10495258},
issn = {10495258},
pages = {469--477},
title = {{Coupled Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1606.07536},
year = {2016}
}
@article{mnih2013atari,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
file = {:C\:/Users/jongwook/Dropbox/References/Playing Atari with Deep Reinforcement Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1312.5602},
keywords = {DQN,RL},
mendeley-tags = {DQN,RL},
title = {{Playing Atari with Deep Reinforcement Learning}},
year = {2013}
}
@article{li2017infinite,
archivePrefix = {arXiv},
arxivId = {1707.08438},
author = {Li, Samuel},
eprint = {1707.08438},
file = {:C\:/Users/jongwook/Dropbox/References/Context-Independent Polyphonic Piano Onset Transcription with an Infinite Training Dataset.pdf:pdf},
journal = {arXiv preprint arXiv:1707.08438},
title = {{Context-Independent Polyphonic Piano Onset Transcription with an Infinite Training Dataset}},
url = {https://arxiv.org/abs/1707.08438v1},
year = {2017}
}
@inproceedings{maezawa2017beat,
author = {Maezawa, Akira},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Fast and Accurate Improving a Simple Beat Tracker With a Selectively-Applied Deep Beat Identification.pdf:pdf},
keywords = {Beat Tracking},
mendeley-tags = {Beat Tracking},
pages = {309--315},
title = {{Fast and Accurate: Improving a Simple Beat Tracker With a Selectively-Applied Deep Beat Identification}},
year = {2017}
}
@article{mahendran2016deepdream,
author = {Mahendran, Aravindh and Vedaldi, Andrea},
file = {:C\:/Users/jongwook/Dropbox/References/Visualizing Deep Convolutional Neural Networks using Natural Pre-Images.pdf:pdf},
journal = {International Journal of Computer Vision},
number = {3},
pages = {233--255},
publisher = {Springer},
title = {{Visualizing Deep Convolutional Neural Networks using Natural Pre-Images}},
volume = {120},
year = {2016}
}
@inproceedings{rezende2014backprop,
archivePrefix = {arXiv},
arxivId = {1401.4082},
author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
doi = {10.1051/0004-6361/201527329},
eprint = {1401.4082},
file = {:C\:/Users/jongwook/Dropbox/References/Stochastic Backpropagation and Approximate Inference in Deep Generative Models.pdf:pdf},
isbn = {9781634393973},
issn = {10495258},
pmid = {23459267},
title = {{Stochastic Backpropagation and Approximate Inference in Deep Generative Models}},
url = {http://arxiv.org/abs/1401.4082},
volume = {32},
year = {2014}
}
@inproceedings{mauch2013adt,
author = {Mauch, M and Ewert, S},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/The Audio Degradation Toolbox and its Application to Robustness Evaluation.pdf:pdf},
title = {{The Audio Degradation Toolbox and Its Application To Robustness Evaluation}},
year = {2013}
}
@inproceedings{schorkhuber2010cqt,
abstract = {This paper proposes a computationally efficient method for computing the constant-Q transform (CQT) of a time- domain signal. CQT refers to a time-frequency represen- tation where the frequency bins are geometrically spaced and the Q-factors (ratios of the center frequencies to band- widths) of all bins are equal. An inverse transform is pro- posed which enables a reasonable-quality (around 55dB signal-to-noise ratio) reconstruction of the original signal from its CQT coefficients. Here CQTs with high Q-factors, equivalent to 12–96 bins per octave, are of particular inter- est. The proposed method is flexible with regard to the number of bins per octave, the applied window function, and the Q-factor, and is particularly suitable for the anal- ysis of music signals. A reference implementation of the proposed methods is published as a Matlab toolbox. The toolbox includes user-interface tools that facilitate spectral data visualization and the indexing and working with the data structure produced by the CQT.},
author = {Sch{\"{o}}rkhuber, Christian and Klapuri, Anssi},
booktitle = {Proceedings of the Sound and Music Computing {(SMC)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Constant-Q Transform Toolbox for Music Processing.pdf:pdf},
keywords = {CQT},
mendeley-tags = {CQT},
number = {JANUARY},
pages = {3--64},
title = {{Constant-Q Transform Toolbox for Music Processing}},
year = {2010}
}
@article{wang2017tacotron,
abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given <text, audio> pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
archivePrefix = {arXiv},
arxivId = {1703.10135},
author = {Wang, Yuxuan and Skerry-Ryan, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Others},
doi = {10.21437/Interspeech.2017-1452},
eprint = {1703.10135},
file = {:C\:/Users/jongwook/Dropbox/References/Tacotron A Fully End-to-End Text-To-Speech Synthesis Model.pdf:pdf},
issn = {19909772},
journal = {arXiv preprint arXiv:1703.10135},
keywords = {Sequence-To-sequence,Text-To-speech synthesis,end-To-end model.},
pages = {4006--4010},
title = {{Tacotron: A Fully End-to-End Text-To-Speech Synthesis Model}},
volume = {2017-Augus},
year = {2017}
}
@book{patel2010musiclanguage,
author = {Patel, Aniruddh D},
file = {:C\:/Users/jongwook/Dropbox/References/Music, Language, and the Brain.pdf:pdf},
publisher = {Oxford university press},
title = {{Music, Language, and the Brain}},
year = {2010}
}
@inproceedings{karras2017pggan,
abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
archivePrefix = {arXiv},
arxivId = {1710.10196},
author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
doi = {10.1002/joe.20070},
eprint = {1710.10196},
file = {:C\:/Users/jongwook/Dropbox/References/Progressive Growing of GANs for Improved Quality, Stability, and Variation.pdf:pdf},
isbn = {9789186871208},
issn = {15311864},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Progressive Growing of GANs for Improved Quality, Stability, and Variation}},
url = {http://arxiv.org/abs/1710.10196},
year = {2018}
}
@inproceedings{zhang2018mixup,
abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
archivePrefix = {arXiv},
arxivId = {1710.09412},
author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1710.09412},
file = {:C\:/Users/jongwook/Dropbox/References/mixup Beyond Empirical Risk Minimization.pdf:pdf},
title = {{mixup: Beyond Empirical Risk Minimization}},
url = {http://arxiv.org/abs/1710.09412},
year = {2018}
}
@inproceedings{sutskever2014seq2seq,
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
file = {:C\:/Users/jongwook/Dropbox/References/Sequence to sequence learning with neural networks.pdf:pdf},
keywords = {RNN},
mendeley-tags = {RNN},
pages = {3104--3112},
title = {{Sequence to sequence learning with neural networks}},
year = {2014}
}
@inproceedings{sarroff2014synthesis,
author = {Sarroff, Andy M and Casey, Michael A},
booktitle = {Proceedings of the International Computer Music Conference {(ICMC)}},
file = {:C\:/Users/jongwook/Dropbox/References/Musical Audio Synthesis Using Autoencoding Neural Nets.pdf:pdf},
keywords = {Synthesis},
mendeley-tags = {Synthesis},
title = {{Musical Audio Synthesis Using Autoencoding Neural Nets}},
year = {2014}
}
@inproceedings{molina2014humming,
abstract = {In this paper, we present a comparative study of several state-of-the-art F0 trackers applied to the context of query- by-singing-humming (QBSH). This study has been carried out using the well known, freely available, MIR-QBSH dataset in different conditions of added pub-style noise and smartphone-style distortion. For audio-to-MIDI melodic matching, we have used two state-of-the-art systems and a simple, easily reproducible baselinemethod. For the evalu- ation, we measured the QBSH performance for 189 differ- ent combinations of F0 tracker, noise/distortion conditions and matcher. Additionally, the overall accuracy of the F0 transcriptions (as defined in MIREX) was also measured. In the results, we found that F0 tracking overall accuracy correlates with QBSH performance, but it does not totally measure the suitability of a pitch vector for QBSH. In ad- dition, we also found clear differences in robustness to F0 transcription errors between differentmatchers.},
author = {Molina, Emilio and Tard{\'{o}}n, Lorenzo J. and Barbancho, Isabel and Barbancho, Ana M.},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/The Importance of F0 Tracking in Query-by-singing-humming.pdf:pdf},
keywords = {F0},
mendeley-tags = {F0},
pages = {277--282},
title = {{The Importance of F0 Tracking in Query-by-singing-humming.}},
year = {2014}
}
@inproceedings{dumoulin2017ali,
abstract = {We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.},
archivePrefix = {arXiv},
arxivId = {1606.00704},
author = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Mastropietro, Olivier and Lamb, Alex and Arjovsky, Martin and Courville, Aaron and Mastropietro, Olivier and Courville, Aaron},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1606.00704},
file = {:C\:/Users/jongwook/Dropbox/References/Adversarially Learned Inference.pdf:pdf},
keywords = {ALI,GAN},
mendeley-tags = {ALI,GAN},
title = {{Adversarially Learned Inference}},
url = {http://arxiv.org/abs/1606.00704},
year = {2017}
}
@inproceedings{dieleman2013multiscale,
abstract = {Content-based music information retrieval tasks are typi- cally solved with a two-stage approach: features are ex- tracted from music audio signals, and are then used as in- put to a regressor or classifier. These features can be engi- neered or learned from data. Although the former approach was dominant in the past, feature learning has started to receive more attention from the MIR community in re- cent years. Recent results in feature learning indicate that simple algorithms such as K-means can be very effective, sometimes surpassing more complicated approaches based on restricted Boltzmann machines, autoencoders or sparse coding. Furthermore, there has been increased interest in multiscale representations of music audio recently. Such representations are more versatile because music audio ex- hibits structure on multiple timescales, which are relevant for different MIR tasks to varying degrees. We develop and compare three approaches to multiscale audio feature learning using the spherical K-means algorithm. We evalu- ate them in an automatic tagging task and a similarity met- ric learning task on the Magnatagatune dataset.},
author = {Dieleman, Sander and Schrauwen, Benjamin},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
doi = {10.1109/IJCNN.2005.1556436},
file = {:C\:/Users/jongwook/Dropbox/References/Multiscale Approaches to Music Audio Feature Learning.pdf:pdf},
isbn = {9780615900650},
issn = {14673045},
keywords = {Features,K,Multiscale,Technology and Engineering,feature learning,means,multiple timescales,music information retrieval},
mendeley-tags = {Features,Multiscale},
pages = {116--121},
pmid = {21555788},
title = {{Multiscale Approaches to Music Audio Feature Learning}},
year = {2013}
}
@inproceedings{bello2005chromagram,
author = {Bello, Juan P and Pickens, Jeremy},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/A Robust Mid-Level Representation for Harmonic Content in Music Signals.pdf:pdf},
isbn = {0955117909},
keywords = {Harmonic description,music similarity,segmentation},
pages = {304--311},
title = {{A Robust Mid-Level Representation for Harmonic Content in Music Signals}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.59.6957&rep=rep1&type=pdf},
year = {2005}
}
@inproceedings{babacan2013comparative,
author = {Babacan, Onur and Drugman, Thomas and D'Alessandro, Nicolas and Henrich, Nathalie and Dutoit, Thierry},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:C\:/Users/jongwook/Dropbox/References/A Comparative Study of Pitch Extraction Algorithms on a Large Variety of Singing Sounds.pdf:pdf},
isbn = {9781479903566},
pages = {7815--7819},
title = {{A Comparative Study of Pitch Extraction Algorithms on a Large Variety of Singing Sounds}},
year = {2013}
}
@article{rabiner1989hmm,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rabiner, L.R.},
doi = {10.1109/5.18626},
eprint = {arXiv:1011.1669v3},
file = {:C\:/Users/jongwook/Dropbox/References/A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the {IEEE}},
number = {2},
pages = {257--286},
pmid = {18626},
title = {{A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition}},
url = {http://ieeexplore.ieee.org/ielx5/5/698/00018626.pdf?tp=&arnumber=18626&isnumber=698%5Cnhttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=18626&tag=1%0Ahttp://ieeexplore.ieee.org/document/18626/},
volume = {77},
year = {1989}
}
@inproceedings{benetos2015probabilistic,
author = {Benetos, Emmanouil and Weyde, Tillman},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/An Efficient Temporally-Constrained Probabilistic Model for Multiple-Instrument Music Transcription.pdf:pdf},
pages = {701--707},
title = {{An Efficient Temporally-Constrained Probabilistic Model for Multiple-Instrument Music Transcription}},
year = {2015}
}
@article{rebelo2012omr,
author = {Rebelo, Ana and Fujinaga, Ichiro and Paszkiewicz, Filipe and Marcal, Andre R.S. and Guedes, Carlos and Cardoso, Jaime S.},
doi = {10.1007/s13735-012-0004-6},
file = {:C\:/Users/jongwook/Dropbox/References/Optical Music Recognition State-of-the-Art and Open Issues.pdf:pdf},
isbn = {1373501200},
issn = {2192662X},
journal = {International Journal of Multimedia Information Retrieval},
keywords = {Computer music,Image processing,Machine learning,Music performance},
number = {3},
pages = {173--190},
title = {{Optical Music Recognition State-of-the-Art and Open Issues}},
volume = {1},
year = {2012}
}
@inproceedings{ycart2017sequence,
author = {Ycart, Adrien and Benetos, Emmanouil},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/A study on LSTM networks for polyphonic music sequence modelling.pdf:pdf},
keywords = {LSTM,Transcription},
mendeley-tags = {LSTM,Transcription},
pages = {421--427},
title = {{A study on LSTM networks for polyphonic music sequence modelling}},
year = {2017}
}
@inproceedings{southall2017drum,
abstract = {Automatic drum transcription is the process of generating symbolic notation for percussion instruments within audio recordings. To date, recurrent neural network (RNN) sys-tems have achieved the highest evaluation accuracies for both drum solo and polyphonic recordings, however the ac-curacies within a polyphonic context still remain relatively low. To improve accuracy for polyphonic recordings, we present two approaches to the ADT problem: First, to cap-ture the dynamism of features in multiple time-step hidden layers, we propose the use of soft attention mechanisms (SA) and an alternative RNN configuration containing ad-ditional peripheral connections (PC). Second, to capture these same trends at the input level, we propose the use of a convolutional neural network (CNN), which uses a larger set of time-step features. In addition, we propose the use of a bidirectional recurrent neural network (BRNN) in the peak-picking stage. The proposed systems are evalu-ated along with two state-of-the-art ADT systems in five evaluation scenarios, including a newly-proposed evalua-tion methodology designed to assess the generalisability of ADT systems. The results indicate that all of the newly proposed systems achieve higher accuracies than the state-of-the-art RNN systems for polyphonic recordings and that the additional BRNN peak-picking stage offers slight im-provement in certain contexts.},
author = {Southall, Carl and Stables, Ryan and Hockman, Jason},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Automatic Drum Transcription for Polyphonic Recordings using Soft Attention Mechanisms and Convolutional Neural Networks.pdf:pdf},
keywords = {Attention,Transcription},
mendeley-tags = {Attention,Transcription},
pages = {606--612},
title = {{Automatic Drum Transcription for Polyphonic Recordings using Soft Attention Mechanisms and Convolutional Neural Networks}},
year = {2017}
}
@article{choi2016crnn,
abstract = {We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with three CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show a strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.},
archivePrefix = {arXiv},
arxivId = {1609.04243},
author = {Choi, Keunwoo and Fazekas, George and Sandler, Mark and Cho, Kyunghyun},
doi = {10.1.1.302.7795},
eprint = {1609.04243},
file = {:C\:/Users/jongwook/Dropbox/References/Convolutional Recurrent Neural Networks for Music Classification.pdf:pdf},
isbn = {9789881701282},
issn = {15209210},
journal = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
keywords = {CRNN},
mendeley-tags = {CRNN},
pages = {1--5},
title = {{Convolutional Recurrent Neural Networks for Music Classification}},
url = {http://arxiv.org/abs/1609.04243},
year = {2016}
}
@article{hochreiter1997lstm,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
file = {:C\:/Users/jongwook/Dropbox/References/Long Short-Term Memory.pdf:pdf},
journal = {Neural Computation},
keywords = {LSTM,RNN},
mendeley-tags = {LSTM,RNN},
number = {8},
pages = {1735--1780},
publisher = {MIT Press},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@article{xu2015leakyrelu,
abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68\% accuracy on CIFAR-100 test set without multiple test or ensemble.},
archivePrefix = {arXiv},
arxivId = {1505.00853},
author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
eprint = {1505.00853},
file = {:C\:/Users/jongwook/Dropbox/References/Empirical Evaluation of Rectified Activations in Convolutional Network.pdf:pdf},
journal = {arXiv preprint arXiv:1505.00853},
keywords = {Activations},
mendeley-tags = {Activations},
title = {{Empirical Evaluation of Rectified Activations in Convolutional Network}},
url = {http://arxiv.org/abs/1505.00853},
year = {2015}
}
@article{lucic2017gan,
abstract = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the original one.},
archivePrefix = {arXiv},
arxivId = {1711.10337},
author = {Lucic, Mario and Kurach, Karol and Michalski, Marcin and Gelly, Sylvain and Bousquet, Olivier},
eprint = {1711.10337},
file = {:C\:/Users/jongwook/Dropbox/References/Are GANs Created Equal A Large-Scale Study.pdf:pdf},
journal = {arXiv preprint arXiv:1711.10337},
keywords = {GAN,Survey},
mendeley-tags = {GAN,Survey},
title = {{Are GANs Created Equal? A Large-Scale Study}},
url = {http://arxiv.org/abs/1711.10337},
year = {2017}
}
@article{bock2012rnn,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {B{\"{o}}ck, Sebastian and Schedl, Markus},
doi = {10.1109/ICASSP.2012.6287832},
eprint = {arXiv:1011.1669v3},
file = {:C\:/Users/jongwook/Dropbox/References/Polyphonic Piano Note Transcription with Recurrent Neural Networks.pdf:pdf},
isbn = {9781467300469},
issn = {15206149},
journal = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
keywords = {music information retrieval,neural networks},
pages = {121--124},
pmid = {15664853},
title = {{Polyphonic Piano Note Transcription with Recurrent Neural Networks}},
year = {2012}
}
@inproceedings{kilcher2017interpolation,
abstract = {In implicit models, one often interpolates between sampled points in latent space. As we show in this paper, care needs to be taken to match-up the distributional assumptions on code vectors with the geometry of the interpolating paths. Otherwise, typical assumptions about the quality and semantics of in-between points may not be justified. Based on our analysis we propose to modify the prior code distribution to put significantly more probability mass closer to the origin. As a result, linear interpolation paths are not only shortest paths, but they are also guaranteed to pass through high-density regions, irrespective of the dimensionality of the latent space. Experiments on standard benchmark image datasets demonstrate clear visual improvements in the quality of the generated samples and exhibit more meaningful interpolation paths.},
archivePrefix = {arXiv},
arxivId = {1710.11381},
author = {Kilcher, Yannic and Lucchi, Aurelien and Hofmann, Thomas},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1710.11381},
file = {:C\:/Users/jongwook/Dropbox/References/Semantic Interpolation in Implicit Models.pdf:pdf},
number = {1},
pages = {1--28},
title = {{Semantic Interpolation in Implicit Models}},
url = {http://arxiv.org/abs/1710.11381},
year = {2017}
}
@article{shelhamer2017fcn,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
file = {:C\:/Users/jongwook/Dropbox/References/Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {01628828},
journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
number = {4},
pages = {640--651},
pmid = {16190471},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
volume = {39},
year = {2017}
}
@article{von2010comparison,
author = {von dem Knesebeck, Adrian and Z{\"{o}}lzer, U},
file = {:C\:/Users/jongwook/Dropbox/References/Comparison of Pitch Trackers for Real-Time Guitar Effects.pdf:pdf},
isbn = {9783200019409},
journal = {Proceedings of the International Conference on Digital Audio Effects {(DAFx)}},
title = {{Comparison of Pitch Trackers for Real-Time Guitar Effects}},
year = {2010}
}
@article{mcinnes2018umap,
abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP as described has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
archivePrefix = {arXiv},
arxivId = {1802.03426},
author = {McInnes, Leland and Healy, John},
eprint = {1802.03426},
file = {:C\:/Users/jongwook/Dropbox/References/UMAP Uniform Manifold Approximation and Projection for Dimension Reduction.pdf:pdf},
journal = {arXiv preprint arXiv:1802.03426},
title = {{UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}},
url = {http://arxiv.org/abs/1802.03426},
year = {2018}
}
@phdthesis{wang2017piano,
author = {Wang, Siying},
file = {:C\:/Users/jongwook/Dropbox/References/Computational Methods for the Alignment and Score-Informed Transcription of Piano Music.pdf:pdf},
school = {Queen Mary University of London},
title = {{Computational Methods for the Alignment and Score-Informed Transcription of Piano Music}},
type = {Doctoral Thesis},
year = {2017}
}
@article{creswell2017gan,
abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this through deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image super-resolution and classification. The aim of this review paper is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
archivePrefix = {arXiv},
arxivId = {1710.07035},
author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A},
doi = {10.1007/978-3-319-56991-8_32},
eprint = {1710.07035},
file = {:C\:/Users/jongwook/Dropbox/References/Generative Adversarial Networks An Overview.pdf:pdf},
isbn = {9781509011216},
issn = {1701.07274},
journal = {{IEEE} Signal Processing Magazine},
keywords = {GAN,Survey},
mendeley-tags = {GAN,Survey},
pages = {53--65},
pmid = {15040217},
title = {{Generative Adversarial Networks: An Overview}},
url = {http://arxiv.org/abs/1710.07035},
year = {2017}
}
@article{emiya2010multipitch,
author = {Emiya, Valentin and Badeau, Roland and David, Bertrand},
file = {:C\:/Users/jongwook/Dropbox/References/Multipitch Estimation of Piano Sounds Using a New Probabilistic Spectral Smoothness Principle.pdf:pdf},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
number = {6},
pages = {1643--1654},
publisher = {IEEE},
title = {{Multipitch Estimation of Piano Sounds Using a New Probabilistic Spectral Smoothness Principle}},
volume = {18},
year = {2010}
}
@inproceedings{kingma2015adam,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
file = {:C\:/Users/jongwook/Dropbox/References/Adam A Method for Stochastic Optimization.pdf:pdf},
isbn = {9781450300728},
issn = {09252312},
keywords = {Optimizer},
mendeley-tags = {Optimizer},
pages = {1--15},
pmid = {172668},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2015}
}
@article{bang2018rfgan,
abstract = {Despite of the success of Generative Adversarial Networks (GANs) for image generation tasks, the trade-off between image diversity and visual quality are an well-known issue. Conventional techniques achieve either visual quality or image diversity; the improvement in one side is often the result of sacrificing the degradation in the other side. In this paper, we aim to achieve both simultaneously by improving the stability of training GANs. A key idea of the proposed approach is to implicitly regularizing the discriminator using a representative feature. For that, this representative feature is extracted from the data distribution, and then transferred to the discriminator for enforcing slow updates of the gradient. Consequently, the entire training process is stabilized because the learning curve of discriminator varies slowly. Based on extensive evaluation, we demonstrate that our approach improves the visual quality and diversity of state-of-the art GANs.},
archivePrefix = {arXiv},
arxivId = {1801.09195},
author = {Bang, Duhyeon and Shim, Hyunjung},
eprint = {1801.09195},
file = {:C\:/Users/jongwook/Dropbox/References/Improved Training of Generative Adversarial Networks Using Representative Features.pdf:pdf},
journal = {arXiv preprint arXiv:1801.09195},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Improved Training of Generative Adversarial Networks Using Representative Features}},
url = {http://arxiv.org/abs/1801.09195},
year = {2018}
}
@article{odena2016acgan,
abstract = {Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.},
archivePrefix = {arXiv},
arxivId = {1610.09585},
author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
eprint = {1610.09585},
file = {:C\:/Users/jongwook/Dropbox/References/Conditional Image Synthesis With Auxiliary Classifier GANs.pdf:pdf},
issn = {1938-7228},
journal = {arXiv preprint arXiv:1610.09585},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Conditional Image Synthesis With Auxiliary Classifier GANs}},
url = {http://arxiv.org/abs/1610.09585},
year = {2016}
}
@article{mirza2014conditional,
abstract = {GAN;},
author = {Mirza, Mehdi and Osindero, Simon},
file = {:C\:/Users/jongwook/Dropbox/References/Conditional Generative Adversarial Nets.pdf:pdf},
journal = {arXiv preprint arXiv:1411.1784},
title = {{Conditional Generative Adversarial Nets}},
year = {2014}
}
@inproceedings{klambauer2017selu,
archivePrefix = {arXiv},
arxivId = {1706.02515},
author = {Klambauer, G{\"{u}}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {1706.02515},
eprint = {1706.02515},
file = {:C\:/Users/jongwook/Dropbox/References/Self-Normalizing Neural Networks.pdf:pdf},
title = {{Self-Normalizing Neural Networks}},
url = {http://arxiv.org/abs/1706.02515},
year = {2017}
}
@article{dozat2016nadam,
author = {Dozat, Timothy},
file = {:C\:/Users/jongwook/Dropbox/References/Incorporating Nesterov Momentum into Adam.pdf:pdf},
journal = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
title = {{Incorporating Nesterov Momentum into Adam}},
year = {2016}
}
@inproceedings{shi2016kalman,
abstract = {Dislocation motion in body centered cubic (bcc) metals displays a number of specific features that result in a strong temperature dependence of the flow stress, and in shear deformation asymmetries relative to the loading direction as well as crystal orientation. Here we develop a generalized dislocation mobility law in bcc metals, and demonstrate its use in discrete Dislocation Dynamics (DD) simulations of plastic flow in tungsten (W) micro pillars. We present the theoretical background for dislocation mobility as a motivating basis for the developed law. Analytical theory, molecular dynamics (MD) simulations, and experimental data are used to construct a general phenomenological description. The usefulness of the mobility law is demonstrated through its application to modeling the plastic deformation of W micro pillars. The model is consistent with experimental observations of temperature and orientation dependence of the flow stress and the corresponding dislocation microstructure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1608.01392v1},
author = {Shi, Liming and Nielsen, Jesper K. and Jensen, Jesper R. and Little, Max A. and Christensen, Mads G.},
booktitle = {Proceedings of the {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics {(WASPAA)}},
eprint = {arXiv:1608.01392v1},
file = {:C\:/Users/jongwook/Dropbox/References/A Kalman-Based Fundamental Frequency Estimation Algorithm.pdf:pdf},
isbn = {5889137697},
keywords = {F0,Kalman,high temperature corrosion,sem,superalloys,xrd},
mendeley-tags = {F0,Kalman},
title = {{A Kalman-Based Fundamental Frequency Estimation Algorithm}},
url = {https://arxiv.org/pdf/1608.01392.pdf},
year = {2017}
}
@inproceedings{khadkevich2009hmm,
author = {Khadkevich, M and Omologo, Maurizio},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Use of Hidden Markov Models and Factored Language Models for Automatic Chord Recognition.pdf:pdf},
isbn = {9780981353708},
pages = {561--566},
title = {{Use of Hidden Markov Models and Factored Language Models for Automatic Chord Recognition.}},
url = {http://ismir2009.ismir.net/proceedings/OS7-4.pdf},
year = {2009}
}
@book{bilmes1998gentle,
archivePrefix = {arXiv},
arxivId = {hep-ph/hep-ph/9605323},
author = {Bilmes, Jeff A.},
doi = {10.1080/0042098032000136147},
eprint = {hep-ph/9605323},
file = {:C\:/Users/jongwook/Dropbox/References/A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models.pdf:pdf},
isbn = {0226775429},
issn = {0042-0980},
pmid = {351},
primaryClass = {hep-ph},
title = {{A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.4498&rep=rep1&type=pdf},
year = {1998}
}
@inproceedings{ewert2017transcription,
abstract = {A central goal in automatic music transcription is to detect individual note events in music recordings. An important variant is instrument-dependent music transcription where methods can use calibration data for the instruments in use. However, despite the additional information, results rarely exceed an f-measure of 80%. As a potential explanation, the transcription problem can be shown to be badly conditioned and thus relies on appropriate regularization. A recently proposed method employs a mixture of simple, convex regularizers (to stabilize the parameter estimation process) and more complex terms (to encourage more meaningful structure). In this paper, we present two extensions to this method. First, we integrate a computational loudness model to better differentiate real from spurious note detections. Second, we employ (Bidirectional) Long Short Term Memory networks to re-weight the likelihood of detected note constellations. Despite their simplicity, our two extensions lead to a drop of about 35% in note error rate compared to the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1707.00160},
author = {Ewert, Sebastian and Sandler, Mark B.},
booktitle = {Proceedings of the {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics {(WASPAA)}},
eprint = {1707.00160},
file = {:C\:/Users/jongwook/Dropbox/References/An Augmented Lagrangian Method for Piano Transcription using Equal Loudness Thresholding and LSTM-based Decoding.pdf:pdf},
keywords = {LSTM,Piano,Transcription},
mendeley-tags = {LSTM,Piano,Transcription},
title = {{An Augmented Lagrangian Method for Piano Transcription using Equal Loudness Thresholding and LSTM-based Decoding}},
url = {http://arxiv.org/abs/1707.00160},
year = {2017}
}
@inproceedings{szegedy2015googlenet,
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}},
file = {:C\:/Users/jongwook/Dropbox/References/Going Deeper with Convolutions.pdf:pdf},
pages = {1--9},
title = {{Going Deeper with Convolutions}},
year = {2015}
}
@inproceedings{petzka2018regularization,
abstract = {This report has several purposes. First, our report is written to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). Second, among the experiments performed in the submitted paper, five aspects were emphasized and reproduced: learning speed, stability, robustness against hyperparameter, estimating the Wasserstein distance, and various sampling method. Finally, we identify which parts of the contribution can be reproduced, and at what cost in terms of resources. All source code for reproduction is open to the public.},
archivePrefix = {arXiv},
arxivId = {1712.05882},
author = {Petzka, Henning and Fischer, Asja and Lukovnicov, Denis},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1712.05882},
file = {:C\:/Users/jongwook/Dropbox/References/On the Regularization of Wasserstein GANs.pdf:pdf},
issn = {0002-3264},
keywords = {GAN},
title = {{On the Regularization of Wasserstein GANs}},
url = {http://arxiv.org/abs/1712.05882},
year = {2018}
}
@book{rosenblatt1957perceptron,
author = {Rosenblatt, Frank},
file = {:C\:/Users/jongwook/Dropbox/References/The Perceptron, a Perceiving and Recognizing Automaton (Project Para).pdf:pdf},
publisher = {Cornell Aeronautical Laboratory},
title = {{The Perceptron, a Perceiving and Recognizing Automaton (Project Para)}},
year = {1957}
}
@inproceedings{tikhonov2017generation,
archivePrefix = {arXiv},
arxivId = {1705.05458},
author = {Tikhonov, Alexey and Yamshchikov, Ivan P.},
booktitle = {Proceedings of the International Symposium on Computer Music Multidisciplinary Research {(CMMR)}},
eprint = {1705.05458},
file = {:C\:/Users/jongwook/Dropbox/References/Music Generation with Variational Recurrent Autoencoder Supported by History.pdf:pdf},
keywords = {artificial intelligence,variational recurrent autoencoder},
title = {{Music Generation with Variational Recurrent Autoencoder Supported by History}},
url = {http://arxiv.org/abs/1705.05458},
year = {2017}
}
@inproceedings{humphrey2012tonnetz,
author = {Humphrey, Eric J. and Cho, Taemin and Bello, Juan P.},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:C\:/Users/jongwook/Dropbox/References/Learning a Robust Tonnetz-Space Transform for Automatic Chord Recognition.pdf:pdf},
isbn = {9781467300469},
pages = {453--456},
title = {{Learning a Robust Tonnetz-Space Transform for Automatic Chord Recognition}},
year = {2012}
}
@inproceedings{he2015prelu,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the {IEEE} International Conference on Computer Vision {(ICCV)}},
file = {:C\:/Users/jongwook/Dropbox/References/Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf:pdf},
keywords = {Activations},
mendeley-tags = {Activations},
pages = {1026--1034},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
year = {2015}
}
@article{brundage2018malicious,
author = {Brundage, Miles and Avin, Shahar and Clark, Jack and Toner, Helen and Eckersley, Peter and Garfinkel, Ben and Dafoe, Allan and Scharre, Paul and Zeitzoff, Thomas and Filar, Bobby and Anderson, Hyrum and Roff, Heather and Allen, Gregory C. and Steinhardt, Jacob and Flynn, Carrick and H{\'{E}}igeartaigh, Se{\'{a}}n {\'{O}} and Beard, Simon and Belfield, Haydn and Farquhar, Sebastian and Lyle, Clare and Crootof, Rebecca and Evans, Owain and Page, Michael and Bryson, Joanna and Yampolskiy, Roman and Amodei, Dario},
file = {:C\:/Users/jongwook/Dropbox/References/The Malicious Use of Artificial Intelligence Forecasting, Prevention, and Mitigation.pdf:pdf},
journal = {arXiv preprint arXiv:1802.07228},
title = {{The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation}}
}
@inproceedings{mcfee2015muda,
author = {McFee, Brian and Humphrey, Eric J and Bello, Juan Pablo},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/A Software Framework for Musical Data Augmentation.pdf:pdf},
keywords = {Augmentation},
mendeley-tags = {Augmentation},
pages = {248--254},
title = {{A Software Framework for Musical Data Augmentation.}},
year = {2015}
}
@article{blaauw2017singing,
abstract = {We present a new model for singing synthesis based on a modified version of the WaveNet architecture. Instead of modeling raw waveform, we model features produced by a parametric vocoder that separates the influence of pitch and timbre. This allows conveniently modifying pitch to match any target melody, facilitates training on more modest dataset sizes, and significantly reduces training and generation times. Our model makes frame-wise predictions using mixture density outputs rather than categorical outputs in order to reduce the required parameter count. As we found overfitting to be an issue with the relatively small datasets used in our experiments, we propose a method to regularize the model and make the autoregressive generation process more robust to prediction errors. Using a simple multi-stream architecture, harmonic, aperiodic and voiced/unvoiced components can all be predicted in a coherent manner. We compare our method to existing parametric statistical and state-of-the-art concatenative methods using quantitative metrics and a listening test. While naive implementations of the autoregressive generation algorithm tend to be inefficient, using a smart algorithm we can greatly speed up the process and obtain a system that's competitive in both speed and quality.},
archivePrefix = {arXiv},
arxivId = {1704.03809},
author = {Blaauw, Merlijn and Bonada, Jordi},
doi = {10.3390/app7121313},
eprint = {1704.03809},
file = {:C\:/Users/jongwook/Dropbox/References/A Neural Parametric Singing Synthesizer Modeling Timbre and Expression from Natural Songs.pdf:pdf},
issn = {2076-3417},
journal = {Applied Sciences},
keywords = {autoregressive models,conditional generative models,deep learning,machine learning,singing synthesis},
number = {12},
pages = {1313},
title = {{A Neural Parametric Singing Synthesizer Modeling Timbre and Expression from Natural Songs}},
url = {http://arxiv.org/abs/1704.03809},
volume = {7},
year = {2017}
}
@inproceedings{roth2017gan,
abstract = {Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f-divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer across several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.},
archivePrefix = {arXiv},
arxivId = {1705.09367},
author = {Roth, Kevin and Lucchi, Aurelien and Nowozin, Sebastian and Hofmann, Thomas},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {10.1007/s00138-014-0623-4},
eprint = {1705.09367},
file = {:C\:/Users/jongwook/Dropbox/References/Stabilizing Training of Generative Adversarial Networks through Regularization.pdf:pdf},
isbn = {0013801406},
issn = {14321769},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Stabilizing Training of Generative Adversarial Networks through Regularization}},
url = {http://arxiv.org/abs/1705.09367},
year = {2017}
}
@inproceedings{zhang2017stackgan,
abstract = {Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256x256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.},
archivePrefix = {arXiv},
arxivId = {1612.03242},
author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
booktitle = {Proceedings of the {IEEE} International Conference on Computer Vision {(ICCV)}},
doi = {10.1109/ICCV.2017.629},
eprint = {1612.03242},
file = {:C\:/Users/jongwook/Dropbox/References/StackGAN Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
pmid = {202927},
title = {{StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1612.03242},
year = {2017}
}
@article{berthelot2017began,
author = {Berthelot, David and Schumm, Tom and Metz, Luke},
file = {:C\:/Users/jongwook/Dropbox/References/BEGAN Boundary Equilibrium Generative Adversarial Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1703.10717},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{BEGAN: Boundary Equilibrium Generative Adversarial Networks}},
year = {2017}
}
@article{bahdanau2014attention,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:C\:/Users/jongwook/Dropbox/References/Neural Machine Translation by Jointly Learning to Align and Translate.pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
journal = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
keywords = {Attention},
mendeley-tags = {Attention},
pmid = {14527267},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {http://arxiv.org/abs/1409.0473},
year = {2015}
}
@article{mao2017effectiveness,
abstract = {Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\chi^2$ divergence. We also present a theoretical analysis about the properties of LSGANs and $\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. For evaluating the image quality, we train LSGANs on several datasets including LSUN and a cat dataset, and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. Furthermore, we evaluate the stability of LSGANs in two groups. One is to compare between LSGANs and regular GANs without gradient penalty. We conduct three experiments, including Gaussian mixture distribution, difficult architectures, and a new proposed method --- datasets with small variance, to illustrate the stability of LSGANs. The other one is to compare between LSGANs with gradient penalty and WGANs with gradient penalty (WGANs-GP). The experimental results show that LSGANs with gradient penalty succeed in training for all the difficult architectures used in WGANs-GP, including 101-layer ResNet.},
archivePrefix = {arXiv},
arxivId = {1712.06391},
author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
eprint = {1712.06391},
file = {:C\:/Users/jongwook/Dropbox/References/On the Effectiveness of Least Squares Generative Adversarial Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1712.06391},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{On the Effectiveness of Least Squares Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1712.06391},
year = {2017}
}
@inproceedings{li2017alice,
abstract = {We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.},
archivePrefix = {arXiv},
arxivId = {1709.01215},
author = {Li, Chunyuan and Liu, Hao and Chen, Changyou and Pu, Yunchen and Chen, Liqun and Henao, Ricardo and Carin, Lawrence},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1709.01215},
file = {:C\:/Users/jongwook/Dropbox/References/ALICE Towards Understanding Adversarial Learning for Joint Distribution Matching.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {1--9},
title = {{ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching}},
url = {http://arxiv.org/abs/1709.01215},
year = {2017}
}
@article{hua2018wavenet,
archivePrefix = {arXiv},
arxivId = {1802.08370},
author = {Hua, Kanru},
eprint = {1802.08370},
file = {:C\:/Users/jongwook/Dropbox/References/Do WaveNets Dream of Acoustic Waves.pdf:pdf},
journal = {arXiv preprint arXiv:1802.08370},
title = {{Do WaveNets Dream of Acoustic Waves?}},
url = {http://arxiv.org/abs/1802.08370},
year = {2018}
}
@inproceedings{donahue2016bigan,
abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
archivePrefix = {arXiv},
arxivId = {1605.09782},
author = {Donahue, Jeff and Kr{\"{a}}henb{\"{u}}hl, Philipp and Darrell, Trevor},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
doi = {10.1038/nphoton.2013.187},
eprint = {1605.09782},
file = {:C\:/Users/jongwook/Dropbox/References/Adversarial Feature Learning.pdf:pdf},
isbn = {2334-2536},
issn = {2334-2536},
keywords = {GAN},
mendeley-tags = {GAN},
pmid = {27377197},
title = {{Adversarial Feature Learning}},
url = {http://arxiv.org/abs/1605.09782},
year = {2017}
}
@article{yang2017midinet,
author = {Yang, Li-Chia and Chou, Szu-Yu and Yang, Yi-Hsuan},
file = {:C\:/Users/jongwook/Dropbox/References/MidiNet A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation using 1D and 2D Conditions.pdf:pdf},
journal = {arXiv preprint arXiv:1703.10847},
keywords = {Symbolic},
mendeley-tags = {Symbolic},
title = {{MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation using 1D and 2D Conditions}},
year = {2017}
}
@article{cemgil2000tempogram,
abstract = {We formulate tempo tracking in a Bayesian framework where a tempo tracker...},
author = {Cemgil, Ali Taylan and Kappen, Bert and Desain, Peter and Honing, Henkjan},
doi = {10.1080/09298210008565462},
file = {:C\:/Users/jongwook/Dropbox/References/On Tempo Tracking Tempogram Representation and Kalman filtering.pdf:pdf},
isbn = {0929821000856},
issn = {0929-8215},
journal = {Journal of New Music Research},
number = {1967},
pages = {259--273},
title = {{On Tempo Tracking: Tempogram Representation and Kalman filtering}},
volume = {29},
year = {2000}
}
@inproceedings{defferrard2016fma,
abstract = {We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies. We here describe the dataset and how it was created, propose a train/validation/test split and three subsets, discuss some suitable MIR tasks, and evaluate some baselines for genre recognition. Code, data, and usage examples are available at https://github.com/mdeff/fma},
archivePrefix = {arXiv},
arxivId = {1612.01840},
author = {Defferrard, Micha{\"{e}}l and Benzi, Kirell and Vandergheynst, Pierre and Bresson, Xavier},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
eprint = {1612.01840},
file = {:C\:/Users/jongwook/Dropbox/References/FMA A Dataset For Music Analysis.pdf:pdf},
keywords = {Dataset},
mendeley-tags = {Dataset},
pages = {316--323},
title = {{FMA: A Dataset For Music Analysis}},
url = {http://arxiv.org/abs/1612.01840},
year = {2016}
}
@article{mnih2015dqn,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Others},
file = {:C\:/Users/jongwook/Dropbox/References/Human-Level Control Through Deep Reinforcement Learning.pdf:pdf},
journal = {Nature},
keywords = {DQN,RL},
mendeley-tags = {DQN,RL},
number = {7540},
pages = {529--533},
publisher = {Nature Research},
title = {{Human-Level Control Through Deep Reinforcement Learning}},
volume = {518},
year = {2015}
}
@inproceedings{vincent2008denoising,
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
file = {:C\:/Users/jongwook/Dropbox/References/Extracting and Composing Robust Features with Denoising Autoencoders.pdf:pdf},
keywords = {Auto-Encoders},
mendeley-tags = {Auto-Encoders},
organization = {ACM},
pages = {1096--1103},
title = {{Extracting and Composing Robust Features with Denoising Autoencoders}},
year = {2008}
}
@article{lecun2015deeplearning,
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
file = {:C\:/Users/jongwook/Dropbox/References/Deep Learning.pdf:pdf},
journal = {Nature},
keywords = {Survey},
mendeley-tags = {Survey},
number = {7553},
pages = {436--444},
publisher = {Nature Research},
title = {{Deep Learning}},
volume = {521},
year = {2015}
}
@inproceedings{oord2017vqvae,
abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
archivePrefix = {arXiv},
arxivId = {1711.00937},
author = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1711.00937},
file = {:C\:/Users/jongwook/Dropbox/References/Neural Discrete Representation Learning.pdf:pdf},
title = {{Neural Discrete Representation Learning}},
url = {http://arxiv.org/abs/1711.00937},
year = {2017}
}
@article{dubnowski1976acf,
author = {Dubnowski, John J. and Schafer, Ronald W. and Rabiner, Lawrence R.},
doi = {10.1109/TASSP.1976.1162765},
file = {:C\:/Users/jongwook/Dropbox/References/Real-Time Digital Hardware Pitch Detector.pdf:pdf},
isbn = {0096-3518},
issn = {00963518},
journal = {{IEEE} Transactions on Acoustics, Speech, and Signal Processing},
number = {1},
pages = {2--8},
title = {{Real-Time Digital Hardware Pitch Detector}},
volume = {24},
year = {1976}
}
@article{cakir2017crnn,
abstract = {Sound events often occur in unstructured environments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNN) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural networks (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a Convolutional Recurrent Neural Network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.},
archivePrefix = {arXiv},
arxivId = {1702.06286},
author = {Cakir, Emre and Parascandolo, Giambattista and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas},
doi = {10.1109/TASLP.2017.2690575},
eprint = {1702.06286},
file = {:C\:/Users/jongwook/Dropbox/References/Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection.pdf:pdf},
isbn = {0849371813},
issn = {23299290},
journal = {{IEEE/ACM} Transactions on Audio, Speech, and Language Processing},
keywords = {Convolutional neural networks (CNNs),deep neural networks,recurrent neural networks (RNNs),sound event detection},
number = {6},
pages = {1291--1303},
pmid = {456984},
title = {{Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection}},
volume = {25},
year = {2017}
}
@inproceedings{sonderby2016ladder,
abstract = {Variational Autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.},
archivePrefix = {arXiv},
arxivId = {1602.02282},
author = {S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1602.02282},
file = {:C\:/Users/jongwook/Dropbox/References/Ladder Variational Autoencoders.pdf:pdf},
issn = {10495258},
keywords = {VAE},
mendeley-tags = {VAE},
title = {{Ladder Variational Autoencoders}},
url = {http://arxiv.org/abs/1602.02282},
year = {2016}
}
@article{cho2010chord,
author = {Cho, Taemin and Weiss, Ron J and Bello, Juan P},
file = {:C\:/Users/jongwook/Dropbox/References/Exploring Common Variations in State-of-the-Art Chord Recognition Systems.pdf:pdf},
journal = {Sound and Music Computing},
pages = {11--22},
title = {{Exploring Common Variations in State-of-the-Art Chord Recognition Systems}},
year = {2010}
}
@inproceedings{chen2016infogan,
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
file = {:C\:/Users/jongwook/Dropbox/References/InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {2172--2180},
title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
year = {2016}
}
@article{winter2017ivegan,
abstract = {Generative adversarial networks (GANs) are a powerful framework for generative tasks. However, they are difficult to train and tend to miss modes of the true data generation process. Although GANs can learn a rich representation of the covered modes of the data in their latent space, the framework misses an inverse mapping from data to this latent space. We propose Invariant Encoding Generative Adversarial Networks (IVE-GANs), a novel GAN framework that introduces such a mapping for individual samples from the data by utilizing features in the data which are invariant to certain transformations. Since the model maps individual samples to the latent space, it naturally encourages the generator to cover all modes. We demonstrate the effectiveness of our approach in terms of generative performance and learning rich representations on several datasets including common benchmark image generation tasks.},
archivePrefix = {arXiv},
arxivId = {1711.08646},
author = {Winter, Robin and Clevert, Djork-Arn{\'{e}}},
eprint = {1711.08646},
file = {:C\:/Users/jongwook/Dropbox/References/IVE-GAN Invariant Encoding Generative Adversarial Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1711.08646},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{IVE-GAN: Invariant Encoding Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1711.08646},
year = {2017}
}
@article{griffin1984lim,
author = {Griffin, Daniel and Lim, Jae S.},
doi = {10.1109/TASSP.1984.1164317},
file = {:C\:/Users/jongwook/Dropbox/References/Signal Estimation from Modified Short-Time Fourier transform.pdf:pdf},
issn = {0096-3518},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
number = {2},
pages = {236--243},
title = {{Signal Estimation from Modified Short-Time Fourier transform}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1164317},
volume = {32},
year = {1984}
}
@inproceedings{kondor2018compact,
abstract = {Convolutional neural networks have been ex-tremely successful in the image recognition do-main because they ensure equivariance to trans-lations. There have been many recent attempts to generalize this framework to other domains, including graphs and data lying on manifolds. In this paper we give a rigorous, theoretical treat-ment of convolution and equivariance in neural networks with respect to not just translations, but the action of any compact group. Our main result is to prove that (given some natural constraints) convolutional structure is not just a sufficient, but also a necessary condition for equivariance to the action of a compact group. Our exposition makes use of concepts from representation theory and noncommutative harmonic analysis and derives new generalized convolution formulae.},
author = {Kondor, Risi and Trivedi, Shubhendu},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
file = {:C\:/Users/jongwook/Dropbox/References/On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups.pdf:pdf},
title = {{On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups}},
year = {2018}
}
@article{che2016mrgan,
author = {Che, Tong and Li, Yanran and Jacob, Athul Paul and Bengio, Yoshua and Li, Wenjie},
file = {:C\:/Users/jongwook/Dropbox/References/Mode Regularized Generative Adversarial Networks.pdf:pdf},
journal = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Mode Regularized Generative Adversarial Networks}},
year = {2017}
}
@inproceedings{reddi2018amsgrad,
abstract = {Several recently proposed stochastic optimization methods that have been suc-cessfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM, etc are based on using gradient updates scaled by square roots of ex-ponential moving averages of squared past gradients. It has been empirically ob-served that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous anal-ysis of ADAM algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with " long-term memory " of past gradi-ents, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:C\:/Users/jongwook/Dropbox/References/On the Convergence of Adam and Beyond.pdf:pdf},
keywords = {Optimizer},
mendeley-tags = {Optimizer},
title = {{On the Convergence of Adam and Beyond}},
year = {2018}
}
@inproceedings{metz2016unrolled,
abstract = {We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.},
archivePrefix = {arXiv},
arxivId = {1611.02163},
author = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1611.02163},
file = {:C\:/Users/jongwook/Dropbox/References/Unrolled Generative Adversarial Networks.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {1--25},
pmid = {202927},
title = {{Unrolled Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1611.02163},
year = {2017}
}
@article{radford2015dcgan,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
doi = {10.1051/0004-6361/201527329},
eprint = {1511.06434},
file = {:C\:/Users/jongwook/Dropbox/References/Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv preprint arXiv:1511.06434v2},
keywords = {GAN},
mendeley-tags = {GAN},
month = {nov},
pages = {1--16},
pmid = {23459267},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
@inproceedings{gregor2015draw,
archivePrefix = {arXiv},
arxivId = {1502.04623},
author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1502.04623},
file = {:C\:/Users/jongwook/Dropbox/References/DRAW A Recurrent Neural Network For Image Generation.pdf:pdf},
title = {{DRAW: A Recurrent Neural Network For Image Generation}},
url = {http://arxiv.org/abs/1502.04623},
year = {2015}
}
@article{nayebi2015gruv,
author = {Nayebi, Aran and Vitelli, Matt},
file = {:C\:/Users/jongwook/Dropbox/References/GRUV Algorithmic Music Generation using Recurrent Neural Networks.pdf:pdf},
journal = {Stanford {CS224d} Class Project},
title = {{GRUV: Algorithmic Music Generation using Recurrent Neural Networks}},
year = {2015}
}
@inproceedings{arjovsky2017principled,
abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
archivePrefix = {arXiv},
arxivId = {1605.07725},
author = {Arjovsky, Martin and Bottou, Leon},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
doi = {10.2507/daaam.scibook.2010.27},
eprint = {1605.07725},
file = {:C\:/Users/jongwook/Dropbox/References/Towards Principled Methods for Training Generative Adversarial Networks.pdf:pdf},
isbn = {1584880309},
issn = {17269687},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {1--17},
title = {{Towards Principled Methods for Training Generative Adversarial Networks}},
url = {https://openreview.net/forum?id=Hk4_qw5xe&noteId=Hk4_qw5xe},
year = {2017}
}
@article{chorowski2015speech,
archivePrefix = {arXiv},
arxivId = {1506.07503},
author = {Chorowski, Jan K. and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1016/j.asr.2015.02.035},
eprint = {1506.07503},
file = {:C\:/Users/jongwook/Dropbox/References/Attention-Based Models for Speech Recognition.pdf:pdf},
issn = {18791948},
journal = {Advances in Neural Information Processing Systems {(NIPS)}},
pages = {577--585},
title = {{Attention-Based Models for Speech Recognition}},
url = {http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition},
year = {2015}
}
@inproceedings{leroux2010spectrogram,
author = {{Le Roux}, Jonathan and Kameoka, Hirokazu and Ono, Nobutaka and Sagayama, Shigeki},
booktitle = {Proceedings of the International Conference on Digital Audio Effects {(DAFx)}},
file = {:C\:/Users/jongwook/Dropbox/References/Fast Signal Reconstruction Frommagnitude Stft Spectrogram Based on Spectrogram Consistency.pdf:pdf},
isbn = {9783200019409},
title = {{Fast Signal Reconstruction Frommagnitude Stft Spectrogram Based on Spectrogram Consistency}},
year = {2010}
}
@inproceedings{cao2018bre,
author = {Cao, Yanshuai and Ding, Gavin Weiguang and Lui, Kry Yik-Chau and Huang, Ruitong},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:C\:/Users/jongwook/Dropbox/References/Improving GAN Training via Binarized Representation Entropy (BRE) Regularization.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Improving GAN Training via Binarized Representation Entropy (BRE) Regularization}},
year = {2018}
}
@inproceedings{zhao2017ebgan,
abstract = {Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data gener-ating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.},
archivePrefix = {arXiv},
arxivId = {arXiv:1612.02136v5},
author = {Zhao, Junbo and Mathieu, Michael and LeCun, Yann},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {arXiv:1612.02136v5},
file = {:C\:/Users/jongwook/Dropbox/References/Energy-based Generative Adversarial Network.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Energy-based Generative Adversarial Network}},
year = {2017}
}
@inproceedings{jansson2017separation,
abstract = {The decomposition of a music audio signal into its vocal and backing track components is analogous to image-to-image translation, where a mixed spectrogram is trans-formed into its constituent sources. We propose a novel application of the U-Net architecture — initially devel-oped for medical imaging — for the task of source sep-aration, given its proven capacity for recreating the fine, low-level detail required for high-quality audio reproduc-tion. Through both quantitative evaluation and subjective assessment, experiments demonstrate that the proposed al-gorithm achieves state-of-the-art performance.},
author = {Jansson, Andreas and Humphrey, Eric and Montecchio, Nicola and Bittner, Rachel and Kumar, Aparna and Weyde, Tillman},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Singing Voice Separation With Deep U-Net Convolutional Networks.pdf:pdf},
keywords = {CNN},
mendeley-tags = {CNN},
pages = {745--751},
title = {{Singing Voice Separation With Deep U-Net Convolutional Networks}},
year = {2017}
}
@inproceedings{grosche2010tempogram,
abstract = {The extraction of local tempo and beat information from audio recordings constitutes a challenging task, particularly for music that reveals significant tempo variations. Furthermore, the existence of various pulse levels such as measure, tactus, and tatum often makes the determination of absolute tempo problematic. In this paper, we present a robust mid-level representation that encodes local tempo information. Similar to the well-known concept of cyclic chroma features, where pitches differing by octaves are identified, we introduce the concept of cyclic tempograms, where tempi differing by a power of two are identified. Furthermore, we describe how to derive cyclic tempograms from music signals using two different methods for periodicity analysis and finally sketch some applications to tempo-based audio segmentation.},
author = {Grosche, Peter and Muller, Meinard and Kurth, Frank},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:C\:/Users/jongwook/Dropbox/References/Cyclic Tempogram - a Mid-Level Tempo Representation for Music Signals.pdf:pdf},
isbn = {9781424442966},
keywords = {audio segmentation,chroma,music signals,tempo,tempogram},
title = {{Cyclic Tempogram - a Mid-Level Tempo Representation for Music Signals}},
year = {2010}
}
@article{cho2014seq2seq,
author = {Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
file = {:C\:/Users/jongwook/Dropbox/References/Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
journal = {Proceedings of the Conference on Empirical Methods in Natural Language Processing {(EMNLP)}},
keywords = {RNN},
mendeley-tags = {RNN},
title = {{Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation}},
year = {2014}
}
@inproceedings{schramm2017acappella,
abstract = {This paper presents a multi-pitch detection and voice as-signment method applied to audio recordings containing a cappella performances with multiple singers. A novel ap-proach combining an acoustic model for multi-pitch detec-tion and a music language model for voice separation and assignment is proposed. The acoustic model is a spectro-gram factorization process based on Probabilistic Latent Component Analysis (PLCA), driven by a 6-dimensional dictionary with pre-learned spectral templates. The voice separation component is based on hidden Markov mod-els that use musicological assumptions. By integrating the models, the system can detect multiple concurrent pitches in vocal music and assign each detected pitch to a specific voice corresponding to a voice type such as soprano, alto, tenor or bass (SATB). This work focuses on four-part com-positions, and evaluations on recordings of Bach Chorales and Barbershop quartets show that our integrated approach achieves an F-measure of over 70% for frame-based multi-pitch detection and over 45% for four-voice assignment.},
author = {Schramm, Rodrigo and McLeod, Andrew and Steedman, Mark and Benetos, Emmanouil},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Multi-Pitch Detection and Voice Assignment for a Cappella Recordings of Multiple Singers.pdf:pdf},
keywords = {Multi-F0},
mendeley-tags = {Multi-F0},
pages = {552--559},
title = {{Multi-Pitch Detection and Voice Assignment for a Cappella Recordings of Multiple Singers}},
url = {http://homepages.inf.ed.ac.uk/s1331854/pdf/Vocal4-ismir.pdf},
year = {2017}
}
@article{bengio2007greedy,
author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo and Others},
file = {:C\:/Users/jongwook/Dropbox/References/Greedy Layer-Wise Training of Deep Networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems {(NIPS)}},
pages = {153},
title = {{Greedy Layer-Wise Training of Deep Networks}},
volume = {19},
year = {2007}
}
@inproceedings{kim2017discogan,
abstract = {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity. Source code for official implementation is publicly available https://github.com/SKTBrain/DiscoGAN},
archivePrefix = {arXiv},
arxivId = {1703.05192},
author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jungkwon Kwon and Kim, Jiwon},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1703.05192},
file = {:C\:/Users/jongwook/Dropbox/References/Learning to Discover Cross-Domain Relations with Generative Adversarial Networks.pdf:pdf},
issn = {1938-7228},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Learning to Discover Cross-Domain Relations with Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1703.05192},
year = {2017}
}
@article{ledig2016superresolution,
abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
archivePrefix = {arXiv},
arxivId = {1609.04802},
author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
doi = {10.1109/CVPR.2017.19},
eprint = {1609.04802},
file = {:C\:/Users/jongwook/Dropbox/References/Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.pdf:pdf},
isbn = {978-1-5386-0457-1},
issn = {0018-5043},
journal = {arXiv preprint arXiv:1609.04802},
keywords = {GAN,Super-Resolution},
mendeley-tags = {GAN,Super-Resolution},
pmid = {428914},
title = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
url = {http://arxiv.org/abs/1609.04802},
year = {2016}
}
@inproceedings{glorot2010initialization,
author = {Glorot, Xavier and Bengio, Yoshua},
booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics {(AISTATS)}},
file = {:C\:/Users/jongwook/Dropbox/References/Understanding the Difficulty of Training Deep Feedforward Neural Networks.pdf:pdf},
title = {{Understanding the Difficulty of Training Deep Feedforward Neural Networks}},
year = {2010}
}
@article{isola2016pix2pix,
abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
archivePrefix = {arXiv},
arxivId = {1611.07004},
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
doi = {10.1109/CVPR.2017.632},
eprint = {1611.07004},
file = {:C\:/Users/jongwook/Dropbox/References/Image-to-Image Translation with Conditional Adversarial Networks.pdf:pdf},
isbn = {978-1-5386-0457-1},
issn = {08883270},
journal = {arXiv preprint arXiv:1611.07004},
keywords = {GAN},
mendeley-tags = {GAN},
pmid = {14706220},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}},
year = {2016}
}
@inproceedings{shen2018tacotron,
archivePrefix = {arXiv},
arxivId = {1712.05884},
author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerry-Ryan, RJ and Saurous, Rif A. and Agiomyrgiannakis, Yannis and Wu, Yonghui},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
eprint = {1712.05884},
file = {:C\:/Users/jongwook/Dropbox/References/Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.pdf:pdf},
pages = {2--6},
title = {{Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions}},
url = {http://arxiv.org/abs/1712.05884},
year = {2018}
}
@article{casey2008mir,
abstract = {The steep rise in music downloading over CD sales has created a major shift in the music industry away from physical media formats and towards online products and services. Music is one of the most popular types of online information and there are now hundreds of music streaming and download services operating on the World-Wide Web. Some of the music collections available are approaching the scale of ten million tracks and this has posed a major challenge for searching, retrieving, and organizing music content. Research efforts in music information retrieval have involved experts from music perception, cognition, musicology, engineering, and computer science engaged in truly interdisciplinary activity that has resulted in many proposed algorithmic and methodological solutions to music search using content-based methods. This paper outlines the problems of content-based music information retrieval and explores the state-of-the-art methods using audio cues (e.g., query by humming, audio fingerprinting, content-based music retrieval) and other cues (e.g., music notation and symbolic representation), and identifies some of the major challenges for the coming years},
author = {Casey, Michael A. and Veltkamp, Remco and Goto, Masataka and Leman, Marc and Rhodes, Christophe and Slaney, Malcolm},
doi = {10.1109/JPROC.2008.916370},
file = {:C\:/Users/jongwook/Dropbox/References/Content-Based Music Information Retrieval Current Directions and Future Challenges.pdf:pdf},
isbn = {0018-9219},
issn = {0018-9219},
journal = {Proceedings of the {IEEE}},
keywords = {MIR},
mendeley-tags = {MIR},
number = {4},
pages = {668--696},
title = {{Content-Based Music Information Retrieval: Current Directions and Future Challenges}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4472077%5Cnpapers2://publication/doi/10.1109/JPROC.2008.916370},
volume = {96},
year = {2008}
}
@inproceedings{donahue2017gan,
archivePrefix = {arXiv},
arxivId = {arXiv:1705.07904v2},
author = {Donahue, Chris and Lipton, Zachary C. and Balsubramani, Akshay and McAuley, Julian},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {arXiv:1705.07904v2},
file = {:C\:/Users/jongwook/Dropbox/References/Semantically Decomposing the Latent Spaces of Generative Adversarial Networks.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Semantically Decomposing the Latent Spaces of Generative Adversarial Networks}},
year = {2018}
}
@book{zubizarreta1998prosody,
author = {Zubizarreta, Maria Luisa},
file = {:C\:/Users/jongwook/Dropbox/References/Prosody, Focus, and Word Order.pdf:pdf},
isbn = {0262240416},
publisher = {MIT Press},
title = {{Prosody, Focus, and Word Order}},
year = {1998}
}
@inproceedings{goodfellow2014gan,
abstract = {We propose a new framework for estimating generative models via an adversar- ial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {10.1017/CBO9781139924801},
eprint = {arXiv:1011.1669v3},
file = {:C\:/Users/jongwook/Dropbox/References/Generative Adversarial Nets.pdf:pdf},
isbn = {9781139924801},
issn = {01420615},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {2672--2680},
pmid = {1107015359},
title = {{Generative Adversarial Nets}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
year = {2014}
}
@article{kalingeri2016generation,
author = {Kalingeri, Vasanth and Grandhe, Srikanth},
file = {:C\:/Users/jongwook/Dropbox/References/Music Generation with Deep Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1612.04928},
title = {{Music Generation with Deep Learning}},
year = {2016}
}
@inproceedings{heusel2017ttur,
abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
archivePrefix = {arXiv},
arxivId = {1706.08500},
author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1706.08500},
file = {:C\:/Users/jongwook/Dropbox/References/GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium}},
url = {http://arxiv.org/abs/1706.08500},
year = {2017}
}
@article{siedenburg2016timbre,
author = {Siedenburg, Kai and Fujinaga, Ichiro and McAdams, Stephen},
file = {:C\:/Users/jongwook/Dropbox/References/A Comparison of Approaches to Timbre Descriptors in Music Information Retrieval and Music Psychology.pdf:pdf},
journal = {Journal of New Music Research},
keywords = {MIR,Psychology,Timbre},
mendeley-tags = {MIR,Psychology,Timbre},
number = {1},
pages = {27--41},
publisher = {Taylor & Francis},
title = {{A Comparison of Approaches to Timbre Descriptors in Music Information Retrieval and Music Psychology}},
volume = {45},
year = {2016}
}
@article{camacho2008swipe,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Camacho, Arturo and Harris, John G.},
doi = {10.1121/1.2951592},
eprint = {arXiv:1011.1669v3},
file = {:C\:/Users/jongwook/Dropbox/References/A Sawtooth Waveform Inspired Pitch Estimator for Speech and Music.pdf:pdf},
isbn = {1520-8524 (Electronic)},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {3},
pages = {1638--1652},
pmid = {19045655},
title = {{A Sawtooth Waveform Inspired Pitch Estimator for Speech and Music}},
url = {http://asa.scitation.org/doi/10.1121/1.2951592},
volume = {124},
year = {2008}
}
@inproceedings{zhu2017cyclegan,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}},
doi = {10.1109/ICCV.2017.244},
eprint = {1703.10593},
file = {:C\:/Users/jongwook/Dropbox/References/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.pdf:pdf},
isbn = {978-1-5386-1032-9},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {2223--2232},
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}},
url = {http://arxiv.org/abs/1703.10593},
year = {2017}
}
@article{jin2015srelu,
author = {Jin, Xiaojie and Xu, Chunyan and Feng, Jiashi and Wei, Yunchao and Xiong, Junjun and Yan, Shuicheng},
file = {:C\:/Users/jongwook/Dropbox/References/Deep Learning with S-shaped Rectified Linear Activation Units.pdf:pdf},
journal = {arXiv preprint arXiv:1512.07030},
title = {{Deep Learning with S-shaped Rectified Linear Activation Units}},
year = {2015}
}
@inproceedings{krizhevsky2012imagenet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
file = {:C\:/Users/jongwook/Dropbox/References/Imagenet classification with deep convolutional neural networks.pdf:pdf},
pages = {1097--1105},
title = {{Imagenet classification with deep convolutional neural networks}},
year = {2012}
}
@inproceedings{humphrey2015timely,
abstract = {Automatic chord estimation (ACE) is a hallmark re-search topic in content-based music informatics, but like many other tasks, system performance appears to be con-verging to yet another glass ceiling. Looking toward trends in other machine perception domains, one might conclude that complex, data-driven methods have the potential to significantly advance the state of the art. Two recent efforts did exactly this for large-vocabulary ACE, but despite ar-guably achieving some of the highest results to date, both approaches plateau well short of having solved the prob-lem. Therefore, this work explores the behavior of these two high performing, systems as a means of understanding obstacles and limitations in chord estimation, arriving at four critical observations: one, music recordings that inval-idate tacit assumptions about harmony and tonality result in erroneous and even misleading performance; two, stan-dard lexicons and comparison methods struggle to reflect the natural relationships between chords; three, conven-tional approaches conflate the competing goals of recogni-tion and transcription to some undefined degree; and four, the perception of chords in real music can be highly subjec-tive, making the very notion of " ground truth " annotations tenuous. Synthesizing these observations, this paper of-fers possible remedies going forward, and concludes with some perspectives on the future of both ACE research and the field at large.},
author = {Humphrey, Eric J and Bello, Juan P},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Four Timely Insights on Automatic Chord Estimation.pdf:pdf},
title = {{Four Timely Insights on Automatic Chord Estimation}},
year = {2015}
}
@inproceedings{tjoa2017accompaniment,
author = {Tjoa, Steven K and Meinard, M and College, Harvey Mudd},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:C\:/Users/jongwook/Dropbox/References/Make Your Own Accompaniment Adapting Full-Mix Recordings To Match Solo-Only User Recordings.pdf:pdf},
pages = {79--86},
title = {{Make Your Own Accompaniment: Adapting Full-Mix Recordings To Match Solo-Only User Recordings}},
year = {2017}
}
@article{seo2017wgan,
abstract = {This report has several purposes. First, our report is written to investigate the reproducibility of the submitted paper On the regularization of Wasserstein GANs (2018). Second, among the experiments performed in the submitted paper, five aspects were emphasized and reproduced: learning speed, stability, robustness against hyperparameter, estimating the Wasserstein distance, and various sampling method. Finally, we identify which parts of the contribution can be reproduced, and at what cost in terms of resources. All source code for reproduction is open to the public.},
archivePrefix = {arXiv},
arxivId = {1712.05882},
author = {Seo, Junghoon and Jeon, Taegyun},
eprint = {1712.05882},
file = {:C\:/Users/jongwook/Dropbox/References/On Reproduction of On the Regularization of Wasserstein GANs.pdf:pdf},
issn = {0002-3264},
journal = {arXiv preprint arXiv:1712.05882},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{On Reproduction of On the Regularization of Wasserstein GANs}},
url = {http://arxiv.org/abs/1712.05882},
year = {2017}
}
@article{cogliati2017lateral,
author = {Cogliati, Andrea and Duan, Zhiyao and Wohlberg, Brendt},
doi = {10.1109/LSP.2017.2666183},
file = {:C\:/Users/jongwook/Dropbox/References/Piano Transcription with Convolutional Sparse Lateral Inhibition.pdf:pdf},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Automatic music transcription (AMT),convolutional sparse coding (CSC),lateral inhibition,offset detection},
number = {4},
pages = {392--396},
title = {{Piano Transcription with Convolutional Sparse Lateral Inhibition}},
volume = {24},
year = {2017}
}
@article{raffel2014mir_eval,
author = {Raffel, Colin and Mcfee, Brian and Humphrey, Eric J. and Salamon, Justin and Nieto, Oriol and Liang, Dawen and Ellis, Daniel P. W.},
file = {:C\:/Users/jongwook/Dropbox/References/mir_eval a Transparent Implementation of Common MIR Metrics.pdf:pdf},
journal = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
pages = {367--372},
title = {{mir_eval: A Transparent Implementation of Common MIR Metrics}},
year = {2014}
}
@article{zeiler2012adadelta,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701},
author = {Zeiler, Matthew D.},
eprint = {1212.5701},
file = {:C\:/Users/jongwook/Dropbox/References/ADADELTA an Adaptive Learning Rate Method.pdf:pdf},
isbn = {1212.5701},
journal = {arXiv preprint arXiv:1212.5701},
keywords = {Optimizer},
mendeley-tags = {Optimizer},
title = {{ADADELTA: an Adaptive Learning Rate Method}},
url = {http://arxiv.org/abs/1212.5701},
year = {2012}
}
@article{alemi2017information,
abstract = {We present an information-theoretic framework for understanding trade-offs in unsupervised learning of deep latent-variables models using variational inference. This framework emphasizes the need to consider latent-variable models along two dimensions: the ability to reconstruct inputs (distortion) and the communication cost (rate). We derive the optimal frontier of generative models in the two-dimensional rate-distortion plane, and show how the standard evidence lower bound objective is insufficient to select between points along this frontier. However, by performing targeted optimization to learn generative models with different rates, we are able to learn many models that can achieve similar generative performance but make vastly different trade-offs in terms of the usage of the latent variable. Through experiments on MNIST and Omniglot with a variety of architectures, we show how our framework sheds light on many recent proposed extensions to the variational autoencoder family.},
archivePrefix = {arXiv},
arxivId = {1711.00464},
author = {Alemi, Alexander A. and Poole, Ben and Fischer, Ian and Dillon, Joshua V. and Saurous, Rif A. and Murphy, Kevin},
eprint = {1711.00464},
file = {:C\:/Users/jongwook/Dropbox/References/An Information-Theoretic Analysis of Deep Latent-Variable Models.pdf:pdf},
journal = {arXiv preprint arXiv:1711.00464},
keywords = {Information Theory},
mendeley-tags = {Information Theory},
title = {{An Information-Theoretic Analysis of Deep Latent-Variable Models}},
url = {http://arxiv.org/abs/1711.00464},
year = {2017}
}
@incollection{werbos1982backpropagation,
author = {Werbos, Paul J},
booktitle = {System Modeling and Optimization},
file = {:C\:/Users/jongwook/Dropbox/References/Applications of Advances in Nonlinear Sensitivity Analysis.pdf:pdf},
keywords = {Theory},
mendeley-tags = {Theory},
pages = {762--770},
publisher = {Springer},
title = {{Applications of Advances in Nonlinear Sensitivity Analysis}},
year = {1982}
}
@article{pascual2017segan,
abstract = {Current speech enhancement techniques operate on the spectral domain and/or exploit some higher-level feature. The majority of them tackle a limited number of noise conditions and rely on first-order statistics. To circumvent these issues, deep networks are being increasingly used, thanks to their ability to learn complex functions from large example sets. In this work, we propose the use of generative adversarial networks for speech enhancement. In contrast to current techniques, we operate at the waveform level, training the model end-to-end, and incorporate 28 speakers and 40 different noise conditions into the same model, such that model parameters are shared across them. We evaluate the proposed model using an independent, unseen test set with two speakers and 20 alternative noise conditions. The enhanced samples confirm the viability of the proposed model, and both objective and subjective evaluations confirm the effectiveness of it. With that, we open the exploration of generative architectures for speech enhancement, which may progressively incorporate further speech-centric design choices to improve their performance.},
archivePrefix = {arXiv},
arxivId = {1703.09452},
author = {Pascual, Santiago and Bonafonte, Antonio and Serr{\`{a}}, Joan},
doi = {10.21437/Interspeech.2017-1428},
eprint = {1703.09452},
file = {:C\:/Users/jongwook/Dropbox/References/SEGAN Speech Enhancement Generative Adversarial Network.pdf:pdf},
issn = {19909772},
journal = {arXiv preprint arXiv:1703.09452},
keywords = {Convolutional neural networks.,Deep learning,Generative adversarial networks,Speech enhancement},
title = {{SEGAN: Speech Enhancement Generative Adversarial Network}},
year = {2017}
}
@article{choi2017stargan,
abstract = {Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
archivePrefix = {arXiv},
arxivId = {1711.09020},
author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
eprint = {1711.09020},
file = {:C\:/Users/jongwook/Dropbox/References/StarGAN Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation.pdf:pdf},
journal = {arXiv preprint arXiv:1711.09020},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation}},
url = {http://arxiv.org/abs/1711.09020},
year = {2017}
}
