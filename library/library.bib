Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{krizhevsky2012imagenet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
file = {:home/jongwook/Dropbox/References/ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
pages = {1097--1105},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{maaten2008tsne,
author = {van der Maaten, Laurens and Hinton, Geoffrey},
file = {:home/jongwook/Dropbox/References/Visualizing Data using t-SNE.pdf:pdf},
journal = {Journal of Machine Learning Research},
number = {2008},
pages = {2579--2605},
title = {{Visualizing Data using t-SNE}},
volume = {9},
year = {2008}
}
@inproceedings{ioffe2015batchnorm,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:home/jongwook/Dropbox/References/Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
number = {2},
pages = {448--456},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
year = {2015}
}
@article{jin2015srelu,
author = {Jin, Xiaojie and Xu, Chunyan and Feng, Jiashi and Wei, Yunchao and Xiong, Junjun and Yan, Shuicheng},
file = {:home/jongwook/Dropbox/References/Deep Learning with S-shaped Rectified Linear Activation Units.pdf:pdf},
journal = {arXiv preprint arXiv:1512.07030},
title = {{Deep Learning with S-shaped Rectified Linear Activation Units}},
year = {2015}
}
@inproceedings{theis2015ride,
archivePrefix = {arXiv},
arxivId = {1506.03478},
author = {Theis, Lucas and Bethge, Matthias},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1506.03478},
file = {:home/jongwook/Dropbox/References/Generative Image Modeling Using Spatial LSTMs.pdf:pdf},
issn = {10495258},
title = {{Generative Image Modeling Using Spatial LSTMs}},
url = {http://arxiv.org/abs/1506.03478},
year = {2015}
}
@article{benetos2011joint,
abstract = {In this paper, a method for automatic transcription of music signals based on joint multiple-F0 estimation is proposed. As a time-frequency representation, the constant-Q resonator time-frequency image is employed, while a novel noise suppression technique based on pink noise assumption is applied in a preprocessing step. In the multiple-F0 estimation stage, the optimal tuning and inharmonicity parameters are computed and a salience function is proposed in order to select pitch candidates. For each pitch candidate combination, an overlapping partial treatment procedure is used, which is based on a novel spectral envelope estimation procedure for the log-frequency domain, in order to compute the harmonic envelope of candidate pitches. In order to select the optimal pitch combination for each time frame, a score function is proposed which combines spectral and temporal characteristics of the candidate pitches and also aims to suppress harmonic errors. For postprocessing, hidden Markov models (HMMs) and conditional random fields (CRFs) trained on MIDI data are employed, in order to boost transcription accuracy. The system was trained on isolated piano sounds from the MAPS database and was tested on classic and jazz recordings from the RWC database, as well as on recordings from a Disklavier piano. A comparison with several state-of-the-art systems is provided using a variety of error metrics, where encouraging results are indicated.},
author = {Benetos, Emmanouil and Dixon, Simon},
doi = {10.1109/JSTSP.2011.2162394},
file = {:home/jongwook/Dropbox/References/Joint Multi-Pitch Detection Using Harmonic Envelope Estimation for Polyphonic Music Transcription.pdf:pdf},
issn = {19324553},
journal = {{IEEE} Journal on Selected Topics in Signal Processing},
keywords = {Automatic music transcription,Conditional random fields (CRFs),Harmonic envelope estimation,Resonator time-frequency image},
number = {6},
pages = {1111--1123},
title = {{Joint Multi-Pitch Detection Using Harmonic Envelope Estimation for Polyphonic Music Transcription}},
volume = {5},
year = {2011}
}
@inproceedings{bay2009evaluation,
author = {Bay, Mert and Ehmann, Andreas F. and Downie, J. Stephen},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Evaluation of Multiple-F0 Estimation and Tracking Systems.pdf:pdf},
isbn = {9780981353708},
pages = {315--320},
title = {{Evaluation of Multiple-F0 Estimation and Tracking Systems}},
year = {2009}
}
@article{lucic2017gan,
archivePrefix = {arXiv},
arxivId = {1711.10337},
author = {Lucic, Mario and Kurach, Karol and Michalski, Marcin and Gelly, Sylvain and Bousquet, Olivier},
eprint = {1711.10337},
file = {:home/jongwook/Dropbox/References/Are GANs Created Equal A Large-Scale Study.pdf:pdf},
journal = {arXiv preprint arXiv:1711.10337},
keywords = {GAN,Survey},
mendeley-tags = {GAN,Survey},
title = {{Are GANs Created Equal? A Large-Scale Study}},
url = {http://arxiv.org/abs/1711.10337},
year = {2017}
}
@inproceedings{lecun1995lenet,
author = {LeCun, Yann and Jackel, Larry D. and Bottou, Leon and Brunot, A. and Cortes, Corinna and Denker, J. S. and Drucker, Harris and Guyon, I. and Muller, U. A. and Sackinger, Eduard and Simard, P. and Vapnik, V.},
booktitle = {Proceedings of the International Conference on Artificial Neural Networks},
file = {:home/jongwook/Dropbox/References/Comparison of Learning Algorithms for Handwritten Digit Recognition.pdf:pdf},
pages = {53--60},
title = {{Comparison of Learning Algorithms for Handwritten Digit Recognition}},
volume = {60},
year = {1995}
}
@inproceedings{dozat2016nadam,
author = {Dozat, Timothy},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:home/jongwook/Dropbox/References/Incorporating Nesterov Momentum into Adam.pdf:pdf},
title = {{Incorporating Nesterov Momentum into Adam}},
year = {2016}
}
@article{lecun2015deeplearning,
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
file = {:home/jongwook/Dropbox/References/Deep Learning.pdf:pdf},
journal = {Nature},
keywords = {Survey},
mendeley-tags = {Survey},
number = {7553},
pages = {436--444},
publisher = {Nature Research},
title = {{Deep Learning}},
volume = {521},
year = {2015}
}
@book{bilmes1998gentle,
archivePrefix = {arXiv},
arxivId = {hep-ph/hep-ph/9605323},
author = {Bilmes, Jeff A.},
doi = {10.1080/0042098032000136147},
eprint = {hep-ph/9605323},
file = {:home/jongwook/Dropbox/References/A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models.pdf:pdf},
isbn = {0226775429},
issn = {0042-0980},
pmid = {351},
primaryClass = {hep-ph},
title = {{A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models}},
year = {1998}
}
@inproceedings{klambauer2017selu,
archivePrefix = {arXiv},
arxivId = {1706.02515},
author = {Klambauer, G{\"{u}}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {1706.02515},
eprint = {1706.02515},
file = {:home/jongwook/Dropbox/References/Self-Normalizing Neural Networks.pdf:pdf},
title = {{Self-Normalizing Neural Networks}},
url = {http://arxiv.org/abs/1706.02515},
year = {2017}
}
@article{rebelo2012omr,
author = {Rebelo, Ana and Fujinaga, Ichiro and Paszkiewicz, Filipe and Marcal, Andre R.S. and Guedes, Carlos and Cardoso, Jaime S.},
doi = {10.1007/s13735-012-0004-6},
file = {:home/jongwook/Dropbox/References/Optical Music Recognition State-of-the-Art and Open Issues.pdf:pdf},
isbn = {1373501200},
issn = {2192662X},
journal = {International Journal of Multimedia Information Retrieval},
keywords = {Computer music,Image processing,Machine learning,Music performance},
number = {3},
pages = {173--190},
title = {{Optical Music Recognition State-of-the-Art and Open Issues}},
volume = {1},
year = {2012}
}
@article{piszczalski1977transcription,
author = {Piszczalski, Martin and Galler, Bernard A.},
doi = {10.1007/978-0-387-30441-0_20},
file = {:home/jongwook/Dropbox/References/Automatic Music Transcription.pdf:pdf},
journal = {Computer Music Journal},
number = {4},
pages = {24--31},
title = {{Automatic Music Transcription}},
url = {http://link.springer.com/10.1007/978-0-387-30441-0_20},
volume = {1},
year = {1977}
}
@inproceedings{bahdanau2014attention,
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:home/jongwook/Dropbox/References/Neural Machine Translation by Jointly Learning to Align and Translate.pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
keywords = {Attention},
mendeley-tags = {Attention},
pmid = {14527267},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {http://arxiv.org/abs/1409.0473},
year = {2015}
}
@inproceedings{pachet2017sampling,
author = {Pachet, Fran{\c{c}}ois and Papadopoulos, Alexandre and Roy, Pierre},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Sampling Variations of Sequences for Structured Music Generation.pdf:pdf},
keywords = {Symbolic},
mendeley-tags = {Symbolic},
title = {{Sampling Variations of Sequences for Structured Music Generation}},
year = {2017}
}
@inproceedings{wei2018wgan,
author = {Wei, Xiang and Liu, Zixia and Wang, Liqiang and Gong, Boqing},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:home/jongwook/Dropbox/References/Improving the Improved Training of Wasserstein GANs a Consistency Term and Its Dual Effect.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Improving the Improved Training of Wasserstein GANs a Consistency Term and Its Dual Effect}},
year = {2018}
}
@article{hyvarinen2000ica,
author = {Hyv{\"{a}}rinen, Aapo and Oja, Erkki},
file = {:home/jongwook/Dropbox/References/Independent Component Analysis Algorithms and Applications.pdf:pdf},
isbn = {3589451327},
journal = {Neural Networks},
number = {4-5},
pages = {411--430},
title = {{Independent Component Analysis: Algorithms and Applications}},
volume = {13},
year = {2000}
}
@article{leveau2008atoms,
abstract = {Several studies have pointed out the need for accurate mid-level representations of music signals for information retrieval and signal processing purposes. In this paper, we propose a new mid-level representation based on the decomposition of a signal into a small number of sound atoms or molecules bearing explicit musical instrument labels. Each atom is a sum of windowed harmonic sinusoidal partials whose relative amplitudes are specific to one instrument, and each molecule consists of several atoms from the same instrument spanning successive time windows. We design efficient algorithms to extract the most prominent atoms or molecules and investigate several applications of this representation, including polyphonic instrument recognition and music visualization.},
author = {Leveau, Pierre and Vincent, Emmanuel and Richard, Ga{\'{e}}l and Daudet, Laurent},
doi = {10.1109/TASL.2007.910786},
file = {:home/jongwook/Dropbox/References/Instrument-Specific Harmonic Atoms for Mid-Level Music Representation.pdf:pdf},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Mid-level representation,Music information retrieval,Music visualization,Sparse decomposition},
number = {1},
pages = {116--127},
title = {{Instrument-Specific Harmonic Atoms for Mid-Level Music Representation}},
volume = {16},
year = {2008}
}
@article{saruwatari2006ica,
author = {Saruwatari, Hiroshi and Kawamura, Toshiya and Nishikawa, Tsuyoki and Lee, Akinobu and Shikano, Kiyohiro},
doi = {10.1109/TSA.2005.855832},
file = {:home/jongwook/Dropbox/References/Blind Source Separation Based on a Fast-Convergence Algorithm Combining ICA and Beamforming.pdf:pdf},
isbn = {1558-7916},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
number = {2},
pages = {666--678},
title = {{Blind Source Separation Based on a Fast-Convergence Algorithm Combining ICA and Beamforming}},
volume = {14},
year = {2006}
}
@article{mao2017lsgan,
abstract = {Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.},
archivePrefix = {arXiv},
arxivId = {1611.04076},
author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
doi = {10.1109/ICCV.2017.304},
eprint = {1611.04076},
file = {:home/jongwook/Dropbox/References/Least Squares Generative Adversarial Networks.pdf:pdf},
journal = {Proceedings of the International Conference on Computer Vision {(ICCV})},
pages = {2794--2802},
title = {{Least Squares Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1611.04076},
year = {2017}
}
@inproceedings{reis2007genetic,
author = {Reis, Gustavo and Fonseca, Nuno and Ferndandez, Francisco},
booktitle = {Proceedings of the {IEEE} International Symposium on Intelligent Signal Processing},
file = {:home/jongwook/Dropbox/References/Genetic Algorithm Approach to Polyphonic Music Transcription.pdf:pdf},
keywords = {- genetic algorithms,multiple fo estimation,music,polyphonic pitch estimation,transcription},
title = {{Genetic Algorithm Approach to Polyphonic Music Transcription}},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4447608},
year = {2007}
}
@inproceedings{hofmann1999plsa,
abstract = {Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.},
author = {Hofmann, Thomas},
booktitle = {Proceedings of the Conference on Uncertainty in Artificial Intelligence {(UAI)}},
file = {:home/jongwook/Dropbox/References/Probabilistic Latent Semantic Analysis.pdf:pdf},
isbn = {1-55860-614-9},
keywords = {artificial intelligence,computing methodologies,knowledge representation and reasoning,mathematics in computing,models of computation,probabilistic computation,probabilistic reasoning,probability and statistics,theory of computation,vagueness and fuzzy logic},
pages = {289--296},
title = {{Probabilistic Latent Semantic Analysis}},
url = {http://dl.acm.org/citation.cfm?id=2073829%5Cnhttp://dl.acm.org/citation.cfm?id=2073796.2073829},
year = {1999}
}
@inproceedings{isik2016deepclustering,
abstract = {Deep clustering is a recently introduced deep learning architecture that uses discriminatively trained embeddings as the basis for clustering. It was recently applied to spectrogram segmentation, resulting in impressive results on speaker-independent multi-speaker separation. In this paper we extend the baseline system with an end-to-end signal approximation objective that greatly improves performance on a challenging speech separation. We first significantly improve upon the baseline system performance by incorporating better regularization, larger temporal context, and a deeper architecture, culminating in an overall improvement in signal to distortion ratio (SDR) of 10.3 dB compared to the baseline of 6.0 dB for two-speaker separation, as well as a 7.1 dB SDR improvement for three-speaker separation. We then extend the model to incorporate an enhancement layer to refine the signal estimates, and perform end-to-end training through both the clustering and enhancement stages to maximize signal fidelity. We evaluate the results using automatic speech recognition. The new signal approximation objective, combined with end-to-end training, produces unprecedented performance, reducing the word error rate (WER) from 89.1% down to 30.8%. This represents a major advancement towards solving the cocktail party problem.},
archivePrefix = {arXiv},
arxivId = {arXiv:1607.02173v1},
author = {Isik, Yusuf and {Le Roux}, Jonathan and Chen, Zhuo and Watanabe, Shinji and Hershey, John R.},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association {(INTERSPEECH)}},
doi = {10.21437/Interspeech.2016-1176},
eprint = {arXiv:1607.02173v1},
file = {:home/jongwook/Dropbox/References/Single-Channel Multi-Speaker Separation Using Deep Clustering.pdf:pdf},
issn = {19909772},
keywords = {Deep learning,Embedding,Single-channel speech separation},
pages = {545--549},
title = {{Single-Channel Multi-Speaker Separation Using Deep Clustering}},
year = {2016}
}
@article{samuel1959ml,
author = {Samuel, Arthur L},
doi = {10.1147/rd.33.0210},
file = {:home/jongwook/Dropbox/References/Some Studies in Machine Learning Using the Game of Checkers.pdf:pdf},
issn = {0018-8646},
journal = {{IBM} Journal of Research and Development},
month = {jul},
number = {3},
pages = {210--229},
title = {{Some Studies in Machine Learning Using the Game of Checkers}},
volume = {3},
year = {1959}
}
@article{zhang2018particle,
author = {Zhang, Weiwei and Chen, Zhe and Member, Senior and Yin, Fuliang and Zhang, Qiaoling},
file = {:home/jongwook/Dropbox/References/Melody Extraction From Polyphonic Music Using Particle Filter and Dynamic Programming.pdf:pdf},
journal = {{IEEE/ACM} Transactions on Audio, Speech, and Language Processing},
number = {9},
pages = {1620--1632},
title = {{Melody Extraction From Polyphonic Music Using Particle Filter and Dynamic Programming}},
volume = {26},
year = {2018}
}
@inproceedings{nagarajan2017local,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.04156v3},
author = {Nagarajan, Vaishnavh and Kolter, J. Zico},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {arXiv:1706.04156v3},
file = {:home/jongwook/Dropbox/References/Gradient Descent GAN Optimization is Locally Stable.pdf:pdf},
title = {{Gradient Descent GAN Optimization is Locally Stable}},
year = {2017}
}
@book{russell2009ai,
author = {Russell, Stuart J and Norvig, Peter},
file = {:home/jongwook/Dropbox/References/Artificial Intelligence a Modern Approach (3rd Edition).pdf:pdf},
keywords = {AI},
mendeley-tags = {AI},
publisher = {Pearson},
title = {{Artificial Intelligence: a Modern Approach (3rd Edition)}},
year = {2009}
}
@inproceedings{mnih2014nvil,
archivePrefix = {arXiv},
arxivId = {arXiv:1402.0030v2},
author = {Mnih, A. and Gregor, Karol},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {arXiv:1402.0030v2},
file = {:home/jongwook/Dropbox/References/Neural Variational Inference and Learning in Belief Networks.pdf:pdf},
isbn = {9781634393973},
keywords = {belief networks,deep learning,variational inference},
title = {{Neural Variational Inference and Learning in Belief Networks}},
volume = {32},
year = {2014}
}
@article{rabiner1989hmm,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rabiner, L.R.},
doi = {10.1109/5.18626},
eprint = {arXiv:1011.1669v3},
file = {:home/jongwook/Dropbox/References/A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the {IEEE}},
number = {2},
pages = {257--286},
pmid = {18626},
title = {{A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition}},
volume = {77},
year = {1989}
}
@inproceedings{esling2018timbre,
abstract = {Generative models aim to understand the properties of data, through the construction of latent spaces that allow classification and generation. However, as the learning is unsupervised, the latent dimensions are not related to perceptual properties. In parallel, music perception research has aimed to understand timbre based on human dissimilarity ratings. These lead to timbre spaces which exhibit perceptual similarities between sounds. However, they do not generalize to novel examples and do not provide an invertible mapping, preventing audio synthesis. Here, we show that Variational Auto-Encoders (VAE) can bridge these lines of research and alleviate their weaknesses by regularizing the latent spaces to match perceptual distances collected from timbre studies. Hence, we propose three types of regularization and show that they lead to spaces that are simultaneously coherent with signal properties and perceptual similarities. We show that these spaces can be used for efficient audio classification. We study how audio descriptors are organized along the latent dimensions and show that even though descriptors behave in a non-linear way across the space, they still exhibit a locally smooth evolution. We also show that, as this space generalizes to novel samples, it can be used to predict perceptual similarities of novel instruments. Finally, we exhibit the generative capabilities of our spaces, that can directly synthesize sounds with continuous evolution of timbre perception.},
author = {Esling, Philippe and Chemla-Romeu-Santos, Axel and Bitton, Adrien},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Bridging Audio Analysis, Perception and Synthesis with Perceptually-Regularized Variational Timbre Spaces.pdf:pdf},
title = {{Bridging Audio Analysis, Perception and Synthesis with Perceptually-Regularized Variational Timbre Spaces}},
year = {2018}
}
@article{koretz2011map,
abstract = {In this paper, a new method for multiple fundamental frequency estimation for speech and music signals is proposed. Applications of audio and speech processing include many well-reviewed algorithms for estimating the fundamental frequency of monophonic speech and music signals. In the case of polyphonic signals, it is more difficult to successfully estimate each of the fundamental frequencies, as reflected by the dearth of existing methods addressing this problem. In this paper, a new method based on the combination of the maximum likelihood and maximum a posteriori probability criteria is derived for fundamental frequencies tracking where each one of the fundamental frequencies is modeled by a first-order Markov process. The dominant signal is modeled as a harmonic source with unknown deterministic amplitudes, while the remaining signals, including other harmonic signals, are modeled as Gaussian interference sources with an unknown covariance matrix. After estimation of the dominant source, it is removed from the signal by projection of the signal into the null subspace spanned by the estimated signal. This procedure is iterated for all the harmonic sources in the data. The algorithm is tested with speech, music, and synthetic signals where in each case, two harmonic sources of the same kind were mixed. The performance of the proposed algorithm is evaluated and compared to an existing reference method in terms of gross-error-rate as a function of signal-to-interference ratio},
author = {Koretz, Amitai and Tabrikian, Joseph},
doi = {10.1109/TASL.2011.2125952},
file = {:home/jongwook/Dropbox/References/Maximum A Posteriori Probability Multiple-Pitch Tracking Using the Harmonic Model.pdf:pdf},
issn = {15587924},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {F0 estimation,harmonic model,maximum a posteriori probability (MAP),multipitch estimation,multiple pitch estimation,pitch tracking},
number = {7},
pages = {2210--2221},
title = {{Maximum A Posteriori Probability Multiple-Pitch Tracking Using the Harmonic Model}},
volume = {19},
year = {2011}
}
@inproceedings{johnson2016loss,
author = {Johnson, Justin and Alahi, Alexandre and Fei-fei, Li},
booktitle = {Proceedings of the European Conference on Computer Vision},
file = {:home/jongwook/Dropbox/References/Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf:pdf},
keywords = {Style Transfer,Super-Resolution,deep learning,style transfer,super-resolution},
mendeley-tags = {Style Transfer,Super-Resolution},
title = {{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}},
year = {2016}
}
@inproceedings{maezawa2017beat,
author = {Maezawa, Akira},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Fast and Accurate Improving a Simple Beat Tracker With a Selectively-Applied Deep Beat Identification.pdf:pdf},
keywords = {Beat Tracking},
mendeley-tags = {Beat Tracking},
pages = {309--315},
title = {{Fast and Accurate: Improving a Simple Beat Tracker With a Selectively-Applied Deep Beat Identification}},
year = {2017}
}
@book{miranda2007evolutionary,
author = {Miranda, Eduardo Reck and Al, John and Eds, Biles},
doi = {10.1007/978-1-84628-600-1},
file = {:home/jongwook/Dropbox/References/Evolutionary Computer Music.pdf:pdf},
isbn = {978-1-84628-599-8},
title = {{Evolutionary Computer Music}},
year = {2007}
}
@inproceedings{reddi2018amsgrad,
author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:home/jongwook/Dropbox/References/On the Convergence of Adam and Beyond.pdf:pdf},
keywords = {Optimizer},
mendeley-tags = {Optimizer},
title = {{On the Convergence of Adam and Beyond}},
year = {2018}
}
@inproceedings{ohanlon2012sparsity,
author = {O'Hanlon, Ken and Nagano, Hidehisa and Plumbley, Mark D.},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:home/jongwook/Dropbox/References/Structured Sparsity for Automatic Music Transcription.pdf:pdf},
title = {{Structured Sparsity for Automatic Music Transcription}},
year = {2012}
}
@inproceedings{zaremba2015recurrent,
abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
archivePrefix = {arXiv},
arxivId = {1409.2329},
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1409.2329},
file = {:home/jongwook/Dropbox/References/Recurrent Neural Network Regularization.pdf:pdf},
title = {{Recurrent Neural Network Regularization}},
url = {http://arxiv.org/abs/1409.2329},
year = {2014}
}
@article{benetos2013shift,
archivePrefix = {arXiv},
arxivId = {1401.3816},
author = {Benetos, Emmanouil and Cherla, Srikanth and Weyde, Tillman},
doi = {10.1057/jibs.2013.68},
eprint = {1401.3816},
file = {:home/jongwook/Dropbox/References//An Efficient Shift-Invariant Model for Polyphonic Music Transcription.pdf:pdf;:home/jongwook/Dropbox/References/An Efficient Shift-Invariant Model for Polyphonic Music Transcription(2).pdf:pdf},
isbn = {9788893912730},
issn = {1201-9712},
journal = {Proceedings of the International Workshop on Machine Learning and Music},
title = {{An Efficient Shift-Invariant Model for Polyphonic Music Transcription}},
year = {2013}
}
@article{belghazi2018hali,
archivePrefix = {arXiv},
arxivId = {1802.01071},
author = {Belghazi, Mohamed Ishmael and Rajeswar, Sai and Mastropietro, Olivier and Rostamzadeh, Negar and Mitrovic, Jovana and Courville, Aaron},
eprint = {1802.01071},
file = {:home/jongwook/Dropbox/References/Hierarchical Adversarially Learned Inference.pdf:pdf},
journal = {arXiv preprint arXiv:1802.01071},
title = {{Hierarchical Adversarially Learned Inference}},
url = {http://arxiv.org/abs/1802.01071},
year = {2018}
}
@inproceedings{larsen2015vaegan,
archivePrefix = {arXiv},
arxivId = {1512.09300},
author = {Larsen, Anders Boesen Lindbo and S{\o}nderby, S{\o}ren Kaae and Larochelle, Hugo and Winther, Ole},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1512.09300},
file = {:home/jongwook/Dropbox/References/Autoencoding Beyond Pixels using a Learned Similarity Metric.pdf:pdf},
isbn = {9781510829008},
keywords = {GAN,VAE},
mendeley-tags = {GAN,VAE},
title = {{Autoencoding Beyond Pixels using a Learned Similarity Metric}},
url = {http://arxiv.org/abs/1512.09300},
year = {2016}
}
@inproceedings{schramm2017acappella,
author = {Schramm, Rodrigo and McLeod, Andrew and Steedman, Mark and Benetos, Emmanouil},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Multi-Pitch Detection and Voice Assignment for a Cappella Recordings of Multiple Singers.pdf:pdf},
keywords = {Multi-F0},
mendeley-tags = {Multi-F0},
pages = {552--559},
title = {{Multi-Pitch Detection and Voice Assignment for a Cappella Recordings of Multiple Singers}},
year = {2017}
}
@inproceedings{lostanlen2016spiral,
abstract = {Musical performance combines a wide range of pitches, nuances, and expressive techniques. Audio-based classification of musical instruments thus requires to build signal representations that are invariant to such transformations. This article investigates the construction of learned convolutional architectures for instrument recognition, given a limited amount of annotated training data. In this context, we benchmark three different weight sharing strategies for deep convolutional networks in the time-frequency domain: temporal kernels; time-frequency kernels; and a linear combination of time-frequency kernels which are one octave apart, akin to a Shepard pitch spiral. We provide an acoustical interpretation of these strategies within the source-filter framework of quasi-harmonic sounds with a fixed spectral envelope, which are archetypal of musical notes. The best classification accuracy is obtained by hybridizing all three convolutional layers into a single deep learning architecture.},
archivePrefix = {arXiv},
arxivId = {1605.06644},
author = {Lostanlen, Vincent and Cella, Carmine-Emanuele},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
eprint = {1605.06644},
file = {:home/jongwook/Dropbox/References/Deep Convolutional Networks on the Pitch Spiral for Music Information Recognition.pdf:pdf},
pages = {612--618},
title = {{Deep Convolutional Networks on the Pitch Spiral for Music Information Recognition}},
url = {http://arxiv.org/abs/1605.06644},
year = {2016}
}
@inproceedings{hawthorne2018onsetsframes,
abstract = {We advance the state of the art in polyphonic piano music transcription by using a deep convolutional and recurrent neural network which is trained to jointly predict onsets and frames. Our model predicts pitch onset events and then uses those predictions to condition framewise pitch predictions. During inference, we restrict the predictions from the framewise detector by not allowing a new note to start unless the onset detector also agrees that an onset for that pitch is present in the frame. We focus on improving onsets and offsets together instead of either in isolation as we believe this correlates better with human musical perception. Our approach results in over a 100% relative improvement in note F1 score (with offsets) on the MAPS dataset. Furthermore, we extend the model to predict relative velocities of normalized audio which results in more natural-sounding transcriptions.},
archivePrefix = {arXiv},
arxivId = {1710.11153},
author = {Hawthorne, Curtis and Elsen, Erich and Song, Jialin and Roberts, Adam and Simon, Ian and Raffel, Colin and Engel, Jesse and Oore, Sageev and Eck, Douglas},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
eprint = {1710.11153},
file = {:home/jongwook/Dropbox/References/Onsets and frames dual-objective piano transcription.pdf:pdf;:home/jongwook/Dropbox/References/Onsets and Frames Dual-Objective Piano Transcription.pdf:pdf},
title = {{Onsets and Frames: Dual-Objective Piano Transcription}},
url = {http://arxiv.org/abs/1710.11153},
year = {2018}
}
@inproceedings{gao2017nmf,
author = {Gao, Lufei and Su, Li and Yang, Yi Hsuan and Lee, T.},
booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}},
doi = {10.1109/ICASSP.2017.7952164},
file = {:home/jongwook/Dropbox/References/Polyphonic Piano Note Transcription with Non-Negative Matrix Factorization of Differential Spectrogram.pdf:pdf},
isbn = {9781509041176},
issn = {15206149},
keywords = {Music information retrieval,differential spectrogram,non-negative matrix factorization,spectral flux},
title = {{Polyphonic Piano Note Transcription with Non-Negative Matrix Factorization of Differential Spectrogram}},
year = {2017}
}
@inproceedings{choi2016crnn,
archivePrefix = {arXiv},
arxivId = {1609.04243},
author = {Choi, Keunwoo and Fazekas, George and Sandler, Mark and Cho, Kyunghyun},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
doi = {10.1.1.302.7795},
eprint = {1609.04243},
file = {:home/jongwook/Dropbox/References/Convolutional Recurrent Neural Networks for Music Classification.pdf:pdf},
isbn = {9789881701282},
issn = {15209210},
keywords = {CRNN},
mendeley-tags = {CRNN},
pages = {1--5},
title = {{Convolutional Recurrent Neural Networks for Music Classification}},
url = {http://arxiv.org/abs/1609.04243},
year = {2016}
}
@misc{brown1994casa,
author = {Brown, Guy J. and Cooke, Martin},
booktitle = {Computer Speech & Language},
doi = {10.1006/csla.1994.1016},
file = {:home/jongwook/Dropbox/References/Computational Auditory Scene Analysis.pdf:pdf},
isbn = {0805822836},
issn = {08852308},
pages = {297--336},
pmid = {7510225},
title = {{Computational Auditory Scene Analysis}},
volume = {8},
year = {1994}
}
@article{burda2016vae,
archivePrefix = {arXiv},
arxivId = {1509.00519},
author = {Burda, Yuri and Crosse, Roger and Salakhutdinov, Ruslan},
eprint = {1509.00519},
file = {:home/jongwook/Dropbox/References/Importance weighted autoencoders.pdf:pdf},
journal = {arXiv preprint arXiv:1509.00519},
keywords = {VAE},
mendeley-tags = {VAE},
title = {{Importance weighted autoencoders}},
url = {https://arxiv.org/abs/1509.00519},
year = {2016}
}
@article{marolt2004connectionist,
abstract = {In this paper, we present a connectionist approach to automatic transcription of polyphonic piano music. We first compare the performance of several neural network models on the task of recognizing tones from time-frequency representation of a musical signal. We then propose a new partial tracking technique, based on a combination of an auditory model and adaptive oscillator networks. We show how synchronization of adaptive oscillators can be exploited to track partials in a musical signal. We also present an extension of our technique for tracking individual partials to a method for tracking groups of partials by joining adaptive oscillators into networks. We show that oscillator networks improve the accuracy of transcription with neural networks. We also provide a short overview of our entire transcription system and present its performance on transcriptions of several synthesized and real piano recordings. Results show that our approach represents a viable alternative to existing transcription systems.},
author = {Marolt, Matija},
doi = {10.1109/TMM.2004.827507},
file = {:home/jongwook/Dropbox/References/A Connectionist Approach to Automatic Transcription of Polyphonic Piano Music.pdf:pdf},
isbn = {1520-9210},
issn = {15209210},
journal = {{IEEE} Transactions on Multimedia},
keywords = {Adaptive oscillators,Music transcription,Neural networks},
number = {3},
pages = {439--449},
title = {{A Connectionist Approach to Automatic Transcription of Polyphonic Piano Music}},
volume = {6},
year = {2004}
}
@inproceedings{rifai2011contractive,
author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
file = {:home/jongwook/Dropbox/References/Contractive Auto-Encoders Explicit Invariance During Feature Extraction.pdf:pdf},
keywords = {Auto-Encoders},
mendeley-tags = {Auto-Encoders},
pages = {833--840},
title = {{Contractive Auto-Encoders: Explicit Invariance During Feature Extraction}},
year = {2011}
}
@inproceedings{kingma2014vae,
author = {Kingma, Diederik P and Welling, Max},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:home/jongwook/Dropbox/References/Auto-Encoding Variational Bayes.pdf:pdf},
keywords = {VAE},
mendeley-tags = {VAE},
title = {{Auto-Encoding Variational Bayes}},
year = {2014}
}
@inproceedings{karpathy2017desc,
archivePrefix = {arXiv},
arxivId = {1412.2306},
author = {Karpathy, Andrej and Fei-Fei, Li},
booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}},
doi = {10.1109/TPAMI.2016.2598339},
eprint = {1412.2306},
file = {:home/jongwook/Dropbox/References/Deep Visual-Semantic Alignments for Generating Image Descriptions.pdf:pdf},
isbn = {9781467369640},
issn = {01628828},
keywords = {Image captioning,deep neural networks,language model,recurrent neural network,visual-semantic embeddings},
pmid = {16873662},
title = {{Deep Visual-Semantic Alignments for Generating Image Descriptions}},
year = {2017}
}
@inproceedings{glorot2010initialization,
author = {Glorot, Xavier and Bengio, Yoshua},
booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics {(AISTATS)}},
file = {:home/jongwook/Dropbox/References/Understanding the Difficulty of Training Deep Feedforward Neural Networks.pdf:pdf},
title = {{Understanding the Difficulty of Training Deep Feedforward Neural Networks}},
year = {2010}
}
@inproceedings{scholz2016mirex,
author = {Scholz, Ricardo and Ramalho, Geber and Cabral, Giordano},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Cross Task Study on {MIREX} Recent Results An Index for Evolution Measurement and Some Stagnation Hypotheses.pdf:pdf},
pages = {372--378},
title = {{Cross Task Study on {MIREX} Recent Results: An Index for Evolution Measurement and Some Stagnation Hypotheses}},
url = {https://www.researchgate.net/profile/Ricardo_Scholz3/publication/303558629_Cross_Task_Study_on_MIREX_Recent_Results_an_Index_for_Evolution_Measurement_and_Some_Stagnation_Hypotheses/links/57ecf3a108ae93b7fa95aec3.pdf https://wp.nyu.edu/ismir2016/wp-conten},
year = {2016}
}
@article{duan2010bach10,
author = {Duan, Zhiyao and Pardo, Bryan and Zhang, Changshui},
doi = {10.1109/TASL.2010.2042119},
file = {:home/jongwook/Dropbox/References/Multiple fundamental frequency estimation by modeling spectral peaks and non-peak regions.pdf:pdf},
isbn = {15587916},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
number = {8},
pages = {2121--2133},
title = {{Multiple fundamental frequency estimation by modeling spectral peaks and non-peak regions}},
volume = {18},
year = {2010}
}
@article{hadjeres2016deepbach,
abstract = {This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach's strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.},
archivePrefix = {arXiv},
arxivId = {1612.01010},
author = {Hadjeres, Ga{\"{e}}tan and Pachet, Fran{\c{c}}ois and Nielsen, Frank},
eprint = {1612.01010},
file = {:home/jongwook/Dropbox/References/DeepBach a Steerable Model for Bach Chorales Generation.pdf:pdf},
isbn = {4573196480018},
journal = {arXiv preprint arXiv:1612.01010},
title = {{DeepBach: a Steerable Model for Bach Chorales Generation}},
url = {http://arxiv.org/abs/1612.01010},
year = {2016}
}
@inproceedings{nam2011classification,
abstract = {Recently unsupervised feature learning methods have shown great promise as a way of extracting features from high dimensional data, such as image or audio. In this paper, we apply deep belief networks to musical data and evaluate the learned feature representations on classification-based polyphonic piano transcription. We also suggest a way of training classifiers jointly for multiple notes to improve training speed and classification performance. Our method is evaluated on three public piano datasets. The results showthat the learned features outperform the baseline features, and also our method gives significantly better frame-level accuracy than other state-of-the-art music transcription methods.},
author = {Nam, J and Ngiam, J and Lee, Honglak and Slaney, Malcolm},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/A Classification-Based Polyphonic Piano Transcription Approach Using Learned Feature Representations.pdf:pdf},
isbn = {9780615548654},
pages = {175--180},
title = {{A Classification-Based Polyphonic Piano Transcription Approach Using Learned Feature Representations.}},
url = {http://www.ismir2011.ismir.net/papers/PS2-1.pdf},
year = {2011}
}
@inproceedings{mor2019universal,
author = {Mor, Noam and Wolf, Lior and Polyak, Adam},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:home/jongwook/Dropbox/References/A Universal Music Translation Network.pdf:pdf},
title = {{A Universal Music Translation Network}},
year = {2019}
}
@inproceedings{szegedy2015inception,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:home/jongwook/Dropbox/References/Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {08866236},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2016}
}
@inproceedings{harte2005chromagram,
author = {Harte, Christopher A. and Sandler, Mark B.},
booktitle = {Proceedings of the {AES} Convention},
file = {:home/jongwook/Dropbox/References/Automatic Chord Identifcation using a Quantised Chromagram.pdf:pdf},
pages = {1--21},
title = {{Automatic Chord Identifcation using a Quantised Chromagram}},
year = {2005}
}
@article{hochreiter1997lstm,
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
file = {:home/jongwook/Dropbox/References/Long Short-Term Memory.pdf:pdf},
journal = {Neural Computation},
keywords = {LSTM,RNN},
mendeley-tags = {LSTM,RNN},
number = {8},
pages = {1735--1780},
publisher = {MIT Press},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@inproceedings{mescheder2017gan,
archivePrefix = {arXiv},
arxivId = {1705.10461},
author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1705.10461},
file = {:home/jongwook/Dropbox/References/The Numerics of GANs.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{The Numerics of GANs}},
url = {http://arxiv.org/abs/1705.10461},
year = {2017}
}
@inproceedings{zhang2018mixup,
archivePrefix = {arXiv},
arxivId = {1710.09412},
author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1710.09412},
file = {:home/jongwook/Dropbox/References/mixup Beyond Empirical Risk Minimization.pdf:pdf},
title = {{mixup: Beyond Empirical Risk Minimization}},
url = {http://arxiv.org/abs/1710.09412},
year = {2018}
}
@article{grey1977multidimensional,
abstract = {PMID: 560400},
author = {Grey, John M},
doi = {10.1121/1.381428},
file = {:home/jongwook/Dropbox/References/Multidimensional Perceptual Scaling of Musical Timbres.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {5},
pages = {1270--7},
pmid = {560400},
title = {{Multidimensional Perceptual Scaling of Musical Timbres.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/560400},
volume = {61},
year = {1977}
}
@inproceedings{hershey2016deepclustering,
abstract = {We address the problem of acoustic source separation in a deep learning framework we call "deep clustering." Rather than directly estimating signals or masking functions, we train a deep network to produce spectrogram embeddings that are discriminative for partition labels given in training data. Previous deep network approaches provide great advantages in terms of learning power and speed, but previously it has been unclear how to use them to separate signals in a class-independent way. In contrast, spectral clustering approaches are flexible with respect to the classes and number of items to be segmented, but it has been unclear how to leverage the learning power and speed of deep networks. To obtain the best of both worlds, we use an objective function that to train embeddings that yield a low-rank approximation to an ideal pairwise affinity matrix, in a class-independent way. This avoids the high cost of spectral factorization and instead produces compact clusters that are amenable to simple clustering methods. The segmentations are therefore implicitly encoded in the embeddings, and can be "decoded" by clustering. Preliminary experiments show that the proposed method can separate speech: when trained on spectrogram features containing mixtures of two speakers, and tested on mixtures of a held-out set of speakers, it can infer masking functions that improve signal quality by around 6dB. We show that the model can generalize to three-speaker mixtures despite training only on two-speaker mixtures. The framework can be used without class labels, and therefore has the potential to be trained on a diverse set of sound types, and to generalize to novel sources. We hope that future work will lead to segmentation of arbitrary sounds, with extensions to microphone array methods as well as image segmentation and other domains.},
author = {Hershey, John R. and Chen, Zhuo and {Le Roux}, Jonathan and Watanabe, Shinji},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
doi = {10.1109/ICASSP.2016.7471631},
file = {:home/jongwook/Dropbox/References/Deep Clustering Discriminative Embeddings for Segmentation and Separation.pdf:pdf},
isbn = {9781479999880},
issn = {15206149},
keywords = {clustering,deep learning,embedding,speech separation},
pages = {31--35},
publisher = {IEEE},
title = {{Deep Clustering: Discriminative Embeddings for Segmentation and Separation}},
year = {2016}
}
@article{goodfellow2016gan,
archivePrefix = {arXiv},
arxivId = {1701.00160},
author = {Goodfellow, Ian},
doi = {10.1001/jamainternmed.2016.8245},
eprint = {1701.00160},
file = {:home/jongwook/Dropbox/References/NIPS 2016 Tutorial Generative Adversarial Networks.pdf:pdf},
isbn = {1581138285},
issn = {0253-0465},
journal = {arXiv preprint arXiv:1701.00160},
pmid = {15040217},
title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1701.00160},
year = {2016}
}
@inproceedings{miyato2018spectral,
author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:home/jongwook/Dropbox/References/Spectral Normalization for Generative Adversarial Networks.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Spectral Normalization for Generative Adversarial Networks}},
year = {2018}
}
@article{yoshii2012nonparametric,
abstract = {The statistical multipitch analyzer described in this paper estimates multiple fundamental frequencies (F0s) in polyphonic music audio signals produced by pitched instruments. It is based on hierarchical nonparametric Bayesian models that can deal with uncertainty of unknown random variables such as model complexities (e.g., the number of F0s and the number of harmonic partials), model parameters (e.g., the values of F0s and the relative weights of harmonic partials), and hyperparameters (i.e., prior knowledge on complexities and parameters). Using these models, we propose a statistical method called infinite latent harmonic allocation (iLHA). To avoid model-complexity control, we allow the observed spectra to contain an unbounded number of sound sources (F0s), each of which is allowed to contain an unbounded number of harmonic partials. More specifically, to model a set of time-sliced spectra, we formulated nested infinite Gaussian mixture models based on hierarchical and generalized Dirichlet processes. To avoid manual tuning of influential hyperparameters, we put noninformative hyperprior distributions on them in a hierarchical manner. For efficient Bayesian inference, we used a modern technique called collapsed variational Bayes. In comparative experiments using audio recordings of piano and guitar solo performances, iLHA yielded promising results and we found that there would be room for improvement based on modeling of temporal continuity and spectral smoothness.},
author = {Yoshii, Kazuyoshi and Goto, Masataka},
doi = {10.1109/TASL.2011.2164530},
file = {:home/jongwook/Dropbox/References/A Nonparametric Bayesian Multipitch Analyzer Based on Infinite Latent Harmonic Allocation.pdf:pdf},
isbn = {9789039353813},
issn = {15587924},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Bayesian nonparametrics,Dirichlet process,infinite latent harmonic allocation (iLHA),multipitch analysis},
number = {3},
pages = {717--730},
title = {{A Nonparametric Bayesian Multipitch Analyzer Based on Infinite Latent Harmonic Allocation}},
volume = {20},
year = {2012}
}
@inproceedings{rezende2014backprop,
archivePrefix = {arXiv},
arxivId = {1401.4082},
author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
doi = {10.1051/0004-6361/201527329},
eprint = {1401.4082},
file = {:home/jongwook/Dropbox/References/Stochastic Backpropagation and Approximate Inference in Deep Generative Models.pdf:pdf},
isbn = {9781634393973},
issn = {10495258},
pmid = {23459267},
title = {{Stochastic Backpropagation and Approximate Inference in Deep Generative Models}},
url = {http://arxiv.org/abs/1401.4082},
volume = {32},
year = {2014}
}
@article{perez2018film,
author = {Perez, Ethan and Strub, Florian and {De Vries}, Harm and Dumoulin, Vincent and Courville, Aaron},
file = {:home/jongwook/Dropbox/References/{FiLM} Visual Reasoning with a General Conditioning Layer.pdf:pdf},
journal = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
title = {{{FiLM}: Visual Reasoning with a General Conditioning Layer}},
year = {2017}
}
@inproceedings{emmanouil2012transcription,
author = {Benetos, Emmanouil and Dixon, Simon and Giannoulis, Dimitrios and {Kirchhoff, Holger}, Anssi Klapuri},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Automatic Music Transcription Breaking the Glass Ceiling.pdf:pdf},
isbn = {9789727521449},
pages = {379--384},
title = {{Automatic Music Transcription: Breaking the Glass Ceiling}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.294.4098},
year = {2012}
}
@article{skerryryan2018prosody,
archivePrefix = {arXiv},
arxivId = {arXiv:submit/2205421},
author = {{RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J. Weiss, Rob Clark}, Rif A. Saurous},
eprint = {2205421},
file = {:home/jongwook/Dropbox/References/Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron(2).pdf:pdf},
journal = {arXiv preprint arXiv:1803.09047},
primaryClass = {arXiv:submit},
title = {{Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron}},
year = {2018}
}
@article{plumbley2002transcription,
author = {Plumbley, M. D. and Abdallah, S. A. and Bello, J. P. and Davies, M. E. and Monti, G. and Sandler, M. B.},
file = {:home/jongwook/Dropbox/References/Automatic Music Transcription and Audio Source Separation.pdf:pdf},
journal = {Cybernetics and Systems: An International Journal},
pages = {603--627},
title = {{Automatic Music Transcription and Audio Source Separation}},
volume = {6},
year = {2002}
}
@article{sigtia2016endtoend,
abstract = {We present a supervised neural network model for polyphonic piano music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language model. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We perform two sets of experiments. We investigate various neural network architectures for the acoustic models and also investigate the effect of combining acoustic and music language model predictions using the proposed architecture. We compare performance of the neural network based acoustic models with two popular unsupervised acoustic models. Results show that convolutional neural network acoustic models yields the best performance across all evaluation metrics. We also observe improved performance with the application of the music language models. Finally, we present an efficient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications.},
archivePrefix = {arXiv},
arxivId = {1508.01774},
author = {Sigtia, Siddharth and Benetos, Emmanouil and DIxon, Simon},
doi = {10.1109/TASLP.2016.2533858},
eprint = {1508.01774},
file = {:home/jongwook/Dropbox/References//An End-to-End Neural Network for Polyphonic Piano Music Transcription.pdf:pdf},
issn = {23299290},
journal = {{IEEE/ACM} Transactions on Audio, Speech, and Language Processing},
keywords = {Automatic music transcription,Deep learning,End-to-End,Music language models,Recurrent neural networks,Transcription},
mendeley-tags = {End-to-End,Transcription},
number = {5},
pages = {927--939},
publisher = {IEEE Press},
title = {{An End-to-End Neural Network for Polyphonic Piano Music Transcription}},
url = {http://arxiv.org/abs/1508.01774},
volume = {24},
year = {2016}
}
@inproceedings{vercoe1984performer,
author = {Vercoe, Barry},
booktitle = {Proceedings of the International Computer Music Conference {(ICMC)}},
file = {:home/jongwook/Dropbox/References/The Synthetic Performer in the Context of Live Performance.pdf:pdf},
issn = {2223-3881},
pages = {199--200},
title = {{The Synthetic Performer in the Context of Live Performance}},
year = {1984}
}
@inproceedings{sigtia2014lm,
author = {Sigtia, Siddharth and Benetos, Emmanouil and Cherla, Srikanth and Weyde, Tillman and d'Avila Garcez, a. and Dixon, Simon},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/An RNN-based Music Language Model for Improving Automatic Music Transcription.pdf:pdf},
keywords = {M Music and Books on Music,QA75 Electronic computers. Computer science},
number = {Ismir},
pages = {53--58},
title = {{An RNN-based Music Language Model for Improving Automatic Music Transcription}},
year = {2014}
}
@inproceedings{cogliati2017metric,
author = {Cogliati, Andrea and Duan, Zhiyao},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/A Metric for Music Notation Transcription Accuracy.pdf:pdf},
keywords = {Metric},
mendeley-tags = {Metric},
pages = {407--413},
title = {{A Metric for Music Notation Transcription Accuracy}},
year = {2017}
}
@inproceedings{kim2019adversarial,
abstract = {Automatic music transcription is considered to be one of the hardest problems in music information retrieval, yet recent deep learning approaches have achieved substantial improvements on transcription performance. These approaches commonly employ supervised learning models that predict various time-frequency representations, by minimizing element-wise losses such as the cross entropy function. However, applying the loss in this manner assumes conditional independence of each label given the input, and thus cannot accurately express inter-label dependencies. To address this issue, we introduce an adversarial training scheme that operates directly on the time-frequency representations and makes the output distribution closer to the ground-truth. Through adversarial learning, we achieve a consistent improvement in both frame-level and note-level metrics over Onsets and Frames, a state-of-the-art music transcription model. Our results show that adversarial learning can significantly reduce the error rate while increasing the confidence of the model estimations. Our approach is generic and applicable to any transcription model based on multi-label predictions, which are very common in music signal analysis.},
archivePrefix = {arXiv},
arxivId = {1906.08512},
author = {Kim, Jong Wook and Bello, Juan Pablo},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
eprint = {1906.08512},
file = {:home/jongwook/Dropbox/References/Adversarial Learning for Improved Onsets and Frames Music Transcription.pdf:pdf},
title = {{Adversarial Learning for Improved Onsets and Frames Music Transcription}},
url = {http://arxiv.org/abs/1906.08512},
year = {2019}
}
@article{ganin2015domain,
abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
archivePrefix = {arXiv},
arxivId = {1505.07818},
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
doi = {10.1088/1475-7516/2015/08/013},
eprint = {1505.07818},
file = {:home/jongwook/Dropbox/References/Domain-Adversarial Training of Neural Networks.pdf:pdf},
isbn = {15324435},
issn = {1475-7516},
journal = {Journal of Machine Learning Research},
pages = {1--35},
title = {{Domain-Adversarial Training of Neural Networks}},
url = {http://arxiv.org/abs/1505.07818},
volume = {17},
year = {2016}
}
@article{russakovsky2015imagenet,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:home/jongwook/Dropbox/References/ImageNet Large Scale Visual Recognition Challenge.pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
mendeley-tags = {Dataset},
number = {3},
pages = {211--252},
pmid = {16190471},
publisher = {Springer US},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@inproceedings{goto2003rwc,
author = {Goto, Masataka and Hashiguchi, Hiroki and Nishimura, Takuichi and Oka, Ryuichi},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/RWC Music Database Music Genre Database and Musical Instrument Sound Database.pdf:pdf},
keywords = {Dataset},
mendeley-tags = {Dataset},
publisher = {Johns Hopkins University},
title = {{RWC Music Database: Music Genre Database and Musical Instrument Sound Database}},
year = {2003}
}
@inproceedings{southall2017drum,
author = {Southall, Carl and Stables, Ryan and Hockman, Jason},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Automatic Drum Transcription for Polyphonic Recordings using Soft Attention Mechanisms and Convolutional Neural Networks.pdf:pdf},
keywords = {Attention,Transcription},
mendeley-tags = {Attention,Transcription},
pages = {606--612},
title = {{Automatic Drum Transcription for Polyphonic Recordings using Soft Attention Mechanisms and Convolutional Neural Networks}},
year = {2017}
}
@inproceedings{lee2001nmf,
archivePrefix = {arXiv},
arxivId = {arXiv:cs/0408058v1},
author = {Lee, Daniel D. and Seung, H. Sebastian},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {10.1109/IJCNN.2008.4634046},
eprint = {0408058v1},
file = {:home/jongwook/Dropbox/References/Algorithms for Non-Negative Matrix Factorization.pdf:pdf},
isbn = {9781424418206},
issn = {10987576},
pages = {556--562},
pmid = {10548103},
primaryClass = {arXiv:cs},
title = {{Algorithms for Non-Negative Matrix Factorization}},
year = {2001}
}
@article{lu2009recommendation,
author = {Lu, Cheng Che and Tseng, Vincent S.},
doi = {10.1016/j.eswa.2009.01.074},
file = {:home/jongwook/Dropbox/References/A Novel Method for Personalized Music Recommendation.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
number = {6},
pages = {10035--10044},
publisher = {Elsevier Ltd},
title = {{A Novel Method for Personalized Music Recommendation}},
volume = {36},
year = {2009}
}
@inproceedings{vincent2008nmf,
abstract = {Polyphonic pitch transcription consists of estimating the onset time, duration and pitch of each note in a music signal. This task is difficult in general, due to the wide range of possible instruments. This issue has been studied using adaptive models such as Nonnegative Matrix Factorization (NMF), which describe the signal as a weighted sum of basis spectra. However basis spectra representing multiple pitches result in inaccurate transcription. To avoid this, we propose a family of constrained NMF models, where each basis spectrum is expressed as a weighted sum of narrowband spectra consisting of a few adjacent partials at harmonic or inharmonic frequencies. The model parameters are adapted via combined multiplicative and Newton updates. The proposed method is shown to outperform standard NMF on a database of piano excerpts.},
author = {Vincent, Emmanuel and Benin, Nancy and Badeau, Roland},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
doi = {10.1109/ICASSP.2008.4517558},
file = {:home/jongwook/Dropbox/References/Harmonic and Inharmonic Nonnegative Matrix Factorization for Polyphonic Pitch Transcription.pdf:pdf},
isbn = {1424414849},
issn = {15206149},
keywords = {Harmonicity,Inharmonicity,Nonnegative matrix factorization,Pitch transcription,Spectral smoothness},
pages = {109--112},
pmid = {4517558},
title = {{Harmonic and Inharmonic Nonnegative Matrix Factorization for Polyphonic Pitch Transcription}},
year = {2008}
}
@inproceedings{liu2017hyperspherical,
archivePrefix = {arXiv},
arxivId = {1711.03189},
author = {Liu, Weiyang and Zhang, Yan-Ming and Li, Xingguo and Yu, Zhiding and Dai, Bo and Zhao, Tuo and Song, Le},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1711.03189},
file = {:home/jongwook/Dropbox/References/Deep Hyperspherical Learning.pdf:pdf},
title = {{Deep Hyperspherical Learning}},
url = {http://arxiv.org/abs/1711.03189},
year = {2017}
}
@article{klapuri2003multiple,
author = {Klapuri, Anssi P},
file = {:home/jongwook/Dropbox/References/Multiple Fundamental Frequency Estimation Based on Harmonicity and Spectral Smoothness.pdf:pdf},
journal = {{IEEE} Transactions on Speech and Audio Processing},
number = {6},
pages = {804--816},
publisher = {IEEE},
title = {{Multiple Fundamental Frequency Estimation Based on Harmonicity and Spectral Smoothness}},
volume = {11},
year = {2003}
}
@article{kalingeri2016generation,
author = {Kalingeri, Vasanth and Grandhe, Srikanth},
file = {:home/jongwook/Dropbox/References/Music Generation with Deep Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1612.04928},
title = {{Music Generation with Deep Learning}},
year = {2016}
}
@inproceedings{liutkus2012separation,
author = {Liutkus, Antoine and Rafii, Zafar and Badeau, Roland and Pardo, Bryan and Richard, Gael},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
doi = {10.1109/ICASSP.2012.6287815},
file = {:home/jongwook/Dropbox/References/Adaptive Filtering for MusicVoice Separation Exploiting the Repeating Musical Structure.pdf:pdf},
isbn = {9781467300469},
issn = {15206149},
pages = {53--56},
title = {{Adaptive Filtering for Music/Voice Separation Exploiting the Repeating Musical Structure}},
year = {2012}
}
@article{yeh2010mffe,
abstract = {This paper presents a frame-based system for es- timating multiple fundamental frequencies (F0s) of polyphonic music signals based on the short-time Fourier transform (STFT) representation. To estimate the number of sources along with their F0s, it is proposed to estimate the noise level beforehand and then jointly evaluate all the possible combinations among pre-selected F0 candidates. Given a set of F0 hypotheses, their hypothetical partial sequences are derived, taking into account where partial overlap may occur. A score function is used to select the plausible sets of F0 hypotheses. To infer the best combination, hypothetical sources are progressively combined and iteratively verified. A hypothetical source is considered valid if it either explains more energy than the noise, or improves significantly the envelope smoothness once the overlapping partials are treated. The proposed system has been submitted to Music Information Retrieval Evaluation eXchange (MIREX) 2007 and 2008 contests where the accuracy has been evaluated with respect to the number of sources inferred and the precision of the F0s estimated. The encouraging results demonstrate its competitive performance among the state-of-the-art methods.},
author = {Yeh, Chunghsin and Roebel, Axel and Rodet, Xavier},
doi = {10.4269/ajtmh.16-0792},
file = {:home/jongwook/Dropbox/References/Multiple Fundamental Frequency Estimation and Polyphony Inference of Polyphonic Music Signals.pdf:pdf},
isbn = {03059332},
issn = {00029637},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
number = {6},
pages = {1116--1126},
pmid = {28070007},
title = {{Multiple Fundamental Frequency Estimation and Polyphony Inference of Polyphonic Music Signals}},
volume = {18},
year = {2010}
}
@article{mahendran2016deepdream,
author = {Mahendran, Aravindh and Vedaldi, Andrea},
file = {:home/jongwook/Dropbox/References/Visualizing Deep Convolutional Neural Networks using Natural Pre-Images.pdf:pdf},
journal = {International Journal of Computer Vision},
number = {3},
pages = {233--255},
publisher = {Springer},
title = {{Visualizing Deep Convolutional Neural Networks using Natural Pre-Images}},
volume = {120},
year = {2016}
}
@article{rosca2017alphagan,
abstract = {Auto-encoding generative adversarial networks (GANs) combine the standard GAN algorithm, which discriminates between real and model-generated data, with a reconstruction loss given by an auto-encoder. Such models aim to prevent mode collapse in the learned generative model by ensuring that it is grounded in all the available training data. In this paper, we develop a principle upon which auto-encoders can be combined with generative adversarial networks by exploiting the hierarchical structure of the generative model. The underlying principle shows that variational inference can be used a basic tool for learning, but with the in- tractable likelihood replaced by a synthetic likelihood, and the unknown posterior distribution replaced by an implicit distribution; both synthetic likelihoods and implicit posterior distributions can be learned using discriminators. This allows us to develop a natural fusion of variational auto-encoders and generative adversarial networks, combining the best of both these methods. We describe a unified objective for optimization, discuss the constraints needed to guide learning, connect to the wide range of existing work, and use a battery of tests to systematically and quantitatively assess the performance of our method.},
archivePrefix = {arXiv},
arxivId = {1706.04987},
author = {Rosca, Mihaela and Lakshminarayanan, Balaji and Warde-Farley, David and Mohamed, Shakir},
eprint = {1706.04987},
file = {:home/jongwook/Dropbox/References/Variational Approaches for Auto-Encoding Generative Adversarial Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1706.04987},
title = {{Variational Approaches for Auto-Encoding Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1706.04987},
year = {2017}
}
@inproceedings{boulangerlewandowski2012temporal,
abstract = {We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.},
archivePrefix = {arXiv},
arxivId = {1206.6392},
author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1206.6392},
file = {:home/jongwook/Dropbox/References/Modeling Temporal Dependencies in High-Dimensional Sequences Application to Polyphonic Music Generation and Transcription.pdf:pdf},
isbn = {978-1-4503-1285-1},
title = {{Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription}},
url = {http://arxiv.org/abs/1206.6392},
year = {2012}
}
@article{tolonen2000multipitch,
abstract = {A computationally efficient model for multipitch and periodicity\nanalysis of complex audio signals is presented. The model essentially\ndivides the signal into two channels, below and above 1000 Hz, computes\na &ldquo;generalized&rdquo; autocorrelation of the low-channel signal\nand of the envelope of the high-channel signal, and sums the\nautocorrelation functions. The summary autocorrelation function (SACF)\nis further processed to obtain an enhanced SACF (ESACF). The SACF and\nESACP representations are used in observing the periodicities of the\nsignal. The model performance is demonstrated to be comparable to those\nof recent time-domain models that apply a multichannel analysis. In\ncontrast to the multichannel models, the proposed pitch analysis model\ncan be run in real time using typical personal computers. The parameters\nof the model are experimentally tuned for best multipitch discrimination\nwith typical mixtures of complex tones. The proposed pitch analysis\nmodel may be used in complex audio signal processing applications, such\nas sound source separation, computational auditory scene analysis, and\nstructural representation of audio signals. The performance of the model\nis demonstrated by pitch analysis examples using sound mixtures which\nare available for download at\nhttp://www.acoustics.hut.fi/-ttolonen/pitchAnalysis/},
author = {Tolonen, Tero and Karjalainen, Matti},
doi = {10.1109/89.876309},
file = {:home/jongwook/Dropbox/References/A computationally efficient multipitch analysis model.pdf:pdf},
isbn = {1063-6676},
issn = {10636676},
journal = {{IEEE} Transactions on Speech and Audio Processing},
keywords = {Auditory modeling,Multipitch analysis,Periodicity analysis,Pitch perception},
number = {6},
pages = {708--716},
title = {{A computationally efficient multipitch analysis model}},
volume = {8},
year = {2000}
}
@inproceedings{he2015prelu,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the {IEEE} International Conference on Computer Vision {(ICCV)}},
file = {:home/jongwook/Dropbox/References/Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf:pdf},
keywords = {Activations},
mendeley-tags = {Activations},
pages = {1026--1034},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
year = {2015}
}
@article{lee2017introspective,
archivePrefix = {arXiv},
arxivId = {1711.08875},
author = {Lee, Kwonjoon and Xu, Weijian and Fan, Fan and Tu, Zhuowen},
eprint = {1711.08875},
file = {:home/jongwook/Dropbox/References/Wasserstein Introspective Neural Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1711.08875},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Wasserstein Introspective Neural Networks}},
url = {http://arxiv.org/abs/1711.08875},
year = {2017}
}
@inproceedings{oord2016pixelrnn,
archivePrefix = {arXiv},
arxivId = {1601.06759},
author = {van den Oord, A{\"{a}}ron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1601.06759},
file = {:home/jongwook/Dropbox/References/Pixel Recurrent Neural Networks.pdf:pdf},
isbn = {9781510829008},
title = {{Pixel Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1601.06759},
year = {2016}
}
@article{oord2016pixelcnn,
abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
archivePrefix = {arXiv},
arxivId = {1606.05328},
author = {van den Oord, A{\"{a}}ron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
eprint = {1606.05328},
file = {:home/jongwook/Dropbox/References/Conditional Image Generation with PixelCNN Decoders.pdf:pdf},
journal = {Advances in Neural Information Processing Systems {(NIPS)}},
title = {{Conditional Image Generation with PixelCNN Decoders}},
url = {http://arxiv.org/abs/1606.05328},
year = {2016}
}
@inproceedings{kelz2017entanglement,
author = {Kelz, Rainer and Widmer, Gerhard},
booktitle = {Proceedings of the {AES} Conference on Semantic Audio},
file = {:home/jongwook/Dropbox/References/An Experimental Analysis of the Entanglement Problem in Neural-Network-based Music Transcription Systems.pdf:pdf},
title = {{An Experimental Analysis of the Entanglement Problem in Neural-Network-based Music Transcription Systems}},
year = {2017}
}
@article{hsu2010mir1k,
abstract = {Monaural singing voice separation is an extremely challenging problem. While efforts in pitch-based inference methods have led to considerable progress in voiced singing voice separation, little attention has been paid to the incapability of such methods to separate unvoiced singing voice due to its in harmonic structure and weaker energy. In this paper, we proposed a systematic approach to identify and separate the unvoiced singing voice from the music accompaniment. We have also enhanced the performance of separating voiced singing via a spectral subtraction method. The proposed system follows the framework of computational auditory scene analysis (CASA) which consists of the segmentation stage and the grouping stage. In the segmentation stage, the input song signals are decomposed into small sensory elements in different time-frequency resolutions. The unvoiced sensory elements are then identified by Gaussian mixture models. The experimental results demonstrated that the quality of the separated singing voice is improved for both the unvoiced and voiced parts. Moreover, to deal with the problem of lack of a publicly available dataset for singing voice separation, we have constructed a corpus called MIR-1K (multimedia information retrieval lab, 1000 song clips) where all singing voices and music accompaniments were recorded separately. Each song clip comes with human-labeled pitch values, unvoiced sounds and vocal/non-vocal segments, and lyrics, as well as the speech recording of the lyrics.},
author = {Hsu, Chao-Ling and Jang, Jyh Shing Roger},
doi = {10.1109/TASL.2009.2026503},
file = {:home/jongwook/Dropbox/References/On the Improvement of Singing Voice Separation for Monaural Recordings Using the MIR-1K Dataset.pdf:pdf},
issn = {15587924},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Computational auditory scene analysis (CASA),singing voice separation,unvoiced sound separation},
number = {2},
pages = {310--319},
title = {{On the Improvement of Singing Voice Separation for Monaural Recordings Using the MIR-1K Dataset}},
volume = {18},
year = {2010}
}
@inproceedings{dannenberg1985accompaniment,
author = {Dannenberg, R B},
booktitle = {Proceedings of the International Computer Music Conference {(ICMC)}},
file = {:home/jongwook/Dropbox/References/An On-Line Algorithm for Real-Time Accompaniment.pdf:pdf},
pages = {193--198},
title = {{An On-Line Algorithm for Real-Time Accompaniment}},
year = {1985}
}
@article{lee2012exemplar,
abstract = {Pitch, together with other midlevel music features such as rhythm and timbre, holds the promise of bridging the semantic gap between low-level features and high-level semantics for music understanding. This paper investigates the pitch estimation of a piano music signal by exemplar-based sparse representation. A note exemplar is a segment of a piano note, stored in the dictionary. We first describe how to represent a segment of the piano music signal as a linear combination of a small number of note exemplars from a large note exemplar dictionary and then show how the sparse representation problem can be solved by -regularized minimization. The proposed approach incorporates tuning factor estimation, note candidate selection, and hidden-Markov-model-based smoothing into the estimation process to improve accuracy. Unlike previous approaches, the proposed approach does not require retraining for a new piano. Instead, only a dozen notes of the new piano are needed. This feature is computationally attractive and avoids intense manual labeling. The system performance is evaluated using 70 classical music recordings of two real pianos under different recording conditions. The results show that the proposed system outperforms four state-of-the-art systems.},
author = {Lee, Cheng Te and Yang, Yi Hsuan and Chen, Homer H.},
doi = {10.1109/TMM.2012.2191398},
file = {:home/jongwook/Dropbox/References/Multipitch Estimation of Piano Music by Exemplar-Based Sparse Representation.pdf:pdf},
isbn = {1520-9210},
issn = {15209210},
journal = {{IEEE} Transactions on Multimedia},
keywords = {Content retrieval,l1-regularized minimization,music transcription,pitch estimation,sparse representation},
number = {3},
pages = {608--618},
title = {{Multipitch Estimation of Piano Music by Exemplar-Based Sparse Representation}},
volume = {14},
year = {2012}
}
@inproceedings{benetos2014unpitched,
author = {Benetos, Emmanouil and Ewert, Sebastian and Weyde, Tillman},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:home/jongwook/Dropbox/References/Automatic Transcription of Pitched and Unpitched Sounds from Polyphonic Music.pdf:pdf},
isbn = {9781479928934},
pages = {3131--3135},
title = {{Automatic Transcription of Pitched and Unpitched Sounds from Polyphonic Music}},
year = {2014}
}
@inproceedings{mcfee2015muda,
author = {McFee, Brian and Humphrey, Eric J and Bello, Juan Pablo},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/A Software Framework for Musical Data Augmentation.pdf:pdf},
keywords = {Augmentation},
mendeley-tags = {Augmentation},
pages = {248--254},
title = {{A Software Framework for Musical Data Augmentation.}},
year = {2015}
}
@article{pesek2017hierarchical,
author = {Pesek, Matev{\v{z}} and Leonardis, Ale{\v{s}} and Marolt, Matija},
doi = {10.1371/journal.pone.0169411},
file = {:home/jongwook/Dropbox/References/Robust Real-Time Music Transcription with a Compositional Hierarchical Model.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {1},
title = {{Robust Real-Time Music Transcription with a Compositional Hierarchical Model}},
volume = {12},
year = {2017}
}
@inproceedings{abdallah2015british,
author = {Abdallah, Samer and Alencar-Brayner, Aquiles and Benetos, Emmanouil and Cottrell, Stephen and Dykes, Jason and Gold, Nicolas and Kachkaev, Alexander and Mahey, Mahendra and Tidhar, Dan and Tovell, Adam and Weyde, Tillman and Wolff, Daniel},
booktitle = {Proceedings of the International Workshop on Folk Music Analysis {(FMA)}},
file = {:home/jongwook/Dropbox/References/Automatic Transcription and Pitch Analysis of the British Library World and Traditional Music Collection.pdf:pdf},
pages = {10--12},
title = {{Automatic Transcription and Pitch Analysis of the British Library World and Traditional Music Collection}},
year = {2015}
}
@inproceedings{gulrajani2017wgan,
archivePrefix = {arXiv},
arxivId = {1704.00028},
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {10.1016/j.aqpro.2013.07.003},
eprint = {1704.00028},
file = {:home/jongwook/Dropbox/References/Improved Training of Wasserstein GANs.pdf:pdf},
isbn = {0030-8870},
issn = {00308870},
pages = {5769--5779},
pmid = {24439530},
title = {{Improved Training of Wasserstein GANs}},
url = {http://arxiv.org/abs/1704.00028},
volume = {30},
year = {2017}
}
@article{dubnowski1976acf,
author = {Dubnowski, John J. and Schafer, Ronald W. and Rabiner, Lawrence R.},
doi = {10.1109/TASSP.1976.1162765},
file = {:home/jongwook/Dropbox/References/Real-Time Digital Hardware Pitch Detector.pdf:pdf},
isbn = {0096-3518},
issn = {00963518},
journal = {{IEEE} Transactions on Acoustics, Speech, and Signal Processing},
number = {1},
pages = {2--8},
title = {{Real-Time Digital Hardware Pitch Detector}},
volume = {24},
year = {1976}
}
@article{brundage2018malicious,
author = {Brundage, Miles and Avin, Shahar and Clark, Jack and Toner, Helen and Eckersley, Peter and Garfinkel, Ben and Dafoe, Allan and Scharre, Paul and Zeitzoff, Thomas and Filar, Bobby and Anderson, Hyrum and Roff, Heather and Allen, Gregory C. and Steinhardt, Jacob and Flynn, Carrick and H{\'{E}}igeartaigh, Se{\'{a}}n {\'{O}} and Beard, Simon and Belfield, Haydn and Farquhar, Sebastian and Lyle, Clare and Crootof, Rebecca and Evans, Owain and Page, Michael and Bryson, Joanna and Yampolskiy, Roman and Amodei, Dario},
file = {:home/jongwook/Dropbox/References/The Malicious Use of Artificial Intelligence Forecasting, Prevention, and Mitigation.pdf:pdf},
journal = {arXiv preprint arXiv:1802.07228},
title = {{The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation}},
year = {2018}
}
@inproceedings{theis2015evaluation,
archivePrefix = {arXiv},
arxivId = {1511.01844},
author = {Theis, Lucas and van den Oord, A{\"{a}}ron and Bethge, Matthias},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
doi = {10.1177/096032717100300408},
eprint = {1511.01844},
file = {:home/jongwook/Dropbox/References/A Note on the Evaluation of Generative Models.pdf:pdf},
isbn = {1511.01844},
issn = {1477-1535},
title = {{A Note on the Evaluation of Generative Models}},
year = {2016}
}
@article{dosovitskiy2016generating,
abstract = {Image-generating machine learning models are typically trained with loss functions based on distance in the image space. This often leads to over-smoothed results. We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric better reflects perceptually similarity of images and thus leads to better results. We show three applications: autoencoder training, a modification of a variational autoencoder, and inversion of deep convolutional networks. In all cases, the generated images look sharp and resemble natural images.},
archivePrefix = {arXiv},
arxivId = {1602.02644},
author = {Dosovitskiy, Alexey and Brox, Thomas},
eprint = {1602.02644},
file = {:home/jongwook/Dropbox/References/Generating Images with Perceptual Similarity Metrics based on Deep Networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems {(NIPS)}},
number = {c},
pages = {1--9},
title = {{Generating Images with Perceptual Similarity Metrics based on Deep Networks}},
url = {http://arxiv.org/abs/1602.02644},
volume = {1},
year = {2016}
}
@article{chen2017gcn,
archivePrefix = {arXiv},
arxivId = {1710.04908},
author = {Chen, Meihao and Lin, Zhuoru and Cho, Kyunghyun},
eprint = {1710.04908},
file = {:home/jongwook/Dropbox/References/Graph Convolutional Networks for Classification with a Structured Label Space.pdf:pdf},
journal = {arXiv preprint arXiv:1710.04908},
keywords = {GCN},
mendeley-tags = {GCN},
title = {{Graph Convolutional Networks for Classification with a Structured Label Space}},
url = {http://arxiv.org/abs/1710.04908},
year = {2017}
}
@article{shepard1964circularity,
author = {Shepard, Roger N.},
doi = {10.1121/1.1919362},
file = {:home/jongwook/Dropbox/References/Circularity in Judgments of Relative Pitch.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {12},
pages = {2346--2353},
publisher = {ASA},
title = {{Circularity in Judgments of Relative Pitch}},
url = {http://asa.scitation.org/doi/10.1121/1.1919362},
volume = {36},
year = {1964}
}
@inproceedings{schorkhuber2010cqt,
abstract = {This paper proposes a computationally efficient method for computing the constant-Q transform (CQT) of a time- domain signal. CQT refers to a time-frequency represen- tation where the frequency bins are geometrically spaced and the Q-factors (ratios of the center frequencies to band- widths) of all bins are equal. An inverse transform is pro- posed which enables a reasonable-quality (around 55dB signal-to-noise ratio) reconstruction of the original signal from its CQT coefficients. Here CQTs with high Q-factors, equivalent to 1296 bins per octave, are of particular inter- est. The proposed method is flexible with regard to the number of bins per octave, the applied window function, and the Q-factor, and is particularly suitable for the anal- ysis of music signals. A reference implementation of the proposed methods is published as a Matlab toolbox. The toolbox includes user-interface tools that facilitate spectral data visualization and the indexing and working with the data structure produced by the CQT.},
author = {Sch{\"{o}}rkhuber, Christian and Klapuri, Anssi P.},
booktitle = {Proceedings of the Sound and Music Computing {(SMC)} Conference},
file = {:home/jongwook/Dropbox/References/Constant-Q Transform Toolbox for Music Processing.pdf:pdf},
keywords = {CQT},
mendeley-tags = {CQT},
pages = {3--64},
title = {{Constant-Q Transform Toolbox for Music Processing}},
year = {2010}
}
@inproceedings{emiya2008hmm,
abstract = {This work deals with the automatic transcription of piano recordings into a MIDI symbolic file. The system consists of subsequent stages of onset detection and multipitch estimation and tracking. The latter is based on a Hidden Markov Model framework, embedding a spectral maximum likelihood method for joint pitch estimation. The complexity issue of joint estimation techniques is solved by selecting subsets of simultaneously played notes within a pre-estimated set of candidates. Tests on a large database and comparisons to state-of-the-art methods show promising results.},
author = {Emiya, Valentin and Badeau, Roland and David, Bertrand},
booktitle = {Proceedings of the European Signal Processing Conference {(EUSIPCO)}},
doi = {10.5281/ZENODO.40892},
file = {:home/jongwook/Dropbox/References/Automatic Transcription of Piano Music Based on HMM Tracking of Jointly-Estimated Pitches.pdf:pdf},
issn = {22195491},
title = {{Automatic Transcription of Piano Music Based on HMM Tracking of Jointly-Estimated Pitches}},
year = {2008}
}
@article{ranganath2013bbvi,
archivePrefix = {arXiv},
arxivId = {1401.0118},
author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
eprint = {1401.0118},
file = {:home/jongwook/Dropbox/References/Black Box Variational Inference.pdf:pdf},
isbn = {1401.0118},
issn = {0002-9513},
journal = {Artificial Intelligence and Statistics},
title = {{Black Box Variational Inference}},
year = {2014}
}
@inproceedings{niedermayer2008division,
abstract = {In this paper we present a new method in the style of non-negative matrix factorization for automatic tran- scription of polyphonic music played by a single instru- ment (e.g., a piano). We suggest using a fixed repos- itory of base vectors corresponding to tone models of single pitches played on a certain instrument. This as- sumption turns the blind factorization into a kind of non-negative matrix division for which an algorithm is presented. The same algorithm can be applied for learning the model dictionary from sample tones as well. This method is biased towards the instrument used during the training phase. But this is admissible in applications like performance analysis of solo music. The proposed approach is tested on a Mozart sonata where a symbolic representation is available as well as the recording on a computer controlled grand piano.},
author = {Niedermayer, Bernhard},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Non-Negative Matrix Division For The Automatic Transcription Of Polyphonic Music.pdf:pdf},
isbn = {9780615248493},
pages = {544--549},
title = {{Non-Negative Matrix Division For The Automatic Transcription Of Polyphonic Music}},
year = {2008}
}
@inproceedings{arora2018gan,
author = {Arora, Sanjeev and Zhang, Yi},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:home/jongwook/Dropbox/References/Do GANs Learn the Distribution Some Theory and Empirics.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Do GANs Learn the Distribution? Some Theory and Empirics}},
year = {2018}
}
@inproceedings{sabour2017capsules,
archivePrefix = {arXiv},
arxivId = {1710.09829},
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1710.09829},
file = {:home/jongwook/Dropbox/References/Dynamic Routing Between Capsules.pdf:pdf},
keywords = {Capsule},
mendeley-tags = {Capsule},
title = {{Dynamic Routing Between Capsules}},
url = {http://arxiv.org/abs/1710.09829},
year = {2017}
}
@inproceedings{cheng2015hmm,
author = {Cheng, Tian and Dixon, Simon and Mauch, Matthias},
booktitle = {Proceedings of the European Signal Processing Conference {(EUSIPCO)}},
file = {:home/jongwook/Dropbox/References/Improving Piano Note Tracking by HMM Smoothing.pdf:pdf},
isbn = {9780992862633},
pages = {2054--2058},
title = {{Improving Piano Note Tracking by HMM Smoothing}},
year = {2015}
}
@inproceedings{dubois2005harmonic,
author = {Dubois, Corentin and Davy, Manuel},
booktitle = {{IEEE/SP} Workshop on Statistical Signal Processing},
file = {:home/jongwook/Dropbox/References/Harmonic Tracking Using Sequential Monte Carlo.pdf:pdf},
organization = {IEEE},
pages = {1292--1297},
title = {{Harmonic Tracking Using Sequential Monte Carlo}},
year = {2005}
}
@inproceedings{kim2019synthesis,
author = {Kim, Jong Wook and Bittner, Rachel and Kumar, Aparna and Bello, Juan Pablo},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:home/jongwook/Dropbox/References/Neural Music Synthesis for Flexible Timbre Control.pdf:pdf},
isbn = {9781538646588},
pages = {176--180},
title = {{Neural Music Synthesis for Flexible Timbre Control}},
year = {2019}
}
@inproceedings{sonderby2016ladder,
archivePrefix = {arXiv},
arxivId = {1602.02282},
author = {S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1602.02282},
file = {:home/jongwook/Dropbox/References/Ladder Variational Autoencoders.pdf:pdf},
issn = {10495258},
keywords = {VAE},
mendeley-tags = {VAE},
title = {{Ladder Variational Autoencoders}},
url = {http://arxiv.org/abs/1602.02282},
year = {2016}
}
@article{brahma2016disentanglement,
author = {Brahma, Pratik Prabhanjan and Wu, Dapeng and She, Yiyuan},
doi = {10.1109/TNNLS.2015.2496947},
file = {:home/jongwook/Dropbox/References/Why Deep Learning Works A Manifold Disentanglement Perspective.pdf:pdf},
issn = {21622388},
journal = {{IEEE} Transactions on Neural Networks and Learning Systems},
keywords = {Deep learning,disentanglement,manifold learning,unsupervised feature transformation},
number = {10},
pages = {1997--2008},
title = {{Why Deep Learning Works: A Manifold Disentanglement Perspective}},
volume = {27},
year = {2016}
}
@article{berthelot2017began,
author = {Berthelot, David and Schumm, Tom and Metz, Luke},
file = {:home/jongwook/Dropbox/References/BEGAN Boundary Equilibrium Generative Adversarial Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1703.10717},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{BEGAN: Boundary Equilibrium Generative Adversarial Networks}},
year = {2017}
}
@inproceedings{choi2017transfer,
archivePrefix = {arXiv},
arxivId = {1703.09179},
author = {Choi, Keunwoo and Fazekas, Gy{\"{o}}rgy and Sandler, Mark and Cho, Kyunghyun},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
eprint = {1703.09179},
file = {:home/jongwook/Dropbox/References/Transfer Learning for Music Classification and Regression Tasks.pdf:pdf},
pages = {141--149},
title = {{Transfer Learning for Music Classification and Regression Tasks}},
url = {http://arxiv.org/abs/1703.09179},
year = {2017}
}
@article{wang2018mlm,
abstract = {{\textcopyright} 2018 by the authors. This paper proposes a note-based music language model (MLM) for improving note-level polyphonic piano transcription. The MLM is based on the recurrent structure, which could model the temporal correlations between notes in music sequences. To combine the outputs of the note-based MLM and acoustic model directly, an integrated architecture is adopted in this paper. We also propose an inference algorithm, in which the note-based MLM is used to predict notes at the blank onsets in the thresholding transcription results. The experimental results show that the proposed inference algorithm improves the performance of note-level transcription. We also observe that the combination of the restricted Boltzmann machine (RBM) and recurrent structure outperforms a single recurrent neural network (RNN) or long short-term memory network (LSTM) in modeling the high-dimensional note sequences. Among all the MLMs, LSTM-RBM helps the system yield the best results on all evaluation metrics regardless of the performance of acoustic models.},
author = {Wang, Qi and Zhou, Ruohua and Yan, Yonghong},
doi = {10.3390/app8030470},
file = {:home/jongwook/Dropbox/References/Polyphonic Piano Transcription with a Note-Based Music Language Model.pdf:pdf},
issn = {2076-3417},
journal = {Applied Sciences},
keywords = {network,note-based music language model,polyphonic piano transcription,recurrent neural,restricted boltzmann machine},
number = {3},
pages = {470},
title = {{Polyphonic Piano Transcription with a Note-Based Music Language Model}},
url = {http://www.mdpi.com/2076-3417/8/3/470},
volume = {8},
year = {2018}
}
@inproceedings{boccardi2001sound,
author = {Boccardi, Federico and Drioli, Carlo},
booktitle = {Proceedings of the International Conference on Digital Audio Effects {(DAFx)}},
file = {:home/jongwook/Dropbox/References/Sound Morphing With Gaussian Mixture Models.pdf:pdf},
pages = {6--9},
title = {{Sound Morphing With Gaussian Mixture Models}},
url = {http://ftp.funet.fi/index/Science/audio/dafx/2001/www.csis.ul.ie/dafx01/proceedings/papers/boccardi.pdf},
year = {2001}
}
@article{arjovsky2017wgan,
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
file = {:home/jongwook/Dropbox/References/Wasserstein GAN.pdf:pdf},
journal = {arXiv preprint arXiv:1701.07875},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Wasserstein GAN}},
year = {2017}
}
@article{ari2014exemplar,
abstract = {Non-negative matrix factorization has been shown to be powerful for modelling audio signals. Many useful applications based on NMF, including musical source separation and polyphonic transcription, have been presented in the field of music information retrieval. The multiplicative update rules for making inference on the NMF model are quite simple and practical; however, they do not scale up well with the increasing size of the dictionary matrices. In this study, we develop efficient approaches based on randomized matrix decompositions and exemplar selection that can easily handle very large dictionary matrices that can be encountered in real applications. We apply our methods on the transcription of polyphonic piano music. The results show that by only retaining 1% of a large dictionary matrix, we still get high performance in terms of objective measures.},
author = {Ar, smail and imekli, Umut and Cemgil, Ali Taylan and Akarun, Lale},
doi = {10.1080/09298215.2014.891628},
file = {:home/jongwook/Dropbox/References/Randomized Matrix Decompositions and Exemplar Selection in Large Dictionaries for Polyphonic Piano Transcription.pdf:pdf},
issn = {17445027},
journal = {Journal of New Music Research},
keywords = {machine learning,music analysis,representation,software},
number = {3},
pages = {255--265},
title = {{Randomized Matrix Decompositions and Exemplar Selection in Large Dictionaries for Polyphonic Piano Transcription}},
volume = {43},
year = {2014}
}
@article{long2015fcn,
abstract = {The recent advances in deep neural networks have convincingly demonstrated high capability in learning vision models on large datasets. Nevertheless, collecting expert labeled datasets especially with pixel-level annotations is an extremely expensive process. An appealing alternative is to render synthetic data (e.g., computer games) and generate ground truth automatically. However, simply applying the models learnt on synthetic images may lead to high generalization error on real images due to domain shift. In this paper, we facilitate this issue from the perspectives of both visual appearance-level and representation-level domain adaptation. The former adapts source-domain images to appear as if drawn from the "style" in the target domain and the latter attempts to learn domain-invariant representations. Specifically, we present Fully Convolutional Adaptation Networks (FCAN), a novel deep architecture for semantic segmentation which combines Appearance Adaptation Networks (AAN) and Representation Adaptation Networks (RAN). AAN learns a transformation from one domain to the other in the pixel space and RAN is optimized in an adversarial learning manner to maximally fool the domain discriminator with the learnt source and target representations. Extensive experiments are conducted on the transfer from GTA5 (game videos) to Cityscapes (urban street scenes) on semantic segmentation and our proposal achieves superior results when comparing to state-of-the-art unsupervised adaptation techniques. More remarkably, we obtain a new record: mIoU of 47.5% on BDDS (drive-cam videos) in an unsupervised setting.},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2018.00712},
file = {:home/jongwook/Dropbox/References/Fully Convolutional Networks for Semantic Segmentation(2).pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {6810--6818},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
year = {2018}
}
@inproceedings{oord2013music,
author = {van den Oord, A{\"{a}}ron and Dieleman, Sander and Schrauwen, Benjamin},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {10.1109/MMUL.2011.34.van},
file = {:home/jongwook/Dropbox/References/Deep Content-Based Music Recommendation.pdf:pdf},
isbn = {9780769535029},
issn = {10495258},
title = {{Deep Content-Based Music Recommendation}},
year = {2013}
}
@article{rumelhart1986backpropagation,
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams., Ronald J.},
file = {:home/jongwook/Dropbox/References/Learning Representations by Back-Propagating Errors.pdf:pdf},
journal = {Nature},
number = {6088},
pages = {533--538},
title = {{Learning Representations by Back-Propagating Errors}},
volume = {323},
year = {1986}
}
@inproceedings{magno2008recommendation,
author = {Magno, Terence and Sable, Carl},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/A Comparison of Signal-Based Music Recommendation To Genre Labels, Collaborative Filtering, Musicological Analysis, Human Recommendation.pdf:pdf},
isbn = {9780615248493},
pages = {161--166},
title = {{A Comparison of Signal-Based Music Recommendation To Genre Labels, Collaborative Filtering, Musicological Analysis, Human Recommendation, and Random Baseline}},
year = {2008}
}
@article{erhan2010pretraining,
author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
file = {:home/jongwook/Dropbox/References/Why Does Unsupervised Pre-training Help Deep Learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
number = {Feb},
pages = {625--660},
title = {{Why Does Unsupervised Pre-training Help Deep Learning?}},
volume = {11},
year = {2010}
}
@article{bello2005onset,
author = {Bello, Juan Pablo and Daudet, Laurent and Abdallah, Samer and Duxbury, Chris and Davies, Mike and Sandler, Mark B.},
doi = {10.1109/TSA.2005.851998},
file = {:home/jongwook/Dropbox/References/A Tutorial on Onset Detection in Music Signals.pdf:pdf},
isbn = {1063-6676},
issn = {10636676},
journal = {{IEEE} Transactions on Speech and Audio Processing},
keywords = {Attack transcients,Audio,MIR,Note segmentation,Novelty detection,Onset},
mendeley-tags = {MIR,Onset},
number = {5},
pages = {1035--1046},
pmid = {1000106150},
title = {{A Tutorial on Onset Detection in Music Signals}},
volume = {13},
year = {2005}
}
@article{xu2015leakyrelu,
archivePrefix = {arXiv},
arxivId = {1505.00853},
author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
eprint = {1505.00853},
file = {:home/jongwook/Dropbox/References/Empirical Evaluation of Rectified Activations in Convolutional Network.pdf:pdf},
journal = {arXiv preprint arXiv:1505.00853},
keywords = {Activations},
mendeley-tags = {Activations},
title = {{Empirical Evaluation of Rectified Activations in Convolutional Network}},
url = {http://arxiv.org/abs/1505.00853},
year = {2015}
}
@article{bello2006transcription,
abstract = {The aim of this paper is to propose solutions to some problems that arise in automatic polyphonic transcription of recorded piano music. First, we propose a method that groups spectral information in the frequency-domain and uses a rule-based framework to deal with the known problems of polyphony and harmonicity. Then, we present a novel method for multipitch-estimation that uses both frequency and time-domain information. It assumes signal segments to be the linearly weighted sum of waveforms in a database of individual piano notes. We propose a solution to the problem of generating those waveforms, by using the frequency-domain approach. We show that accurate time-domain transcription can be achieved given an adequate estimation of the database. This suggests an alternative to common frequency-domain approaches that does not require any prior training on a separate database of isolated notes},
author = {Bello, Juan P. and Daudet, Laurent and Sandler, Mark B.},
doi = {10.1109/TASL.2006.872609},
file = {:home/jongwook/Dropbox/References/Automatic Piano Transcription Using Frequency and Time-Domain Information.pdf:pdf},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Audio,F0 estimation,Multiple pitch estimation,Music},
number = {6},
pages = {2242--2251},
title = {{Automatic Piano Transcription Using Frequency and Time-Domain Information}},
volume = {14},
year = {2006}
}
@article{srivastava2014dropout,
author = {Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:home/jongwook/Dropbox/References/Dropout a Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
journal = {Journal of Machine Learning Research},
number = {1},
pages = {1929--1958},
title = {{Dropout: a Simple Way to Prevent Neural Networks from Overfitting.}},
volume = {15},
year = {2014}
}
@article{blaauw2017singing,
archivePrefix = {arXiv},
arxivId = {1704.03809},
author = {Blaauw, Merlijn and Bonada, Jordi},
doi = {10.3390/app7121313},
eprint = {1704.03809},
file = {:home/jongwook/Dropbox/References/A Neural Parametric Singing Synthesizer Modeling Timbre and Expression from Natural Songs.pdf:pdf},
issn = {2076-3417},
journal = {Applied Sciences},
keywords = {autoregressive models,conditional generative models,deep learning,machine learning,singing synthesis},
number = {12},
pages = {1313},
title = {{A Neural Parametric Singing Synthesizer Modeling Timbre and Expression from Natural Songs}},
url = {http://arxiv.org/abs/1704.03809},
volume = {7},
year = {2017}
}
@article{moorer1977transcription,
author = {Moorer, James A},
file = {:home/jongwook/Dropbox/References/On the Transcription of Musical Sound by Computer.pdf:pdf},
journal = {Computer Music Journal},
number = {4},
pages = {32--38},
title = {{On the Transcription of Musical Sound by Computer}},
volume = {1},
year = {1977}
}
@inproceedings{bertin2009nmf,
author = {Bertin, Nancy and Badeau, Roland and Vincent, Emmanuel},
booktitle = {Proceedings of the {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics {(WASPAA)}},
file = {:home/jongwook/Dropbox/References/Fast Bayesian NMF Algorithms Enforcing Harmonicity and Temporal Continuity in Polyphonic Music Transcription.pdf:pdf},
isbn = {9781424436798},
title = {{Fast Bayesian NMF Algorithms Enforcing Harmonicity and Temporal Continuity in Polyphonic Music Transcription}},
year = {2009}
}
@inproceedings{mauch2015computer,
author = {Mauch, Matthias and Cannam, Chris and Bittner, Rachel and Fazekas, George and Salamon, Justin and Dai, Jiajie and Bello, Juan and Dixon, Simon},
booktitle = {Proceedings of the First International Conference on Technologies for Music Notation and Representation},
doi = {10.1121/1.4881915},
file = {:home/jongwook/Dropbox/References/Computer-Aided Melody Note Transcription Using the Tony Software Accuracy and Efficiency.pdf:pdf},
pages = {8},
title = {{Computer-Aided Melody Note Transcription Using the Tony Software: Accuracy and Efficiency}},
year = {2015}
}
@article{lin2017pacgan,
archivePrefix = {arXiv},
arxivId = {1712.04086},
author = {Lin, Zinan and Khetan, Ashish and Fanti, Giulia and Oh, Sewoong},
eprint = {1712.04086},
file = {:home/jongwook/Dropbox/References/PacGAN The power of two samples in generative adversarial networks.pdf:pdf},
journal = {arXiv preprint arXiv:1712.04086},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{PacGAN: The power of two samples in generative adversarial networks}},
url = {http://arxiv.org/abs/1712.04086},
year = {2017}
}
@article{mescheder2018convergence,
abstract = {Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this note we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is generally not convergent. Furthermore, we discuss recent regularization strategies that were proposed to stabilize GAN training. Our analysis shows that while GAN training with instance noise or gradient penalties converges, Wasserstein-GANs and Wasserstein-GANs-GP with a finite number of discriminator updates per generator update do in general not converge to the equilibrium point. We explain these results and show that both instance noise and gradient penalties constitute solutions to the problem of purely imaginary eigenvalues of the Jacobian of the gradient vector field. Based on our analysis, we also propose a simplified gradient penalty with the same effects on local convergence as more complicated penalties.},
archivePrefix = {arXiv},
arxivId = {1801.04406},
author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
eprint = {1801.04406},
file = {:home/jongwook/Dropbox/References/Which Training Methods for GANs do Actually Converge.pdf:pdf},
journal = {arXiv preprint arXiv:1801.04406},
title = {{Which Training Methods for GANs do Actually Converge?}},
url = {http://arxiv.org/abs/1801.04406},
year = {2018}
}
@article{fan2017svsgan,
archivePrefix = {arXiv},
arxivId = {1710.11428},
author = {Fan, Zhe-Cheng and Lai, Yen-Lin and Jang, Jyh-Shing Roger},
eprint = {1710.11428},
file = {:home/jongwook/Dropbox/References/SVSGAN Singing Voice Separation via Generative Adversarial Network.pdf:pdf},
journal = {arXiv preprint arXiv:1710.11428},
keywords = {GAN,Source Separation},
mendeley-tags = {GAN,Source Separation},
title = {{SVSGAN: Singing Voice Separation via Generative Adversarial Network}},
url = {http://arxiv.org/abs/1710.11428},
year = {2017}
}
@article{kelz2018transcribe,
abstract = {Rethinking how to model polyphonic transcription formally, we frame it as a reinforcement learning task. Such a task formulation encompasses the notion of a musical agent and an environment containing an instrument as well as the sound source to be transcribed. Within this conceptual framework, the transcription process can be described as the agent interacting with the instrument in the environment, and obtaining reward by playing along with what it hears. Choosing from a discrete set of actions - the notes to play on its instrument - the amount of reward the agent experiences depends on which notes it plays and when. This process resembles how a human musician might approach the task of transcription, and the satisfaction she achieves by closely mimicking the sound source to transcribe on her instrument. Following a discussion of the theoretical framework and the benefits of modelling the problem in this way, we focus our attention on several practical considerations and address the difficulties in training an agent to acceptable performance on a set of tasks with increasing difficulty. We demonstrate promising results in partially constrained environments.},
archivePrefix = {arXiv},
arxivId = {1805.11526},
author = {Kelz, Rainer and Widmer, Gerhard},
eprint = {1805.11526},
file = {:home/jongwook/Dropbox/References/Learning to Transcribe by Ear.pdf:pdf},
journal = {arXiv preprint arXiv:1805.11526},
title = {{Learning to Transcribe by Ear}},
url = {http://arxiv.org/abs/1805.11526},
year = {2018}
}
@article{choi2017stargan,
archivePrefix = {arXiv},
arxivId = {1711.09020},
author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
eprint = {1711.09020},
file = {:home/jongwook/Dropbox/References/StarGAN Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation.pdf:pdf},
journal = {arXiv preprint arXiv:1711.09020},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation}},
url = {http://arxiv.org/abs/1711.09020},
year = {2017}
}
@inproceedings{smilkov2019tensorflowjs,
abstract = {TensorFlow.js is a library for building and executing machine learning algorithms in JavaScript. TensorFlow.js models run in a web browser and in the Node.js environment. The library is part of the TensorFlow ecosystem, providing a set of APIs that are compatible with those in Python, allowing models to be ported between the Python and JavaScript ecosystems. TensorFlow.js has empowered a new set of developers from the extensive JavaScript community to build and deploy machine learning models and enabled new classes of on-device computation. This paper describes the design, API, and implementation of TensorFlow.js, and highlights some of the impactful use cases.},
archivePrefix = {arXiv},
arxivId = {1901.05350},
author = {Smilkov, Daniel and Thorat, Nikhil and Assogba, Yannick and Yuan, Ann and Kreeger, Nick and Yu, Ping and Zhang, Kangyi and Cai, Shanqing and Nielsen, Eric and Soergel, David and Bileschi, Stan and Terry, Michael and Nicholson, Charles and Gupta, Sandeep N. and Sirajuddin, Sarah and Sculley, D. and Monga, Rajat and Corrado, Greg and Vi{\'{e}}gas, Fernanda B. and Wattenberg, Martin},
booktitle = {Proceedings of the {SysML} Conference},
eprint = {1901.05350},
file = {:home/jongwook/Dropbox/References/TensorFlow.js Machine Learning for the Web and Beyond.pdf:pdf},
title = {{TensorFlow.js: Machine Learning for the Web and Beyond}},
url = {http://arxiv.org/abs/1901.05350},
year = {2019}
}
@inproceedings{salimans2016improved,
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
file = {:home/jongwook/Dropbox/References/Improved Techniques for Training GANs.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {2226--2234},
title = {{Improved Techniques for Training GANs}},
year = {2016}
}
@article{huang2018transformer,
abstract = {Music relies heavily on self-reference to build structure and meaning. We explore the Transformer architecture (Vaswani et al., 2017) as a generative model for music, as self-attention has shown compelling results on tasks that require long-term structure such as Wikipedia summary generation (Liu et al, 2018). However, timing information is critical for polyphonic music, and Transformer does not explicitly model absolute or relative timing in its structure. To address this challenge, Shaw et al. (2018) introduced relative position representations to self-attention to improve machine translation. However, the formulation was not scalable to longer sequences. We propose an improved formulation which reduces the memory requirements of the relative position computation from $O(l^2d)$ to $O(ld)$, making it possible to train much longer sequences and achieve faster convergence. In experiments on symbolic music we find that relative self-attention substantially improves sample quality for unconditioned generation and is able to generate sequences of lengths longer than those from the training set. When primed with an initial sequence, the model generates continuations that develop the prime coherently and exhibit long-term structure. Relative self-attention can be instrumental in capturing richer relationships within a musical piece.},
archivePrefix = {arXiv},
arxivId = {1809.04281},
author = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Eck, Douglas},
eprint = {1809.04281},
file = {:home/jongwook/Dropbox/References/Music Transformer.pdf:pdf},
journal = {arXiv preprint arXiv:1809.04281},
title = {{Music Transformer}},
url = {http://arxiv.org/abs/1809.04281},
year = {2018}
}
@inproceedings{sainath2015cldnn,
author = {Sainath, Tara N. and Weiss, Ron J. and Senior, Andrew and Wilson, Kevin W. and Vinyals, Oriol},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association {(INTERSPEECH)}},
doi = {10.1.1.706.7430},
file = {:home/jongwook/Dropbox/References/Learning the Speech Front-End with Raw Waveform CLDNNs.pdf:pdf},
issn = {19909772},
keywords = {Raw,Speech},
mendeley-tags = {Raw,Speech},
title = {{Learning the Speech Front-End with Raw Waveform CLDNNs}},
year = {2015}
}
@inproceedings{shi2017pianorolls,
author = {Shi, Zhengshan and Arul, Kumaran and Smith, Julius O},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Modeling and Digitizing Reproducing Piano Rolls.pdf:pdf},
keywords = {Symbolic},
mendeley-tags = {Symbolic},
pages = {197--203},
title = {{Modeling and Digitizing Reproducing Piano Rolls}},
year = {2017}
}
@article{grindlay2011eigeninstruments,
abstract = {This paper presents a general probabilistic model for transcribing single-channel music recordings containing multiple polyphonic instrument sources. The system requires no prior knowledge of the instruments present in the mixture (other than the number), although it can benefit from information about instrument type if available. In contrast to many existing polyphonic transcription systems, our approach explicitly models the individual instruments and is thereby able to assign detected notes to their respective sources. We use training instruments to learn a set of linear manifolds in model parameter space which are then used during transcription to constrain the properties of models fit to the target mixture. This leads to a hierarchical mixture-of-subspaces design which makes it possible to supply the system with prior knowledge at different levels of abstraction. The proposed technique is evaluated on both recorded and synthesized mixtures containing two, three, four, and five instruments each. We compare our approach in terms of transcription with (i.e., detected pitches must be associated with the correct instrument) and without source-assignment to another multi-instrument transcription system as well as a baseline non-negative matrix factorization (NMF) algorithm. For two-instrument mixtures evaluated with source-assignment, we obtain average frame-level <formula formulatype="inline"> <tex Notation="TeX">${\rm F}$</tex></formula>-measures of up to 0.52 in the completely blind transcription setting (i.e., no prior knowledge of the instruments in the mixture) and up to 0.67 if we assume knowledge of the basic instrument types. For transcription without source assignment, these numbers rise to 0.76 and 0.83, respectively.},
author = {Grindlay, Graham and Ellis, Daniel P. W.},
doi = {10.1109/JSTSP.2011.2162395},
file = {:home/jongwook/Dropbox/References/Transcribing Multi-Instrument Polyphonic Music with Hierarchical Eigeninstruments.pdf:pdf},
issn = {19324553},
journal = {{IEEE} Journal on Selected Topics in Signal Processing},
keywords = {Eigeninstruments,Music,Non-negative matrix factorization (NMF),Polyphonic transcription,Subspace},
number = {6},
pages = {1159--1169},
title = {{Transcribing Multi-Instrument Polyphonic Music with Hierarchical Eigeninstruments}},
volume = {5},
year = {2011}
}
@inproceedings{saatchi2017bayesian,
abstract = {Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which are hard to model with an explicit likelihood. We present a practical Bayesian formulation for unsupervised and semi-supervised learning with GANs. Within this framework, we use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of the generator and discriminator networks. The resulting approach is straightforward and obtains good performance without any standard interventions such as feature matching, or mini-batch discrimination. By exploring an expressive posterior over the parameters of the generator, the Bayesian GAN avoids mode-collapse, produces interpretable and diverse candidate samples, and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN, Wasserstein GANs, and DCGAN ensembles.},
archivePrefix = {arXiv},
arxivId = {1705.09558},
author = {Saatchi, Yunus and Wilson, Andrew Gordon},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1705.09558},
file = {:home/jongwook/Dropbox/References/Bayesian GAN.pdf:pdf},
title = {{Bayesian GAN}},
url = {http://arxiv.org/abs/1705.09558},
year = {2017}
}
@inproceedings{mehri2016samplernn,
author = {Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:home/jongwook/Dropbox/References/SampleRNN An Unconditional End-to-End Neural Audio Generation Model.pdf:pdf},
keywords = {RNN},
mendeley-tags = {RNN},
title = {{SampleRNN: An Unconditional End-to-End Neural Audio Generation Model}},
year = {2017}
}
@inproceedings{cemgil2003generative,
author = {Cemgil, Ali Taylan and Kappen, Bert and Barber, David},
booktitle = {{IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics},
file = {:home/jongwook/Dropbox/References/Generative Model Based Polyphonic Music Transcription.pdf:pdf},
isbn = {0780378504},
title = {{Generative Model Based Polyphonic Music Transcription}},
year = {2003}
}
@inproceedings{oramas2017genre,
archivePrefix = {arXiv},
arxivId = {1707.04916},
author = {Oramas, Sergio and Nieto, Oriol and Barbieri, Francesco and Serra, Xavier},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
eprint = {1707.04916},
file = {:home/jongwook/Dropbox/References/Multi-Label Music Genre Classification from Audio, Text, and Images Using Deep Features.pdf:pdf},
keywords = {Genre Classification},
mendeley-tags = {Genre Classification},
pages = {23--30},
title = {{Multi-Label Music Genre Classification from Audio, Text, and Images Using Deep Features}},
url = {http://arxiv.org/abs/1707.04916},
year = {2017}
}
@article{mcburney2006aid,
author = {Dahlstedt, Palle and Mcburney, Peter},
doi = {10.1038/kisup.2013.30},
file = {:home/jongwook/Dropbox/References/Musical Agents Toward Computer-Aided Music Composition Using Autonomous Software Agents.pdf:pdf},
issn = {2157-1724},
journal = {Leonardo},
number = {5},
pages = {469--470},
pmid = {25018997},
title = {{Musical Agents: Toward Computer-Aided Music Composition Using Autonomous Software Agents}},
volume = {39},
year = {2006}
}
@article{ghias1995humming,
author = {Ghias, Asif and Logan, Jonathan and Chamberlin, David and Smith, Brian C.},
doi = {10.1145/217279.215273},
file = {:home/jongwook/Dropbox/References/Query By Humming Musical Information Retrieval in An Audio Database.pdf:pdf},
isbn = {0897917510},
journal = {Proceedings of the {ACM} International Conference on Multimedia},
pages = {110--113},
title = {{Query By Humming: Musical Information Retrieval in An Audio Database}},
volume = {6},
year = {1995}
}
@article{koushik2016eve,
author = {Koushik, Jayanth and Hayashi, Hiroaki},
file = {:home/jongwook/Dropbox/References/Improving Stochastic Gradient Descent with Feedback.pdf:pdf},
journal = {arXiv preprint arXiv:1611.01505},
keywords = {Optimizer},
mendeley-tags = {Optimizer},
title = {{Improving Stochastic Gradient Descent with Feedback}},
year = {2016}
}
@inproceedings{ullrich2017seq2seq,
abstract = {Automatic Music Transcription (AMT) is a fundamen-tal problem in Music Information Retrieval (MIR). The challenge is to translate an audio sequence to a symbolic representation of music. Recently, convolutional neural networks (CNNs) have been successfully applied to the task by translating frames of audio [44, 46]. However, those models can by their nature not model temporal re-lations and long time dependencies. Furthermore, it is extremely labor intense to get annotations for supervised learning in this setting. We propose a model that over-comes all these problems. The convolutional sequence to sequence (Cseq2seq) model applies a CNN to learn a low dimensional representation of audio frames and a sequen-tial model to translate these learned features to a symbolic representation directly. Our approach has three advantages over other methods: (i) extracting audio frame representa-tions and learning the sequential model is jointly trained end-to-end, (ii) the recurrent model can capture temporal features in musical pieces in order to improve transcrip-tion, and (iii) our model learns from entire sequences as opposed to temporally accurately annotated onsets and off-sets for each note thus making it possible to train on large already existing corpora of music. For the purpose of test-ing our method we created our own dataset of 17K mono-phonic songs and respective MusicXML files. Initial ex-periments proof the validity of our approach.},
author = {Ullrich, Karen and {Van Der Wel}, Eelco},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Music Transcription With Convolutional Sequence-To-Sequence Models.pdf:pdf},
title = {{Music Transcription With Convolutional Sequence-To-Sequence Models}},
url = {http://karenullrich.info/pdfs/2017_ismir_2.pdf},
year = {2017}
}
@article{rafii2013separation,
author = {Rafii, Zafar and Pardo, Bryan},
doi = {10.1109/TASL.2012.2213249},
file = {:home/jongwook/Dropbox/References/{RE}peating pattern extraction technique {(REPET)} A simple method for musicvoice separation.pdf:pdf},
isbn = {1558-7916},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
number = {1},
pages = {71--82},
title = {{{RE}peating pattern extraction technique {(REPET)}: A simple method for music/voice separation}},
volume = {21},
year = {2013}
}
@article{linnainmaa1970ad,
author = {Linnainmaa, Seppo},
journal = {Master's Thesis (in Finnish), University of Helsinki},
pages = {6--7},
title = {{The Representation of the Cumulative Rounding Error of an Algorithm as a Taylor Expansion of the Local Rounding Errors}},
year = {1970}
}
@inproceedings{szegedy2015googlenet,
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}},
file = {:home/jongwook/Dropbox/References/Going Deeper with Convolutions.pdf:pdf},
pages = {1--9},
title = {{Going Deeper with Convolutions}},
year = {2015}
}
@inproceedings{zhao2018arae,
archivePrefix = {arXiv},
arxivId = {1706.04223},
author = {Zhao, Junbo and Kim, Yoon and Zhang, Kelly and Rush, Alexander M. and LeCun, Yann and Junbo and Zhao and Kim, Yoon and Zhang, Kelly and Rush, Alexander M. and LeCun, Yann},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1706.04223},
file = {:home/jongwook/Dropbox/References/Adversarially Regularized Autoencoders.pdf:pdf},
pages = {1--18},
title = {{Adversarially Regularized Autoencoders}},
url = {http://arxiv.org/abs/1706.04223},
year = {2017}
}
@article{hinton2005cd,
author = {Carreira-Perpi{\~{n}}{\'{a}}n, Miguel A and Hinton, Geoffrey E.},
doi = {10.3389/conf.neuro.10.2009.14.121},
file = {:home/jongwook/Dropbox/References/On Contrastive Divergence Learning.pdf:pdf},
isbn = {0818681322},
issn = {16625188},
journal = {Artificial Intelligence and Statistics},
pages = {17},
title = {{On Contrastive Divergence Learning}},
year = {2005}
}
@inproceedings{mcfee2015librosa,
abstract = {This document describes version 0.4.0 of librosa: a Python package for audio and music signal processing. At a high level, librosa provides implementations of a variety of common functions used throughout the field of music information retrieval. In this document, a brief overview of the library's functionality is provided, along with explanations of the design goals, software development practices, and notational conventions.},
author = {Mcfee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel P. W. and Mcvicar, Matt and Battenberg, Eric and Nieto, Oriol},
booktitle = {Proceedings of the Python in Science Conference},
file = {:home/jongwook/Dropbox/References/librosa Audio and Music Signal Analysis in Python.pdf:pdf},
keywords = {Index Terms-audio,music,signal processing},
pages = {18--25},
title = {{librosa: Audio and Music Signal Analysis in Python}},
url = {http://conference.scipy.org/proceedings/scipy2015/pdfs/brian_mcfee.pdf},
year = {2015}
}
@article{tzanetakis2002genre,
archivePrefix = {arXiv},
arxivId = {62},
author = {Tzanetakis, George and Cook, Perry},
doi = {10.1109/TSA.2002.800560},
eprint = {62},
file = {:home/jongwook/Dropbox/References/Musical Genre Classification of Audio Signals.pdf:pdf},
isbn = {1063-6676},
issn = {10636676},
journal = {{IEEE} Transactions on Speech and Audio Processing},
keywords = {Audio classification,Beat analysis,Feature extraction,Musical genre classification,Wavelets},
number = {5},
pages = {293--302},
pmid = {7359731},
title = {{Musical Genre Classification of Audio Signals}},
volume = {10},
year = {2002}
}
@inproceedings{hansen2016codebook,
author = {Hansen, Martin Weiss and Jensen, Jesper Rindom and Christensen, Mads Graesboll},
booktitle = {Proceedings of the European Signal Processing Conference {(EUSIPCO)}},
file = {:home/jongwook/Dropbox/References/Estimation of Multiple Pitches in Stereophonic Mixtures Using a Codebook-Based Approach.pdf:pdf},
isbn = {9780992862657},
keywords = {Multi-F0},
mendeley-tags = {Multi-F0},
pages = {983--987},
title = {{Estimation of Multiple Pitches in Stereophonic Mixtures Using a Codebook-Based Approach}},
year = {2016}
}
@book{hartmann1997signals,
author = {Hartmann, William M},
file = {:home/jongwook/Dropbox/References/Signals, Sound, and Sensation.pdf:pdf},
isbn = {1563962837},
publisher = {Springer},
title = {{Signals, Sound, and Sensation}},
year = {1997}
}
@article{benetos2013amt,
abstract = {Automatic music transcription is considered by many to be a key enabling technology in music signal processing. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, although the field is still very active. In this paper we analyse limitations of current methods and identify promising directions for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. One way to overcome the limited performance of transcription systems is to tailor algorithms to specific use-cases. Semi-automatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich potential source of training data, via forced alignment of audio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information from multiple algorithms and different musical aspects.},
author = {Benetos, Emmanouil and Dixon, Simon and Giannoulis, Dimitrios and Kirchhoff, Holger and Klapuri, Anssi P.},
doi = {10.1007/s10844-013-0258-3},
file = {:home/jongwook/Dropbox/References/Automatic music transcription Challenges and future directions.pdf:pdf;:home/jongwook/Dropbox/References/Automatic Music Transcription Challenges and Future Directions.pdf:pdf},
issn = {09259902},
journal = {Journal of Intelligent Information Systems},
keywords = {Automatic music transcription,Music information retrieval,Music signal analysis},
number = {3},
pages = {407--434},
title = {{Automatic Music Transcription: Challenges and Future Directions}},
volume = {41},
year = {2013}
}
@article{peeling2011poisson,
abstract = {Novel statistical models are proposed and developed in this paper for automated multiple-pitch estimation problems. Point estimates of the parameters of partial frequencies of a musical note are modeled as realizations from a non-homogeneous Poisson process defined on the frequency axis. When several notes are combined, the processes for the individual notes combine to give a new Poisson process whose likelihood is easy to compute. This model avoids the data-association step of linking the harmonics of each note with the corresponding partials and is ideal for efficient Bayesian inference of unknown multiple fundamental frequencies in a signal.},
author = {Peeling, Paul H. and Godsill, Simon J.},
doi = {10.1109/JSTSP.2011.2158804},
file = {:home/jongwook/Dropbox/References/Multiple Pitch Estimation Using Non-Homogeneous Poisson Process.pdf:pdf},
issn = {19324553},
journal = {{IEEE} Journal on Selected Topics in Signal Processing},
keywords = {Bayesian methods,Frequency estimation,Matching pursuit algorithms,Music information retrieval,Spectral analysis},
number = {6},
pages = {1133--1143},
title = {{Multiple Pitch Estimation Using Non-Homogeneous Poisson Process}},
volume = {5},
year = {2011}
}
@inproceedings{sigtia2015hybrid,
abstract = {We investigate the problem of incorporating higher-level symbolic score-like information into Automatic Music Transcription (AMT) systems to improve their performance. We use recurrent neural networks (RNNs) and their variants as music language models (MLMs) and present a generative architecture for combining these models with predictions from a frame level acoustic classifier. We also compare different neural network architectures for acoustic modeling. The proposed model computes a distribution over possible output sequences given the acoustic input signal and we present an algorithm for performing a global search for good candidate transcriptions. The performance of the proposed model is evaluated on piano music from the MAPS dataset and we observe that the proposed model consistently outperforms existing transcription methods.},
archivePrefix = {arXiv},
arxivId = {1411.1623},
author = {Sigtia, Siddharth and Benetos, Emmanouil and Boulanger-Lewandowski, Nicolas and Weyde, Tillman and d'Avila Garcez, Artur S. and Dixon, Simon},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
eprint = {1411.1623},
file = {:home/jongwook/Dropbox/References/A Hybrid Recurrent Neural Network For Music Transcription.pdf:pdf},
title = {{A Hybrid Recurrent Neural Network For Music Transcription}},
url = {http://arxiv.org/abs/1411.1623},
year = {2015}
}
@article{ross1974amdf,
author = {Ross, Myron J. and Shaffer, Harry L. and Cohen, Andrew and Freudberg, Richard and Manley, Harold J.},
doi = {10.1109/TASSP.1974.1162598},
file = {:home/jongwook/Dropbox/References/Average Magnitude Difference Function Pitch Extractor.pdf:pdf},
issn = {00963518},
journal = {{IEEE} Transactions on Acoustics, Speech, and Signal Processing},
number = {5},
pages = {353--362},
title = {{Average Magnitude Difference Function Pitch Extractor}},
volume = {ASSP-22},
year = {1974}
}
@article{bogdanov2013recommendation,
author = {Bogdanov, Dmitry and Haro, Mart{\'{i}}n and Fuhrmann, Ferdinand and Xamb{\'{o}}, Anna and G{\'{o}}mez, Emilia and Herrera, Perfecto},
doi = {10.1016/j.ipm.2012.06.004},
file = {:home/jongwook/Dropbox/References/Semantic audio content-based music recommendation and visualization based on user preference examples.pdf:pdf},
isbn = {03064573},
issn = {03064573},
journal = {Information Processing and Management},
number = {1},
pages = {13--33},
pmid = {83573603},
publisher = {Elsevier Ltd},
title = {{Semantic audio content-based music recommendation and visualization based on user preference examples}},
volume = {49},
year = {2013}
}
@phdthesis{miron2018thesis,
author = {Miron, Marius},
file = {:home/jongwook/Dropbox/References/Source Separation Methods for Orchestral Music Timbre-Informed and Score-Informed Strategies.pdf:pdf},
school = {Pompeu Fabra University, Barcelona},
title = {{Source Separation Methods for Orchestral Music: Timbre-Informed and Score-Informed Strategies}},
year = {2018}
}
@inproceedings{bosch2014melody,
author = {Bosch, Juan J. and G{\'{o}}mez, Emilia},
booktitle = {Proceedings of the Conference on Interdisciplinary Musicology},
file = {:home/jongwook/Dropbox/References/Melody Extraction in Symphonic Classical Music a Comparative Study of Mutual Agreement Between Humans and Algorithms.pdf:pdf},
title = {{Melody Extraction in Symphonic Classical Music: a Comparative Study of Mutual Agreement Between Humans and Algorithms}},
year = {2014}
}
@inproceedings{kondor2018compact,
author = {Kondor, Risi and Trivedi, Shubhendu},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
file = {:home/jongwook/Dropbox/References/On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups.pdf:pdf},
title = {{On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups}},
year = {2018}
}
@inproceedings{zhao2017ebgan,
archivePrefix = {arXiv},
arxivId = {arXiv:1612.02136v5},
author = {Zhao, Junbo and Mathieu, Michael and LeCun, Yann},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {arXiv:1612.02136v5},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Energy-based Generative Adversarial Network}},
year = {2017}
}
@article{simonyan2014vgg,
author = {Simonyan, Karen and Zisserman, Andrew},
file = {:home/jongwook/Dropbox/References/Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
journal = {arXiv preprint arXiv:1409.1556},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
year = {2014}
}
@inproceedings{nair2010relu,
author = {Nair, Vinod and Hinton, Geoffrey E},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
file = {:home/jongwook/Dropbox/References/Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:pdf},
keywords = {Activations},
mendeley-tags = {Activations},
pages = {807--814},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{cakir2017crnn,
archivePrefix = {arXiv},
arxivId = {1702.06286},
author = {Cakir, Emre and Parascandolo, Giambattista and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas},
doi = {10.1109/TASLP.2017.2690575},
eprint = {1702.06286},
isbn = {0849371813},
issn = {23299290},
journal = {{IEEE/ACM} Transactions on Audio, Speech, and Language Processing},
number = {6},
pages = {1291--1303},
pmid = {456984},
title = {{Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection}},
volume = {25},
year = {2017}
}
@article{saito2008specmurt,
abstract = {This paper introduces a new music signal processing method to extract multiple fundamental frequencies, which we call specmurt analysis. In contrast with cepstrum which is the inverse Fourier transform of log-scaled power spectrum with linear frequency, specmurt is defined as the inverse Fourier transform of linear power spectrum with log-scaled frequency. Assuming that all tones in a polyphonic sound have a common harmonic pattern, the sound spectrum can be regarded as a sum of linearly stretched common harmonic structures along frequency. In the log-frequency domain, it is formulated as the convolution of a common harmonic structure and the distribution density of the fundamental frequencies of multiple tones. The fundamental frequency distribution can be found by deconvolving the observed spectrum with the assumed common harmonic structure, where the common harmonic structure is given heuristically or quasi-optimized with an iterative algorithm. The efficiency of specmurt analysis is experimentally demonstrated through generation of a piano-roll-like display from a polyphonic music signal and automatic sound-to-MIDI conversion. Multipitch estimation accuracy is evaluated over several polyphonic music signals and compared with manually annotated MIDI data.},
author = {Saito, Shoichiro and Kameoka, Hirokazu and Takahashi, Keigo and Nishimoto, Takuya and Sagayama, Shigeki},
doi = {10.1109/TASL.2007.912998},
file = {:home/jongwook/Dropbox/References/Specmurt Analysis of Polyphonic Music Signals.pdf:pdf},
isbn = {1558-7916},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Inverse filtering,Iteration algorithm,Multipitch analysis,Pitch visualization,Polyphonic music signals},
number = {3},
pages = {639--650},
title = {{Specmurt Analysis of Polyphonic Music Signals}},
volume = {16},
year = {2008}
}
@article{esling2018generative,
abstract = {Timbre spaces have been used in music perception to study the relationships between instruments based on dissimilarity ratings. However, these spaces do not generalize, need to be reconstructed for each novel example and are not continuous, preventing audio synthesis. In parallel, generative models have aimed to provide methods for synthesizing novel timbres. However, these systems do not provide an explicit control structure, nor do they provide an understanding of their inner workings and are not related to any perceptually relevant information. Here, we show that Variational Auto-Encoders (VAE) can alleviate these limitations by constructing generative timbre spaces. To do so, we adapt VAEs to create a generative latent space, while using perceptual ratings from timbre studies to regularize the organization of this space. The resulting space allows to analyze novel instruments, while being able to synthesize audio from any point of this space. We introduce a specific regularization allowing to directly enforce given similarity ratings onto these spaces. We compare the resulting space to existing timbre spaces and show that they provide almost similar distance relationships. We evaluate several spectral transforms and show that the Non-Stationary Gabor Transform (NSGT) provides the highest correlation to timbre spaces and the best quality of synthesis. We show that these spaces can generalize to novel instruments and can generate any path between instruments to understand their timbre relationships. As these spaces are continuous, we study how the traditional acoustic descriptors behave along the latent dimensions. We show that descriptors have an overall non-linear topology, but follow a locally smooth evolution. Based on this, we introduce a method for descriptor-based synthesis and show that we can control the descriptors of an instrument while keeping its timbre structure.},
archivePrefix = {arXiv},
arxivId = {1805.08501},
author = {Esling, Philippe and Chemla--Romeu-Santos, Axel and Bitton, Adrien},
eprint = {1805.08501},
file = {:home/jongwook/Dropbox/References/Generative Timbre Spaces with Variational Audio Synthesis.pdf:pdf},
pages = {1--8},
title = {{Generative Timbre Spaces with Variational Audio Synthesis}},
url = {http://arxiv.org/abs/1805.08501},
year = {2018}
}
@article{casey2008mir,
author = {Casey, Michael A. and Veltkamp, Remco and Goto, Masataka and Leman, Marc and Rhodes, Christophe and Slaney, Malcolm},
doi = {10.1109/JPROC.2008.916370},
file = {:home/jongwook/Dropbox/References/Content-Based Music Information Retrieval Current Directions and Future Challenges.pdf:pdf},
isbn = {0018-9219},
issn = {0018-9219},
journal = {Proceedings of the {IEEE}},
keywords = {MIR},
mendeley-tags = {MIR},
number = {4},
pages = {668--696},
title = {{Content-Based Music Information Retrieval: Current Directions and Future Challenges}},
volume = {96},
year = {2008}
}
@phdthesis{cemgil2004transcription,
author = {Cemgil, Ali Taylan},
doi = {10.1121/1.2934702},
file = {:home/jongwook/Dropbox/References/Bayesian Music Transcription.pdf:pdf},
isbn = {9090184546},
issn = {00014966},
school = {Radboud Universiteit Nijmegen},
title = {{Bayesian Music Transcription}},
year = {2004}
}
@article{chemilier2001grammar,
author = {Chemilier, Marc},
file = {:home/jongwook/Dropbox/References/Improvising Jazz Chord Sequences by Means of Formal Grammars.pdf:pdf},
journal = {Journee d'Informatique Musicale},
pages = {121--126},
title = {{Improvising Jazz Chord Sequences by Means of Formal Grammars}},
year = {2001}
}
@inproceedings{ezzat2005morphing,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
author = {Ezzat, Tony and Meyers, Ethan and Glass, James R. and Poggio, Tomaso},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association {(INTERSPEECH)}},
file = {:home/jongwook/Dropbox/References/Morphing Spectral Envelopes Using Audio Flow(3).pdf:pdf},
pages = {2545--2548},
title = {{Morphing Spectral Envelopes Using Audio Flow}},
year = {2005}
}
@article{benetos2012latent,
author = {Benetos, Emmanouil and Dixon, Simon},
doi = {10.1162/COMJ},
file = {:home/jongwook/Dropbox/References/A Shift-Invariant Latent Variable Model for Automatic Music Transcription.pdf:pdf},
isbn = {8187672641},
issn = {1531-5169},
journal = {Computer Music Journal},
number = {4},
pages = {81--94},
title = {{A Shift-Invariant Latent Variable Model for Automatic Music Transcription}},
volume = {36},
year = {2012}
}
@inproceedings{li2017alice,
archivePrefix = {arXiv},
arxivId = {1709.01215},
author = {Li, Chunyuan and Liu, Hao and Chen, Changyou and Pu, Yunchen and Chen, Liqun and Henao, Ricardo and Carin, Lawrence},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1709.01215},
file = {:home/jongwook/Dropbox/References/ALICE Towards Understanding Adversarial Learning for Joint Distribution Matching.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {1--9},
title = {{ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching}},
url = {http://arxiv.org/abs/1709.01215},
year = {2017}
}
@article{mnih2013atari,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
file = {:home/jongwook/Dropbox/References/Playing Atari with Deep Reinforcement Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1312.5602},
keywords = {DQN,RL},
mendeley-tags = {DQN,RL},
title = {{Playing Atari with Deep Reinforcement Learning}},
year = {2013}
}
@inproceedings{bello2005chromagram,
author = {Bello, Juan P and Pickens, Jeremy},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/A Robust Mid-Level Representation for Harmonic Content in Music Signals.pdf:pdf},
isbn = {0955117909},
keywords = {Harmonic description,music similarity,segmentation},
pages = {304--311},
title = {{A Robust Mid-Level Representation for Harmonic Content in Music Signals}},
year = {2005}
}
@inproceedings{wu2019musicnet,
author = {Wu, Yu-Te and Chen, Berlin and Su, Li},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:home/jongwook/Dropbox/References/Polyphonic Music Transcription with Semantic Segmentation.pdf:pdf},
isbn = {9781538646588},
pages = {166--170},
title = {{Polyphonic Music Transcription with Semantic Segmentation}},
year = {2019}
}
@inproceedings{cogliati2015sparse,
author = {Cogliati, Andrea and Duan, Zhiyao and Wohlberg, Brendt},
booktitle = {Proceedings of the {IEEE} International Workshop on Machine Learning for Signal Processing},
file = {:home/jongwook/Dropbox/References/Piano Music Transcription with Fast Convolutional Sparse Coding.pdf:pdf},
isbn = {9781467374545},
title = {{Piano Music Transcription with Fast Convolutional Sparse Coding}},
year = {2015}
}
@inproceedings{benetos2010mirex,
author = {Benetos, Emmanouil and Dixon, Simon},
booktitle = {Music Information Retrieval Evaluation eX-change {(MIREX)}},
file = {:home/jongwook/Dropbox/References/Multiple-F0 Estimation and Note Tracking Using a Convolutive Probabilistic Model.pdf:pdf},
title = {{Multiple-F0 Estimation and Note Tracking Using a Convolutive Probabilistic Model}},
url = {http://www.music-ir.org/mirex/abstracts/2011/BD1.pdf},
year = {2010}
}
@article{su2009cf,
archivePrefix = {arXiv},
arxivId = {421425},
author = {Su, Xiaoyuan and Khoshgoftaar, Taghi M.},
doi = {10.1155/2009/421425},
eprint = {421425},
file = {:home/jongwook/Dropbox/References/A Survey of Collaborative Filtering Techniques.pdf:pdf},
isbn = {1687-7470},
issn = {1687-7470},
journal = {Advances in Artificial Intelligence},
keywords = {Collaborative Filtering,Recommender Systems},
mendeley-tags = {Collaborative Filtering,Recommender Systems},
pages = {1--19},
pmid = {11867525},
title = {{A Survey of Collaborative Filtering Techniques}},
volume = {2009},
year = {2009}
}
@inproceedings{osaka1995timbre,
author = {Osaka, Naotoshi},
booktitle = {Proceedings of the International Computer Music Conference {(ICMC)}},
file = {:home/jongwook/Dropbox/References/Timbre Interpolation of Sounds using a Sinusoidal Model.pdf:pdf},
keywords = {mss morphing and interpolation},
pages = {408--411},
title = {{Timbre Interpolation of Sounds using a Sinusoidal Model}},
year = {1995}
}
@article{mnih2015dqn,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg},
file = {:home/jongwook/Dropbox/References/Human-Level Control Through Deep Reinforcement Learning.pdf:pdf},
journal = {Nature},
keywords = {DQN,RL},
mendeley-tags = {DQN,RL},
number = {7540},
pages = {529--533},
publisher = {Nature Research},
title = {{Human-Level Control Through Deep Reinforcement Learning}},
volume = {518},
year = {2015}
}
@inproceedings{bittner2015contour,
author = {Bittner, Rachel M and Salamon, Justin and Essid, Slim and Bello, Juan P},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Melody Extraction by Contour Classification.pdf:pdf},
isbn = {978-84-606-8853-2},
pages = {500--506},
title = {{Melody Extraction by Contour Classification}},
year = {2015}
}
@article{koren2008cf,
archivePrefix = {arXiv},
arxivId = {62},
author = {Koren, Yehuda},
doi = {10.1145/1401890.1401944},
eprint = {62},
file = {:home/jongwook/Dropbox/References/Factorization Meets the Neighborhood a Multifaceted Collaborative Filtering Model.pdf:pdf},
isbn = {1605581933},
issn = {1605581933},
journal = {Proceedings of the {ACM SIGKDD} International Conference on Knowledge Discovery and Data Mining},
keywords = {collaborative filtering,recommender systems},
pages = {426--434},
pmid = {2428061},
title = {{Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model}},
url = {ACM},
year = {2008}
}
@article{agostini2003timbres,
author = {Agostini, G. and Longari, M. and Pollastri, E.},
file = {:home/jongwook/Dropbox/References/Musical Instrument Timbres Classification With Spectral Features.pdf:pdf},
isbn = {0780370252},
journal = {{EURASIP} Journal on Advances in Signal Processing},
pages = {97--102},
title = {{Musical Instrument Timbres Classification With Spectral Features}},
year = {2003}
}
@book{bregman1990asa,
author = {Bregman, Albert S.},
publisher = {MIT Press},
title = {{Auditory Scene Analysis}},
year = {1990}
}
@inproceedings{shen2018tacotron,
archivePrefix = {arXiv},
arxivId = {1712.05884},
author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerry-Ryan, RJ and Saurous, Rif A. and Agiomyrgiannakis, Yannis and Wu, Yonghui},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
eprint = {1712.05884},
file = {:home/jongwook/Dropbox/References/Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.pdf:pdf},
pages = {2--6},
title = {{Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions}},
url = {http://arxiv.org/abs/1712.05884},
year = {2018}
}
@article{ding2008equiv,
abstract = {Non-negative Matrix Factorization (NMF) and Probabilistic Latent Semantic Indexing (PLSI) have been successfully applied to document clustering recently. In this paper, we show that PLSI and NMF (with the I-divergence objective function) optimize the same objective function, although PLSI and NMF are different algorithms as verified by experiments. This provides a theoretical basis for a new hybrid method that runs PLSI and NMF alternatively, each jumping out of the local minima of the other method successively, thus achieving a better final solution. Extensive experiments on five real-life datasets show relations between NMF and PLSI, and indicate that the hybrid method leads to significant improvements over NMF-only or PLSI-only methods. We also show that at first-order approximation, NMF is identical to the ??2-statistic. ?? 2008.},
author = {Ding, Chris and Li, Tao and Peng, Wei},
doi = {10.1016/j.csda.2008.01.011},
file = {:home/jongwook/Dropbox/References/On the Equivalence Between Non-Negative Matrix Factorization and Probabilistic Latent Semantic Indexing.pdf:pdf},
isbn = {01679473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
number = {8},
pages = {3913--3927},
title = {{On the Equivalence Between Non-Negative Matrix Factorization and Probabilistic Latent Semantic Indexing}},
volume = {52},
year = {2008}
}
@article{sakaue2013overtone,
abstract = {We present a Bayesian analysis method that estimates the harmonic structure of musical instruments in music signals on the basis of psychoacoustic evidence. Since the main objective of multipitch analysis is joint estimation of the fundamental frequencies and their harmonic structures, the performance of harmonic structure estimation significantly affects fundamental frequency estimation accuracy. Many methods have been proposed for estimating the harmonic structure accurately, but no method has been proposed that satisfies all these requirements: robust against initialization, optimization-free, and psychoacoustically appropriate and thus easy to develop further. Our method satisfies these requirements by explicitly incorporating Terhardt's virtual pitch theory within a Bayesian framework. It does this by automatically learning the valid weight range of the harmonic components using a MIDI synthesizer. The bounds are termed "overtone corpus." Modeling demonstrated that the proposed overtone corpus method can stably estimate the harmonic structure of 40 musical pieces for a wide variety of initial settings. {\textcopyright} 2013 Information Processing Society of Japan.},
author = {Sakaue, D. and Itoyama, K. and Ogata, T. and Okuno, H.G.},
doi = {10.2197/ipsjjip.21.246},
file = {:home/jongwook/Dropbox/References/Robust Multipitch Analyzer against Initialization based on Latent Harmonic Allocation using Overtone Corpus.pdf:pdf},
issn = {03875806},
journal = {Journal of Information Processing},
keywords = {harmonic clustering,multipitch estimation,musical instrument sounds,overtone estimation},
number = {2},
pages = {246--255},
title = {{Robust Multipitch Analyzer against Initialization based on Latent Harmonic Allocation using Overtone Corpus}},
volume = {21},
year = {2013}
}
@inproceedings{ewert2015struck,
abstract = {Given a musical audio recording, the goal of music transcription is to determine a score-like representation of the piece underlying the recording. Most current transcription methods employ variants of non-negative matrix factorization (NMF), which often fails to robustly model instruments producing non-stationary sounds. Using entire time-frequency patterns to represent sounds, non-negative matrix deconvolution (NMD) can capture certain types of non-stationary behavior but is only applicable if all sounds have the same length. In this paper, we present a novel method that combines the non-stationarity modeling capabilities available with NMD with the variable note lengths possible with NMF. Identifying frames in NMD patterns with states in a dynamical system, our method iteratively generates sound-object candidates separately for each pitch, which are then combined in a global optimization. We demonstrate the transcription capabilities of our method using piano pieces assuming the availability of single note recordings as training data.},
author = {Ewert, Sebastian and Plumbley, Mark D. and Sandler, Mark},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
doi = {10.1109/ICASSP.2015.7178033},
file = {:home/jongwook/Dropbox/References/A Dynamic Programming Variant of Non-Negative Matrix Deconvolution for the Transcription of Struck String Instruments.pdf:pdf},
isbn = {9781467369978},
issn = {15206149},
keywords = {Convolutive Signal Models,Dynamical Systems,Music Transcription,Non-Negative Matrix Deconvolution},
pages = {569--573},
title = {{A Dynamic Programming Variant of Non-Negative Matrix Deconvolution for the Transcription of Struck String Instruments}},
year = {2015}
}
@incollection{schedl2015music,
author = {Schedl, Markus and Knees, Peter and Mcfee, Brian and Bogdanov, Dmitry and Kaminskas, Marius},
booktitle = {Recommender Systems Handbook},
chapter = {13},
doi = {10.1007/978-1-4899-7637-6},
file = {:home/jongwook/Dropbox/References/Music Recommender Systems.pdf:pdf},
isbn = {978-1-4899-7636-9},
title = {{Music Recommender Systems}},
year = {2015}
}
@inproceedings{ping2018deepvoice3,
archivePrefix = {arXiv},
arxivId = {1710.07654},
author = {Ping, Wei and Peng, Kainan and Gibiansky, Andrew and Arik, Sercan O. and Kannan, Ajay and Narang, Sharan and Raiman, Jonathan and Miller, John},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1710.07654},
file = {:home/jongwook/Dropbox/References/Deep Voice 3 2000-Speaker Neural Text-to-Speech.pdf:pdf},
title = {{Deep Voice 3: 2000-Speaker Neural Text-to-Speech}},
url = {http://arxiv.org/abs/1710.07654},
year = {2018}
}
@inproceedings{shi2016kalman,
archivePrefix = {arXiv},
arxivId = {arXiv:1608.01392v1},
author = {Shi, Liming and Nielsen, Jesper K. and Jensen, Jesper R. and Little, Max A. and Christensen, Mads G.},
booktitle = {Proceedings of the {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics {(WASPAA)}},
eprint = {arXiv:1608.01392v1},
file = {:home/jongwook/Dropbox/References/A Kalman-Based Fundamental Frequency Estimation Algorithm.pdf:pdf},
isbn = {5889137697},
keywords = {F0,Kalman,high temperature corrosion,sem,superalloys,xrd},
mendeley-tags = {F0,Kalman},
title = {{A Kalman-Based Fundamental Frequency Estimation Algorithm}},
url = {https://arxiv.org/pdf/1608.01392.pdf},
year = {2017}
}
@inproceedings{ohanlon2014nmf,
author = {O'Hanlon, Ken and Plumbley, Mark D.},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:home/jongwook/Dropbox/References/Polyphonic Piano Transcription using Non-Negative Matrix Factorisation with Group Sparsity.pdf:pdf},
number = {May},
pages = {3136--3140},
title = {{Polyphonic Piano Transcription using Non-Negative Matrix Factorisation with Group Sparsity}},
volume = {1},
year = {2014}
}
@article{degani2015tuning,
author = {Degani, Alessio and Dalai, Marco and Leonardi, Riccardo and Migliorati, Pierangelo},
doi = {10.1007/s11042-014-1897-2},
file = {:home/jongwook/Dropbox/References/Comparison of Tuning Frequency Estimation Methods.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
number = {15},
pages = {5917--5934},
title = {{Comparison of Tuning Frequency Estimation Methods}},
volume = {74},
year = {2015}
}
@article{emiya2010smoothness,
abstract = {A new method for the estimation of multiple concurrent pitches in piano recordings is presented. It addresses the issue of overlapping overtones by modeling the spectral envelope of the overtones of each note with a smooth autoregressive model. For the background noise, a moving-average model is used and the combination of both tends to eliminate harmonic and sub-harmonic erroneous pitch estimations. This leads to a complete generative spectral model for simultaneous piano notes, which also explicitly includes the typical deviation from exact harmonicity in a piano overtone series. The pitch set which maximizes an approximate likelihood is selected from among a restricted number of possible pitch combinations as the one. Tests have been conducted on a large homemade database called MAPS, composed of piano recordings from a real upright piano and from high-quality samples.},
author = {Emiya, Valentin and Badeau, Roland and David, Bertrand},
doi = {10.1109/TASL.2009.2038819},
file = {:home/jongwook/Dropbox/References/Multipitch Estimation of Piano Sounds Using a New Probabilistic Spectral Smoothness Principle.pdf:pdf},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Acoustic signal analysis,audio processing,multipitch estimation (MPE),piano,spectral smoothness,transcription},
number = {6},
pages = {1643--1654},
publisher = {IEEE},
title = {{Multipitch Estimation of Piano Sounds Using a New Probabilistic Spectral Smoothness Principle}},
volume = {18},
year = {2010}
}
@inproceedings{sukhbaatar2015memory,
archivePrefix = {arXiv},
arxivId = {1503.08895},
author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {v5},
eprint = {1503.08895},
file = {:home/jongwook/Dropbox/References/End-To-End Memory Networks.pdf:pdf},
isbn = {1551-6709},
issn = {10495258},
pmid = {9377276},
title = {{End-To-End Memory Networks}},
url = {http://arxiv.org/abs/1503.08895},
year = {2015}
}
@article{steeg2017synergy,
archivePrefix = {arXiv},
arxivId = {1710.03839},
author = {Steeg, Greg Ver and Brekelmans, Rob and Harutyunyan, Hrayr and Galstyan, Aram},
eprint = {1710.03839},
file = {:home/jongwook/Dropbox/References/Disentangled Representations via Synergy Minimization.pdf:pdf},
journal = {arXiv preprint arXiv:1710.03839},
title = {{Disentangled Representations via Synergy Minimization}},
year = {2017}
}
@article{makhzani2015aae,
archivePrefix = {arXiv},
arxivId = {1511.05644},
author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
eprint = {1511.05644},
file = {:home/jongwook/Dropbox/References/Adversarial Autoencoders.pdf:pdf},
isbn = {9781509008063},
journal = {arXiv preprint arXiv:1511.05644},
title = {{Adversarial Autoencoders}},
url = {http://arxiv.org/abs/1511.05644},
year = {2015}
}
@article{decheveigne2002yin,
author = {de Cheveign{\'{e}}, Alain and Kawahara, Hideki},
doi = {10.1121/1.1458024},
file = {:home/jongwook/Dropbox/References/YIN, a Fundamental Frequency Estimator for Speech and Music.pdf:pdf},
isbn = {0001-4966 (Print)},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
keywords = {F0,YIN},
mendeley-tags = {F0,YIN},
number = {4},
pages = {1917--1930},
pmid = {12002874},
title = {{YIN, a Fundamental Frequency Estimator for Speech and Music}},
url = {http://asa.scitation.org/doi/10.1121/1.1458024},
volume = {111},
year = {2002}
}
@inproceedings{wang2018deepclustering,
abstract = {The recently proposed deep clustering framework represents a sig- nificant step towards solving the cocktail party problem. This study proposes and compares a variety of alternative objective functions for training deep clustering networks. In addition, whereas the orig- inal deep clustering work relied on k-means clustering for test-time inference, here we investigate inference methods that are matched to the training objective. Furthermore, we explore the use of an im- proved chimera network architecture for speech separation, which combines deep clustering with mask-inference networks in a multi- objective training scheme. The deep clustering loss acts as a regular- izer while training the end-to-end mask inference network for best separation. With further iterative phase reconstruction, our best pro- posed method achieves a state-of-the-art 11.5 dB signal-to-distortion ratio (SDR) result on the publicly available wsj0-2mix dataset, with a much simpler architecture than the previous best approach.},
author = {Wang, Zhong Qiu and Roux, Jonathan Le and Hershey, John R.},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
doi = {10.1109/ICASSP.2018.8462507},
file = {:home/jongwook/Dropbox/References/Alternative Objective Functions for Deep Clustering.pdf:pdf},
isbn = {9781538646588},
issn = {15206149},
keywords = {Chimera network,Cocktail party problem,Deep clustering,Speaker-independent multi-talker speech separation},
pages = {686--690},
publisher = {IEEE},
title = {{Alternative Objective Functions for Deep Clustering}},
year = {2018}
}
@book{pirkle2014synthesizer,
author = {Pirkle, Will},
publisher = {CRC Press},
title = {{Designing software synthesizer plug-ins in C++: for RackAFX, VST3, and Audio Units}},
year = {2014}
}
@inproceedings{khadkevich2009hmm,
author = {Khadkevich, M and Omologo, Maurizio},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Use of Hidden Markov Models and Factored Language Models for Automatic Chord Recognition.pdf:pdf},
isbn = {9780981353708},
pages = {561--566},
title = {{Use of Hidden Markov Models and Factored Language Models for Automatic Chord Recognition.}},
year = {2009}
}
@book{goertzel2007agi,
author = {Goertzel, Ben and Pennachin, Cassio},
doi = {10.1007/978-3-540-68677-4_11},
file = {:home/jongwook/Dropbox/References/Artificial General Intelligence.pdf:pdf},
isbn = {9783540237334},
issn = {16112482},
publisher = {Springer},
title = {{Artificial General Intelligence}},
year = {2007}
}
@inproceedings{oudre2009chord,
archivePrefix = {arXiv},
arxivId = {1605.07008},
author = {Oudre, Laurent and Grenier, Yves and F{\'{e}}votte, C{\'{e}}dric},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
doi = {10.1109/MLSP.2016.7738895},
eprint = {1605.07008},
file = {:home/jongwook/Dropbox/References/Template-Based Chord Recognition Influence of the Chord Types.pdf:pdf},
isbn = {9780981353708},
issn = {21610371},
pages = {153--158},
title = {{Template-Based Chord Recognition: Influence of the Chord Types}},
year = {2009}
}
@incollection{talkin1995rapt,
author = {Talkin, David and Kleijn, W. B. and Paliwal, K. K.},
booktitle = {Speech Coding and Synthesis},
chapter = {14},
file = {:home/jongwook/Dropbox/References/A Robust Algorithm for Pitch Tracking (RAPT).pdf:pdf},
isbn = {978-0444821690},
pages = {495--518},
publisher = {Elsevier Science},
title = {{A Robust Algorithm for Pitch Tracking (RAPT)}},
year = {1995}
}
@inproceedings{miron2015separation,
author = {Miron, Marius and Carabias, Julio Jos{\'{e}} and Janer, Jordi},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Improving Score-Informed Source Separation for Classical Music Through Note Refinement.pdf:pdf},
pages = {448--454},
title = {{Improving Score-Informed Source Separation for Classical Music Through Note Refinement}},
url = {http://mtg.upf.edu/node/3291},
year = {2015}
}
@article{wu2016google,
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and {Jeff Klingner} and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Lukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
file = {:home/jongwook/Dropbox/References/Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
journal = {arXiv preprint arXiv:1609.08144},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
year = {2016}
}
@article{yang2017midinet,
author = {Yang, Li-Chia and Chou, Szu-Yu and Yang, Yi-Hsuan},
file = {:home/jongwook/Dropbox/References/MidiNet A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation using 1D and 2D Conditions.pdf:pdf},
journal = {arXiv preprint arXiv:1703.10847},
keywords = {Symbolic},
mendeley-tags = {Symbolic},
title = {{MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation using 1D and 2D Conditions}},
year = {2017}
}
@inproceedings{bittner2017deepsalience,
author = {Bittner, Rachel M. and Mcfee, Brian and Salamon, Justin and Li, Peter and Bello, Juan P.},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Deep Salience Representations for F0 Estimation in Polyphonic Music.pdf:pdf},
pages = {23--27},
title = {{Deep Salience Representations for F0 Estimation in Polyphonic Music}},
year = {2017}
}
@inproceedings{smaragdis2003nmf,
abstract = {In this paper we present a methodology for analyzing polyphonic musical passages comprised by notes that exhibit a harmonically fixed spectral profile (such as piano notes). Taking advantage of this unique note structure we can model the audio content of the musical passage by a linear basis transform and use non-negative matrix decomposition methods to estimate the spectral profile and the temporal information of every note. This approach results in a very simple and compact system that is not knowledge-based, but rather learns notes by observation.},
author = {Smaragdis, Paris and Brown, Judith C},
booktitle = {Proceedings of the {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics {(WASPAA)}},
doi = {10.1109/ASPAA.2003.1285860},
file = {:home/jongwook/Dropbox/References/Non-Negative Matrix Factorization for Polyphonic Music Transcription.pdf:pdf},
isbn = {0780378504},
issn = {0780378504},
number = {3},
pages = {177--180},
title = {{Non-Negative Matrix Factorization for Polyphonic Music Transcription}},
year = {2003}
}
@article{mao2018effectiveness,
archivePrefix = {arXiv},
arxivId = {1712.06391},
author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
doi = {10.1109/ICCV.2017.304},
eprint = {1712.06391},
file = {:home/jongwook/Dropbox/References//On the Effectiveness of Least Squares Generative Adversarial Networks.pdf:pdf;:home/jongwook/Dropbox/References/On the Effectiveness of Least Squares Generative Adversarial Networks.pdf:pdf},
isbn = {978-1-5386-1032-9},
issn = {15505499},
journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{On the Effectiveness of Least Squares Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1611.04076 http://arxiv.org/abs/1712.06391},
year = {2018}
}
@inproceedings{bock2012rnn,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {B{\"{o}}ck, Sebastian and Schedl, Markus},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
doi = {10.1109/ICASSP.2012.6287832},
eprint = {arXiv:1011.1669v3},
file = {:home/jongwook/Dropbox/References/Polyphonic Piano Note Transcription with Recurrent Neural Networks(2).pdf:pdf;:home/jongwook/Dropbox/References/Polyphonic Piano Note Transcription with Recurrent Neural Networks.pdf:pdf},
isbn = {9781467300469},
issn = {15206149},
keywords = {music information retrieval,neural networks},
pages = {1--4},
pmid = {15664853},
title = {{Polyphonic Piano Note Transcription with Recurrent Neural Networks}},
year = {2012}
}
@inproceedings{cheng2013annealing,
author = {Cheng, Tian and Dixon, Simon and Mauch, Matthias},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/A Deterministic Annealing EM Algorithm for Automatic Music Transcription.pdf:pdf},
pages = {1--6},
title = {{A Deterministic Annealing EM Algorithm for Automatic Music Transcription.}},
url = {papers3://publication/uuid/89EB0CDA-0DD1-423C-94C8-473CB1AFD504%5Cnhttp://ismir2013.ismir.net/wp-content/uploads/2013/09/155_Paper.pdf%5Cnhttp://matthiasmauch.de/_pdf/cheng_dae_2013.pdf%5Cnhttp://ismir2013.ismir.net/wp-content/uploads/2013/09/155_Paper.pd},
year = {2013}
}
@inproceedings{pons2018tagging,
abstract = {The lack of data tends to limit the outcomes of deep learning research - specially, when dealing with end-to-end learning stacks processing raw data such as waveforms. In this study we make use of musical labels annotated for 1.2 million tracks. This large amount of data allows us to unrestrictedly explore different front-end paradigms: from assumption-free models - using waveforms as input with very small convolutional filters; to models that rely on domain knowledge - log-mel spectrograms with a convolutional neural network designed to learn temporal and timbral features. Results suggest that while spectrogram-based models surpass their waveform-based counterparts, the difference in performance shrinks as more data are employed.},
archivePrefix = {arXiv},
arxivId = {1711.02520},
author = {Pons, Jordi and Nieto, Oriol and Prockup, Matthew and Schmidt, Erik M. and Ehmann, Andreas F. and Serra, Xavier},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
eprint = {1711.02520},
file = {:home/jongwook/Dropbox/References/End-to-End Learning for Music Audio Tagging at Scale.pdf:pdf},
title = {{End-to-End Learning for Music Audio Tagging at Scale}},
url = {http://arxiv.org/abs/1711.02520},
year = {2018}
}
@article{costantini2013nmf,
abstract = {Music transcription consists in transforming the musical content of audio data into a symbolic representation. The objective of this study is to investigate a transcription system for polyphonic piano. The proposed method focuses on temporal musical structures, note events and their main characteristics: the attack instant and the pitch. Onset detection exploits a time-frequency representation of the audio signal. Feature extraction is based on Sparse Nonnegative Matrix Factorization (SNMF) and Constant Q Transform (CQT), while note classification is based on Support Vector Machines (SVMs). Finally, to validate our method, we present a collection of experiments using a wide number of musical pieces of heterogeneous styles.},
author = {Costantini, Giovanni and Todisco, Massimiliano and Perfetti, Renzo},
file = {:home/jongwook/Dropbox/References/NMF Based Dictionary Learning for Automatic Transcription of Polyphonic Piano Music.pdf:pdf},
issn = {17905052},
journal = {WSEAS Transactions on Signal Processing},
keywords = {Classification,Constant Q transform,Music transcription,Nonnegative matrix factorization,Support vector machines},
number = {3},
pages = {148--157},
title = {{NMF Based Dictionary Learning for Automatic Transcription of Polyphonic Piano Music}},
volume = {9},
year = {2013}
}
@inproceedings{mohamed2016implicit,
archivePrefix = {arXiv},
arxivId = {1610.03483},
author = {Mohamed, Shakir and Lakshminarayanan, Balaji},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1610.03483},
file = {:home/jongwook/Dropbox/References/Learning in Implicit Generative Models(2).pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Learning in Implicit Generative Models}},
url = {http://arxiv.org/abs/1610.03483},
year = {2017}
}
@inproceedings{kingma2016iaf,
archivePrefix = {arXiv},
arxivId = {1606.04934},
author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1606.04934},
isbn = {9781611970685},
issn = {10495258},
keywords = {VAE},
mendeley-tags = {VAE},
title = {{Improving Variational Inference with Inverse Autoregressive Flow}},
url = {http://arxiv.org/abs/1606.04934},
year = {2016}
}
@article{bang2018rfgan,
archivePrefix = {arXiv},
arxivId = {1801.09195},
author = {Bang, Duhyeon and Shim, Hyunjung},
eprint = {1801.09195},
file = {:home/jongwook/Dropbox/References/Improved Training of Generative Adversarial Networks Using Representative Features.pdf:pdf},
journal = {arXiv preprint arXiv:1801.09195},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Improved Training of Generative Adversarial Networks Using Representative Features}},
url = {http://arxiv.org/abs/1801.09195},
year = {2018}
}
@inproceedings{farbood2001markov,
author = {Farbood, Mary and Schoner, Bernd},
booktitle = {Proceedings of the International Computer Music Conference {(ICMC)}},
file = {:home/jongwook/Dropbox/References/Analysis and Synthesis of Palestrina-Style Counterpoint Using Markov Chains.pdf:pdf},
title = {{Analysis and Synthesis of Palestrina-Style Counterpoint Using Markov Chains}},
year = {2001}
}
@article{hua2018wavenet,
archivePrefix = {arXiv},
arxivId = {1802.08370},
author = {Hua, Kanru},
eprint = {1802.08370},
file = {:home/jongwook/Dropbox/References/Do WaveNets Dream of Acoustic Waves.pdf:pdf},
journal = {arXiv preprint arXiv:1802.08370},
title = {{Do WaveNets Dream of Acoustic Waves?}},
url = {http://arxiv.org/abs/1802.08370},
year = {2018}
}
@article{cogliati2017lateral,
abstract = {{\textcopyright} 2017 IEEE. This letter extends our prior work on context-dependent piano transcription to estimate the length of the notes in addition to their pitch and onset. This approach employs convolutional sparse coding along with lateral inhibition constraints to approximate a musical signal as the sum of piano note waveforms (dictionary elements) convolved with their temporal activations. The waveforms are pre-recorded for the specific piano to be transcribed in the specific environment. A dictionary containing multiple waveforms per pitch is generated by truncating a long waveform for each pitch to different lengths. During transcription, the dictionary elements are fixed and their temporal activations are estimated and postprocessed to obtain the pitch, onset, and note length estimation. A sparsity penalty promotes globally sparse activations of the dictionary elements, and a lateral inhibition term penalizes concurrent activations of different waveforms corresponding to the same pitch within a temporal neighborhood, to achieve note length estimation. Experiments on the MIDI aligned piano sounds dataset show that the proposed approach significantly outperforms a state-of-the-art music transcription method trained in the same context-dependent setting in transcription accuracy.},
author = {Cogliati, Andrea and Duan, Zhiyao and Wohlberg, Brendt},
doi = {10.1109/LSP.2017.2666183},
file = {:home/jongwook/Dropbox/References/Piano Transcription with Convolutional Sparse Lateral Inhibition.pdf:pdf;:home/jongwook/Dropbox/References/Piano Transcription with Convolutional Sparse Lateral Inhibition.pdf:pdf},
issn = {10709908},
journal = {{IEEE} Signal Processing Letters},
keywords = {Automatic music transcription (AMT),convolutional sparse coding (CSC),lateral inhibition,offset detection},
number = {4},
pages = {392--396},
title = {{Piano Transcription with Convolutional Sparse Lateral Inhibition}},
volume = {24},
year = {2017}
}
@inproceedings{vincent2008denoising,
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
file = {:home/jongwook/Dropbox/References/Extracting and Composing Robust Features with Denoising Autoencoders.pdf:pdf},
keywords = {Auto-Encoders},
mendeley-tags = {Auto-Encoders},
organization = {ACM},
pages = {1096--1103},
title = {{Extracting and Composing Robust Features with Denoising Autoencoders}},
year = {2008}
}
@inproceedings{sutskever2014seq2seq,
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
file = {:home/jongwook/Dropbox/References/Sequence to sequence learning with neural networks.pdf:pdf},
keywords = {RNN},
mendeley-tags = {RNN},
pages = {3104--3112},
title = {{Sequence to sequence learning with neural networks}},
year = {2014}
}
@article{bengio2013representation,
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
file = {:home/jongwook/Dropbox/References/Representation Learning A Review and New Perspectives.pdf:pdf},
journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Survey},
mendeley-tags = {Survey},
number = {8},
pages = {1798--1828},
publisher = {IEEE},
title = {{Representation Learning: A Review and New Perspectives}},
volume = {35},
year = {2013}
}
@inproceedings{dannenberg2003following,
author = {Dannenberg, R B and Hu, N},
booktitle = {Proceedings of the International Computer Music Conference {(ICMC)}},
file = {:home/jongwook/Dropbox/References/Polyphonic Audio Matching for Score Following and Intelligent Audio Editors.pdf:pdf},
pages = {27--33},
title = {{Polyphonic Audio Matching for Score Following and Intelligent Audio Editors}},
year = {2003}
}
@inproceedings{smaragdis2006latent,
abstract = {In this paper we describe a model developed for the analysis of acoustic spectra. Unlike decompositions techniques that can result in difficult to interpret results this model explicitly models spectra as distributions and extracts sets of additive and semantically useful components that facilitate a variety of applications rang- ing from source separation, denoising, music transcription and sound recognition. This model is probabilistic in nature and is easily extended to produce sparse codes, and discover transform invariant components which can be optimized for particular applications.},
author = {Smaragdis, Paris and Raj, Bhiksha and Shashanka, Madhusudana},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
file = {:home/jongwook/Dropbox/References/A Probabilistic Latent Variable Model for Acoustic Modeling.pdf:pdf},
title = {{A Probabilistic Latent Variable Model for Acoustic Modeling}},
url = {http://www.merl.com/reports/docs/TR2006-121.pdf},
year = {2006}
}
@inproceedings{cont2006realtime,
abstract = {In this paper we introduce a new approach for realtime multiple pitch observation of musical instruments. The proposed algorithm is quite different from others in the literature both in its purpose and approach. It is destined not for continuous multiple f0recognition but rather for projection of the ongoing spectrum to learned pitch templates. The decomposition algorithm on the otherhand, does not compromise signal processing models for pitches and consists of an algorithm for efficient decomposition of a spectrum using known pitch structures and based on sparse non-negative constraints. After introducing the algorithm along with evaluations,a real-time implementation of the algorithm is provided for free download on the MaxMSP realtime programming environment.},
author = {Cont, Arshia},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Realtime Multiple Pitch Observation using Sparse Non-Negative Constraints.pdf:pdf},
isbn = {1550583492},
keywords = {machine learning,multiple-pitch observation,non-negative ma-,sparseness constraints,trix factorization},
pages = {206--211},
title = {{Realtime Multiple Pitch Observation using Sparse Non-Negative Constraints}},
url = {http://hal.upmc.fr/hal-00723223},
year = {2006}
}
@book{zubizarreta1998prosody,
author = {Zubizarreta, Maria Luisa},
file = {:home/jongwook/Dropbox/References/Prosody, Focus, and Word Order.pdf:pdf},
isbn = {0262240416},
publisher = {MIT Press},
title = {{Prosody, Focus, and Word Order}},
year = {1998}
}
@inproceedings{kilcher2017interpolation,
archivePrefix = {arXiv},
arxivId = {1710.11381},
author = {Kilcher, Yannic and Lucchi, Aurelien and Hofmann, Thomas},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1710.11381},
file = {:home/jongwook/Dropbox/References/Semantic Interpolation in Implicit Models.pdf:pdf},
number = {1},
pages = {1--28},
title = {{Semantic Interpolation in Implicit Models}},
url = {http://arxiv.org/abs/1710.11381},
year = {2017}
}
@inproceedings{thome2017crnn,
author = {Thom{\'{e}}, Carl and Ahlb{\"{a}}ck, Sven},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Polyphonic Pitch Detection With Convolutional Recurrent Neural Networks.pdf:pdf},
title = {{Polyphonic Pitch Detection With Convolutional Recurrent Neural Networks}},
year = {2017}
}
@inproceedings{dessein2010beta,
author = {Dessein, Arnaud and Cont, Arshia and Lemaitre, Guillaume},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Real-Time Polyphonic Music Transcription With Non-Negative Matrix Factorization and Beta-Divergence.pdf:pdf},
pages = {489--494},
title = {{Real-Time Polyphonic Music Transcription With Non-Negative Matrix Factorization and Beta-Divergence}},
year = {2010}
}
@article{caetano2013musical,
abstract = {Sound morphing is a transformation that gradually blurs the distinction between the source and target sounds. For musical instrument sounds, the morph must operate across timbre dimensions to create the auditory illusion of hybridmusical instruments. The ultimate goal of sound morphing is to perform perceptually linear transitions, which requires an appropriate model to represent the sounds being morphed and an interpolation function to obtain intermediate sounds. Typically, morphing techniques directly interpolate the parameters of the sound model without considering the perceptual impact or evaluating the results. Perceptual evaluations are cumbersome and not always conclusive. In this work, we seek parameters of a sound model that favor linear variation of perceptually motivated temporal and spectral features used to guide the morph towards more perceptually linear results. The requirement of linear variation of feature values gives rise to objective evaluation criteria for sound morphing. We investigate several spectral envelope morphing techniques to determine which spectral representation renders the most linear transformation in the spectral shape feature domain. We found that interpolation of line spectral frequencies gives the most linear spectral envelope morphs. Analogously, we study temporal envelope morphing techniques and we concluded that interpolation of cepstral coefficients results in the most linear temporal envelope morph.},
author = {Caetano, Marcelo and Rodet, Xavier},
doi = {10.1109/TASL.2013.2260154},
file = {:home/jongwook/Dropbox/References/Musical Instrument Sound Morphing Guided by Perceptually Motivated Features.pdf:pdf},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Musical instrument sounds,sound morphing,source-filter model},
number = {8},
pages = {1666--1675},
publisher = {IEEE},
title = {{Musical Instrument Sound Morphing Guided by Perceptually Motivated Features}},
volume = {21},
year = {2013}
}
@inproceedings{denton2015lapgan,
archivePrefix = {arXiv},
arxivId = {1506.05751},
author = {Denton, Emily L and Chintala, Soumith and Szlam, Arthur and Fergus, Rob and Others},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1506.05751},
file = {:home/jongwook/Dropbox/References/Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks.pdf:pdf},
isbn = {1505.05770},
issn = {10495258},
keywords = {GAN,LAPGAN},
mendeley-tags = {GAN,LAPGAN},
pages = {1486--1494},
title = {{Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks}},
url = {http://arxiv.org/abs/1506.05751},
year = {2015}
}
@inproceedings{wang2003msssim,
abstract = {The structural similarity image quality paradigm is based on the assumption that the human visual system is highly adapted for extracting structural information from the scene, and therefore a measure of structural similarity can provide a good approxima- tion to perceived image quality. This paper proposes a multi-scale structural similarity method, which supplies more flexibility than previous single-scale methods in incorporating the variations of viewing conditions. We develop an image synthesis method to calibrate the parameters that define the relative importance of dif- ferent scales. Experimental comparisons demonstrate the effec- tiveness of the proposed method.},
author = {Wang, Zhou and Simoncelli, Eero P and Bovik, Alan C},
booktitle = {Proceedings of the {IEEE} Asilomar Conference on Signals, Systems and Computers},
doi = {10.1109/ACSSC.2003.1292216},
file = {:home/jongwook/Dropbox/References/Multi-Scale Structural Similarity for Image Quality Assessment.pdf:pdf},
isbn = {0-7803-8104-1},
pages = {9--13},
title = {{Multi-Scale Structural Similarity for Image Quality Assessment}},
volume = {2},
year = {2003}
}
@inproceedings{benetos2015probabilistic,
author = {Benetos, Emmanouil and Weyde, Tillman},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/An Efficient Temporally-Constrained Probabilistic Model for Multiple-Instrument Music Transcription.pdf:pdf},
pages = {701--707},
title = {{An Efficient Temporally-Constrained Probabilistic Model for Multiple-Instrument Music Transcription}},
year = {2015}
}
@article{siedenburg2016timbre,
author = {Siedenburg, Kai and Fujinaga, Ichiro and McAdams, Stephen},
file = {:home/jongwook/Dropbox/References/A Comparison of Approaches to Timbre Descriptors in Music Information Retrieval and Music Psychology.pdf:pdf},
journal = {Journal of New Music Research},
keywords = {MIR,Psychology,Timbre},
mendeley-tags = {MIR,Psychology,Timbre},
number = {1},
pages = {27--41},
publisher = {Taylor & Francis},
title = {{A Comparison of Approaches to Timbre Descriptors in Music Information Retrieval and Music Psychology}},
volume = {45},
year = {2016}
}
@incollection{werbos1982backpropagation,
author = {Werbos, Paul J},
booktitle = {System Modeling and Optimization},
file = {:home/jongwook/Dropbox/References/Applications of Advances in Nonlinear Sensitivity Analysis.pdf:pdf},
keywords = {Theory},
mendeley-tags = {Theory},
pages = {762--770},
publisher = {Springer},
title = {{Applications of Advances in Nonlinear Sensitivity Analysis}},
year = {1982}
}
@inproceedings{donahue2016bigan,
archivePrefix = {arXiv},
arxivId = {1605.09782},
author = {Donahue, Jeff and Kr{\"{a}}henb{\"{u}}hl, Philipp and Darrell, Trevor},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
doi = {10.1038/nphoton.2013.187},
eprint = {1605.09782},
file = {:home/jongwook/Dropbox/References/Adversarial Feature Learning.pdf:pdf},
isbn = {2334-2536},
issn = {2334-2536},
keywords = {GAN},
mendeley-tags = {GAN},
pmid = {27377197},
title = {{Adversarial Feature Learning}},
url = {http://arxiv.org/abs/1605.09782},
year = {2017}
}
@article{salamon2014melody,
author = {Salamon, Justin and G{\'{o}}mez, Emilia and Ellis, Daniel P W and Richard, Ga{\"{e}}l},
file = {:home/jongwook/Dropbox/References/Melody Extraction from Polyphonic Music Signals Approaches, Applications, and Challenges.pdf:pdf},
journal = {{IEEE} Signal Processing Magazine},
number = {2},
pages = {118--134},
title = {{Melody Extraction from Polyphonic Music Signals: Approaches, Applications, and Challenges}},
volume = {31},
year = {2014}
}
@inproceedings{gregor2013darn,
archivePrefix = {arXiv},
arxivId = {1310.8499},
author = {Gregor, Karol and Danihelka, Ivo and Mnih, Andriy and Blundell, Charles and Wierstra, Daan},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1310.8499},
file = {:home/jongwook/Dropbox/References/Deep AutoRegressive Networks.pdf:pdf},
isbn = {9781634393973},
title = {{Deep AutoRegressive Networks}},
url = {http://arxiv.org/abs/1310.8499},
year = {2014}
}
@article{roberts2018hierarchical,
archivePrefix = {arXiv},
arxivId = {1803.05428},
author = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
eprint = {1803.05428},
file = {:home/jongwook/Dropbox/References/A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music(2).pdf:pdf},
journal = {arXiv preprint arXiv:1803.05428},
title = {{A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music}},
year = {2018}
}
@article{doersch2016tutorial,
author = {Doersch, Carl},
file = {:home/jongwook/Dropbox/References/Tutorial on Variational Autoencoders.pdf:pdf},
journal = {arXiv preprint arXiv:1606.05908},
keywords = {VAE},
mendeley-tags = {VAE},
title = {{Tutorial on Variational Autoencoders}},
year = {2016}
}
@inproceedings{downie2014mirex,
author = {Downie, J. Stephen and Hu, Xiao and Lee, Jin Ha and Choi, Kahyun and Cunningham, Sally Jo and Hao, Yun},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Ten Years of MIREX Reflections, Challenges, and Opportunities.pdf:pdf},
keywords = {MIR,MIREX},
mendeley-tags = {MIR,MIREX},
number = {Ismir},
organization = {ISMIR},
pages = {657--662},
title = {{Ten Years of MIREX: Reflections, Challenges, and Opportunities}},
year = {2014}
}
@article{martin1996blackboard,
abstract = {It is only very recently that systems have been de- veloped that transcribe polyphonic music with more than two voices in even limited generality. Two of these systems [Kashino et al.1995, Martin 1996] have been built within a blackboard framework, integrating front ends based on sinusoidal analysis with musical knowledge. These and other systems to date rely on instrument models for detecting octaves. Recent re- sults have shown that an autocorrelation-based front end may make bottom-up detection of octaves pos- sible, thereby improving system performance as well as reducing the distance between transcription models and human audition. This report outlines the black- board approach to automatic transcription and presents a new system based on the log-lag correlogram of [El- lis 1996]. Preliminary results are presented, outlining the bottom-up detection of octaves and transcription of simple polyphonic music.},
author = {Martin, Keith D.},
doi = {10.1121/1.416589},
file = {:home/jongwook/Dropbox/References/Automatic Transcription of Simple Polyphonic Music.pdf:pdf},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {399},
pages = {2813},
title = {{Automatic Transcription of Simple Polyphonic Music}},
volume = {100},
year = {1996}
}
@article{mcinnes2018umap,
archivePrefix = {arXiv},
arxivId = {1802.03426},
author = {McInnes, Leland and Healy, John},
eprint = {1802.03426},
file = {:home/jongwook/Dropbox/References/UMAP Uniform Manifold Approximation and Projection for Dimension Reduction.pdf:pdf},
journal = {arXiv preprint arXiv:1802.03426},
title = {{UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}},
url = {http://arxiv.org/abs/1802.03426},
year = {2018}
}
@inproceedings{ronneberger2015unet,
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention {(MICCAI)}},
editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M and Frangi, Alejandro F},
file = {:home/jongwook/Dropbox/References/U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:pdf},
isbn = {978-3-319-24574-4},
pages = {234--241},
publisher = {Springer International Publishing},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
year = {2015}
}
@inproceedings{cao2018bre,
author = {Cao, Yanshuai and Ding, Gavin Weiguang and Lui, Kry Yik-Chau and Huang, Ruitong},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:home/jongwook/Dropbox/References/Improving GAN Training via Binarized Representation Entropy (BRE) Regularization.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Improving GAN Training via Binarized Representation Entropy (BRE) Regularization}},
year = {2018}
}
@inproceedings{lee2011sparse,
author = {Lee, Cheng Te and Yang, Yi Hsuan and Chen, Homer},
booktitle = {Proceedings of the {IEEE} International Conference on Multimedia and Expo},
doi = {10.1109/ICME.2011.6012000},
file = {:home/jongwook/Dropbox/References/Automatic Transcription of Piano Music by Sparse Representation of Magnitude Spectra.pdf:pdf},
isbn = {9781612843490},
issn = {19457871},
keywords = {F0 estimation,l1-regularized minimization,multiple pitch estimation,sparse representation},
title = {{Automatic Transcription of Piano Music by Sparse Representation of Magnitude Spectra}},
year = {2011}
}
@inproceedings{rezende2015flow,
archivePrefix = {arXiv},
arxivId = {1505.05770},
author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1505.05770},
file = {:home/jongwook/Dropbox/References/Variational Inference with Normalizing Flows.pdf:pdf},
isbn = {1505.05770},
issn = {1938-7228},
title = {{Variational Inference with Normalizing Flows}},
url = {http://arxiv.org/abs/1505.05770},
volume = {37},
year = {2015}
}
@inproceedings{humphrey2012tonnetz,
author = {Humphrey, Eric J. and Cho, Taemin and Bello, Juan P.},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:home/jongwook/Dropbox/References/Learning a Robust Tonnetz-Space Transform for Automatic Chord Recognition.pdf:pdf},
isbn = {9781467300469},
pages = {453--456},
title = {{Learning a Robust Tonnetz-Space Transform for Automatic Chord Recognition}},
year = {2012}
}
@article{ewert2016admm,
abstract = {Given a musical audio recording, the goal of automatic music transcription is to determine a score-like representation of the piece underlying the recording. Despite significant interest within the research community, several studies have reported on a 'glass ceiling' effect, an apparent limit on the transcription accuracy that current methods seem incapable of overcoming. In this paper, we explore how much this effect can be mitigated by focusing on a specific instrument class and making use of additional information on the recording conditions available in studio or home recording scenarios. In particular, exploiting the availability of single note recordings for the instrument in use we develop a novel signal model employing variable-length spectro-temporal patterns as its central building blocks - tailored for pitched percussive instruments such as the piano. Temporal dependencies between spectral templates are modeled, resembling characteristics of factorial scaled hidden Markov models (FS-HMM) and other methods combining Non-Negative Matrix Factorization with Markov processes. In contrast to FS-HMMs, our parameter estimation is developed in a global, relaxed form within the extensible alternating direction method of multipliers (ADMM) framework, which enables the systematic combination of basic regularizers propagating sparsity and local stationarity in note activity with more complex regularizers imposing temporal semantics. The proposed method achieves an f-measure of 93-95% for note onsets on pieces recorded on a Yamaha Disklavier (MAPS DB).},
archivePrefix = {arXiv},
arxivId = {1606.00785},
author = {Ewert, Sebastian and Sandler, Mark},
doi = {10.1109/TASLP.2016.2593801},
eprint = {1606.00785},
file = {:home/jongwook/Dropbox/References/Piano Transcription in the Studio Using an Extensible Alternating Directions Framework.pdf:pdf},
issn = {23299290},
journal = {{IEEE/ACM} Transactions on Audio, Speech, and Language Processing},
keywords = {Alternating direction method of multipliers (ADMM),Markov-regularizer,music transcription,non-negative matrix factorization (NMF),structured sparsity},
number = {11},
pages = {1983--1997},
title = {{Piano Transcription in the Studio Using an Extensible Alternating Directions Framework}},
volume = {24},
year = {2016}
}
@inproceedings{humphrey2012rethinking,
author = {Humphrey, Eric J. and Bello, Juan P.},
booktitle = {Proceedings of the International Conference on Machine Learning and Applications and Workshops {(ICMLA)}},
doi = {10.1109/ICMLA.2012.220},
file = {:home/jongwook/Dropbox/References/Rethinking Automatic Chord Recognition with Convolutional Neural Networks.pdf:pdf},
isbn = {9780769549132},
keywords = {automatic music transcription,chord recognition,convolutional neural nets},
pages = {357--362},
title = {{Rethinking Automatic Chord Recognition with Convolutional Neural Networks}},
volume = {2},
year = {2012}
}
@inproceedings{babacan2013comparative,
author = {Babacan, Onur and Drugman, Thomas and D'Alessandro, Nicolas and Henrich, Nathalie and Dutoit, Thierry},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:home/jongwook/Dropbox/References/A Comparative Study of Pitch Extraction Algorithms on a Large Variety of Singing Sounds.pdf:pdf},
isbn = {9781479903566},
pages = {7815--7819},
title = {{A Comparative Study of Pitch Extraction Algorithms on a Large Variety of Singing Sounds}},
year = {2013}
}
@article{wang2004ssim,
abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Wang, Zhou and Bovik, Alan Conrad and Sheikh, Hamid Rahim and Simoncelli, Eero P.},
doi = {10.1109/TIP.2003.819861},
eprint = {9411012},
file = {:home/jongwook/Dropbox/References/Image Quality Assessment From Error Visibility to Structural Similarity.pdf:pdf},
isbn = {9781439829356},
issn = {10577149},
journal = {{IEEE} Transactions on Image Processing},
keywords = {Error sensitivity,Human visual system (HVS),Image coding,Image quality assessment,JPEG,JPEG2000,Perceptual quality,Structural information,Structural similarity (SSIM)},
number = {4},
pages = {600--612},
pmid = {15376593},
primaryClass = {chao-dyn},
title = {{Image Quality Assessment: From Error Visibility to Structural Similarity}},
volume = {13},
year = {2004}
}
@article{davy2003harmonic,
abstract = {This paper is concerned with the Bayesian analysis of musical signals. The ultimate \naim is to use Bayesian hierarchical structures in order to infer quantities at the highest \nlevel, including such quantities as musical pitch, dynamics, timbre, instrument identity, \netc. Analysis of real musical signals is complicated by many things, including the presence \nof transient sounds, noises and the complex structure of musical pitches in the frequency \ndomain. The problem is truly Bayesian in that there is a wealth of (often subjective) prior \nknwowledge about how musical signals are constructed, which can be exploited in order \nto achieve more accurate inference about the musical structure. Here we propose devel- \nopments to an earlier Bayesian model which describes each component `note' at a given \ntime in terms of a fundamental frequency, partials (`harmonics'), and amplitude. This \nbasic model is modified for greater realism to include non-white residuals, time-varying \namplitudes and partials `detuned' from the natural linear relationship. The unknown pa- \nrameters of the new model are simulated using a variable dimension MCMC algorithm, \nleading to a highly sophisticated analysis tool. We discuss how the models and algorithms \ncan be applied for feature extraction, polyphonic music transcription, source separation \nand restoration of musical sources. \n},
author = {Davy, Manuel and Godsill, Simon J.},
file = {:home/jongwook/Dropbox/References/Bayesian Harmonic Models for Musical Signal Analysis.pdf:pdf},
journal = {Bayesian Statistics},
keywords = {a uditory scene analysis,a utomatic pitch transcription,i nstrument c lassification,m usical analysis,p itch,tion},
pages = {1--16},
title = {{Bayesian Harmonic Models for Musical Signal Analysis}},
url = {http://www-lagis.univ-lille1.fr/$\sim$davy../papers/Davy_Bayes7_2002.pdf},
year = {2003}
}
@article{raczynski2013dynamic,
author = {Raczy{\'{n}}ski, Stanis{\l}aw A. and Vincent, Emmanuel and Sagayama, Shigeki},
doi = {10.1109/TASL.2013.2258012},
file = {:home/jongwook/Dropbox/References/Dynamic Bayesian Networks for Symbolic Polyphonic Pitch Modeling.pdf:pdf},
issn = {1558-7916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
number = {9},
pages = {1830--1840},
publisher = {IEEE},
title = {{Dynamic Bayesian Networks for Symbolic Polyphonic Pitch Modeling}},
volume = {21},
year = {2013}
}
@article{stoller2017separation,
archivePrefix = {arXiv},
arxivId = {1711.00048},
author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
eprint = {1711.00048},
file = {:home/jongwook/Dropbox/References/Adversarial Semi-Supervised Audio Source Separation applied to Singing Voice Extraction.pdf:pdf},
journal = {arXiv preprint arXiv:1711.00048},
keywords = {GAN,Source Separation},
mendeley-tags = {GAN,Source Separation},
title = {{Adversarial Semi-Supervised Audio Source Separation applied to Singing Voice Extraction}},
url = {http://arxiv.org/abs/1711.00048},
year = {2017}
}
@article{bengio2009deeplearning,
author = {Bengio, Yoshua},
file = {:home/jongwook/Dropbox/References/Learning Deep Architectures for AI.pdf:pdf},
journal = {Foundations and Trends in Machine Learning},
keywords = {Survey},
mendeley-tags = {Survey},
number = {1},
pages = {1--127},
publisher = {Now Publishers, Inc.},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@book{celma2010music,
author = {Celma, {\'{O}}scar},
file = {:home/jongwook/Dropbox/References/Music Recommendation and Discovery The Long Tail, Long Fail, and Long Play in the Digital Music Space.pdf:pdf},
isbn = {9783642132865},
publisher = {Springer},
title = {{Music Recommendation and Discovery: The Long Tail, Long Fail, and Long Play in the Digital Music Space}},
year = {2010}
}
@article{riedmiller1994mlp,
author = {Riedmiller, Martin},
file = {:home/jongwook/Dropbox/References/Advanced Supervised Learning in Multi-layer Perceptrons - From Backpropagation to Adaptive Learning Algorithms.pdf:pdf},
journal = {Computer Standards and Interfaces},
pages = {265--278},
title = {{Advanced Supervised Learning in Multi-layer Perceptrons - From Backpropagation to Adaptive Learning Algorithms}},
volume = {16},
year = {1994}
}
@article{li2017infinite,
archivePrefix = {arXiv},
arxivId = {1707.08438},
author = {Li, Samuel},
eprint = {1707.08438},
file = {:home/jongwook/Dropbox/References/Context-Independent Polyphonic Piano Onset Transcription with an Infinite Training Dataset.pdf:pdf},
journal = {arXiv preprint arXiv:1707.08438},
title = {{Context-Independent Polyphonic Piano Onset Transcription with an Infinite Training Dataset}},
url = {https://arxiv.org/abs/1707.08438v1},
year = {2017}
}
@inproceedings{unterthiner2017coulomb,
archivePrefix = {arXiv},
arxivId = {1708.08819},
author = {Unterthiner, Thomas and Nessler, Bernhard and Seward, Calvin and Klambauer, G{\"{u}}nter and Heusel, Martin and Ramsauer, Hubert and Hochreiter, Sepp},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1708.08819},
file = {:home/jongwook/Dropbox/References/Coulomb GANs Provably Optimal Nash Equilibria via Potential Fields.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields}},
url = {http://arxiv.org/abs/1708.08819},
year = {2017}
}
@phdthesis{benetos2012thesis,
author = {Benetos, Emmanouil},
file = {:home/jongwook/Dropbox/References/Automatic Transcription of Polyphonic Music Exploiting Temporal Evolution.pdf:pdf},
school = {Queen Mary University of London},
title = {{Automatic Transcription of Polyphonic Music Exploiting Temporal Evolution}},
year = {2012}
}
@inproceedings{bengio2007greedy,
author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo and Others},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
file = {:home/jongwook/Dropbox/References/Greedy Layer-Wise Training of Deep Networks.pdf:pdf},
pages = {153},
title = {{Greedy Layer-Wise Training of Deep Networks}},
volume = {19},
year = {2007}
}
@article{argenti2011bispectral,
abstract = {In the area of music information retrieval (MIR), automatic music transcription is considered one of the most challenging tasks, for which many different techniques have been proposed. This paper presents a new method for polyphonic music transcription: a system that aims at estimating pitch, onset times, durations, and intensity of concurrent sounds in audio recordings, played by one or more instruments. Pitch estimation is carried out by means of a front-end that jointly uses a constant-Q and a bispectral analysis of the input audio signal; subsequently, the processed signal is correlated with a fixed 2-D harmonic pattern. Onsets and durations detection procedures are based on the combination of the constant-Q bispectral analysis with information from the signal spectrogram. The detection process is agnostic and it does not need to take into account musicological and instrumental models or other a priori knowledge. The system has been validated against the standard Real-World Computing (RWC)&#x2014;Classical Audio Database. The proposed method has demonstrated good performances in the multiple <formula formulatype="inline"><tex Notation="TeX">$F0$</tex> </formula> tracking task, especially for piano-only automatic transcription at MIREX 2009.},
author = {Argenti, Fabrizio and Nesi, Paolo and Pantaleo, Gianni},
doi = {10.1109/TASL.2010.2093894},
file = {:home/jongwook/Dropbox/References/Automatic Transcription of Polyphonic Music Based on the Constant-Q Bispectral Analysis.pdf:pdf},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Audio signals processing,Bispectrum,Constant-Q analysis,Higher order spectra,Music information retrieval (MIR),Polyphonic music transcription},
number = {6},
pages = {1610--1630},
title = {{Automatic Transcription of Polyphonic Music Based on the Constant-Q Bispectral Analysis}},
volume = {19},
year = {2011}
}
@book{nierhaus2009composition,
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Nierhaus, Gerhard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {9809069v1},
file = {:home/jongwook/Dropbox/References/Algorithmic Composition Paradigms of Automated Music Generation.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Algorithmic Composition: Paradigms of Automated Music Generation}},
year = {2009}
}
@inproceedings{li2017vibrato,
author = {Li, Bochen and Dinesh, Karthik and Sharma, Gaurav and Duan, Zhiyao},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Video-Based Vibrato Detection and Analysis for Polyphonic String Music.pdf:pdf},
pages = {123--130},
title = {{Video-Based Vibrato Detection and Analysis for Polyphonic String Music}},
year = {2017}
}
@inproceedings{bergkirkpatrick2014unsupervised,
abstract = {We present a new probabilistic model for transcribing piano music from audio to a symbolic form. Our model reflects the process by which discrete musical events give rise to acoustic signals that are then superimposed to produce the observed data. As a result, the inference procedure for our model naturally resolves the source separation problem introduced by the the piano's polyphony. In order to adapt to the properties of a new instrument or acoustic environment being tran-scribed, we learn recording-specific spectral profiles and temporal envelopes in an unsupervised fashion. Our system outperforms the best published approaches on a standard piano transcription task, achieving a 10.6% relative gain in note onset F 1 on real piano audio.},
author = {Berg-Kirkpatrick, Taylor and Andreas, Jacob and Klein, Dan},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
file = {:home/jongwook/Dropbox/References/Unsupervised Transcription of Piano Music.pdf:pdf},
issn = {10495258},
pages = {1538--1546},
title = {{Unsupervised Transcription of Piano Music}},
year = {2014}
}
@misc{lecun1998mnist,
author = {LeCun, Yann and Cortes, Corinna and Burges, Christopher J C},
title = {{The MNIST Database of Handwritten Digits. http://yann.lecun.com/exdb/mnist/index.html}},
year = {1998}
}
@inproceedings{wu2011multipitch,
abstract = {Multipitch estimation techniques are widely used for music transcription and acquisition of musical data from digital signals. In this paper, we propose a flexible harmonic temporal timbre model to decompose the spectral energy of the signal in the time-frequency domain into individual pitched notes. Each note is modeled with a 2-dimensional Gaussian mixture. Unlike previous approaches, the proposed model is able to represent not only the harmonic partials but also the inharmonic attack of each note. We derive an Expectation-Maximization (EM) algorithm to estimate the parameters of this model and illustrate the higher performance of the proposed algorithm than NMF algorithm [9] and HTC algorithm [10] for the task of multipitch estimation over synthetic and real-world data.},
author = {Wu, Jun and Vincent, Emmanuel and Raczynski, Stanislaw Andrzej and Nishimoto, Takuya and Ono, Nobutaka and Sagayama, Shigeki},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
doi = {10.1109/ICASSP.2011.5946319},
file = {:home/jongwook/Dropbox/References/Multipitch Estimation by Joint Modeling of Harmonic and Transient Sounds.pdf:pdf},
isbn = {9781457705397},
issn = {15206149},
keywords = {EM algorithm,GMM,attack,multipitch estimation},
pages = {25--28},
title = {{Multipitch Estimation by Joint Modeling of Harmonic and Transient Sounds}},
year = {2011}
}
@article{kodali2017gan,
archivePrefix = {arXiv},
arxivId = {1705.07215},
author = {Kodali, Naveen and Abernethy, Jacob and Hays, James and Kira, Zsolt},
eprint = {1705.07215},
file = {:home/jongwook/Dropbox/References/On Convergence and Stability of GANs.pdf:pdf},
isbn = {978 0 340 99716 1},
journal = {arXiv preprint arXiv:1705.07215},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{On Convergence and Stability of GANs}},
url = {http://arxiv.org/abs/1705.07215},
year = {2017}
}
@inproceedings{daskalakis2018gan,
archivePrefix = {arXiv},
arxivId = {1711.00141},
author = {Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1711.00141},
file = {:home/jongwook/Dropbox/References/Training GANs with Optimism.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Training GANs with Optimism}},
url = {http://arxiv.org/abs/1711.00141},
year = {2018}
}
@article{silver2016alphago,
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Others},
file = {:home/jongwook/Dropbox/References/Mastering the Game of Go with Deep Neural Networks and Tree Search.pdf:pdf},
journal = {Nature},
keywords = {RL},
mendeley-tags = {RL},
number = {7587},
pages = {484--489},
publisher = {Nature Publishing Group},
title = {{Mastering the Game of Go with Deep Neural Networks and Tree Search}},
volume = {529},
year = {2016}
}
@inproceedings{gaussier2005plsa,
author = {Gaussier, Eric and Goutte, Cyril},
booktitle = {Proceedings of the {ACM SIGIR} Conference on Research and Development in Information Retrieval},
file = {:home/jongwook/Dropbox/References/Relation between PLSA and NMF and Implications.pdf:pdf},
isbn = {1595930345},
keywords = {document clustering,nmf,plsa,probabilistic models},
pages = {601--602},
title = {{Relation between PLSA and NMF and Implications}},
year = {2005}
}
@book{patel2010musiclanguage,
author = {Patel, Aniruddh D},
file = {:home/jongwook/Dropbox/References/Music, Language, and the Brain.pdf:pdf},
publisher = {Oxford university press},
title = {{Music, Language, and the Brain}},
year = {2010}
}
@inproceedings{salamon2017analysis,
author = {Salamon, Justin and Bittner, Rachel M. and Bonada, Jordi and Bosch, Juan J. and Gomez, Emilia and juan pablo Bello},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/An AnalysisSynthesis Framework for Automatic F0 Annotation of Multitrack Datasets.pdf:pdf},
keywords = {Synthesis},
mendeley-tags = {Synthesis},
pages = {71--78},
title = {{An Analysis/Synthesis Framework for Automatic F0 Annotation of Multitrack Datasets}},
year = {2017}
}
@article{dong2018binary,
abstract = {It has been shown recently that deep convolutional generative adversarial networks (GANs) can learn to generate music in the form of piano-rolls, which represent music by binary-valued time-pitch matrices. However, existing models can only generate real-valued piano-rolls and require further post-processing, such as hard thresholding (HT) or Bernoulli sampling (BS), to obtain the final binary-valued results. In this paper, we study whether we can have a convolutional GAN model that directly creates binary-valued piano-rolls by using binary neurons. Specifically, we propose to append to the generator an additional refiner network, which uses binary neurons at the output layer. The whole network is trained in two stages. Firstly, the generator and the discriminator are pretrained. Then, the refiner network is trained along with the discriminator to learn to binarize the real-valued piano-rolls the pretrained generator creates. Experimental results show that using binary neurons instead of HT or BS indeed leads to better results in a number of objective measures. Moreover, deterministic binary neurons perform better than stochastic ones in both objective measures and a subjective test. The source code, training data and audio examples of the generated results can be found at https://salu133445.github.io/bmusegan/ .},
archivePrefix = {arXiv},
arxivId = {1804.09399},
author = {Dong, Hao-Wen and Yang, Yi-Hsuan},
doi = {arXiv:1804.09399v2},
eprint = {1804.09399},
file = {:home/jongwook/Dropbox/References/Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation.pdf:pdf},
journal = {arXiv preprint arXiv:1804.09399},
title = {{Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation}},
url = {http://arxiv.org/abs/1804.09399},
year = {2018}
}
@inproceedings{karras2019stylegan,
abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
archivePrefix = {arXiv},
arxivId = {1812.04948},
author = {Karras, Tero and Laine, Samuli and Aila, Timo},
booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}},
eprint = {1812.04948},
file = {:home/jongwook/Dropbox/References/A Style-Based Generator Architecture for Generative Adversarial Networks.pdf:pdf},
pages = {4401--4410},
title = {{A Style-Based Generator Architecture for Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1812.04948},
year = {2019}
}
@misc{lecun2016unsupervised,
author = {LeCun, Yann},
file = {:home/jongwook/Dropbox/References/Unsupervised Learning.pdf:pdf},
publisher = {NYU},
title = {{Unsupervised Learning}},
year = {2016}
}
@article{sturm2013classification,
author = {Sturm, Bob L.},
doi = {10.1007/s10844-013-0250-y},
file = {:home/jongwook/Dropbox/References/Classification Accuracy is Not Enough On the Evaluation of Music Genre Recognition Systems.pdf:pdf},
issn = {09259902},
journal = {Journal of Intelligent Information Systems},
keywords = {Classification,Evaluation,Genre,Music},
number = {3},
pages = {371--406},
title = {{Classification Accuracy is Not Enough: On the Evaluation of Music Genre Recognition Systems}},
volume = {41},
year = {2013}
}
@article{li2007separation,
author = {Li, Yipeng and Wang, Deliang},
doi = {10.1109/TASL.2006.889789},
file = {:home/jongwook/Dropbox/References/Separation of Singing Voice From Music Accompaniment for Monaural Recordings.pdf:pdf},
isbn = {9781605607122},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
number = {4},
pages = {1475--1487},
title = {{Separation of Singing Voice From Music Accompaniment for Monaural Recordings}},
volume = {15},
year = {2007}
}
@inproceedings{prenger2019waveglow,
author = {Prenger, Ryan and Valle, Rafael and Catanzaro, Bryan},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:home/jongwook/Dropbox/References/WaveGlow A Flow-Based Generative Network for Speech Synthesis.pdf:pdf},
isbn = {9781538646588},
pages = {3617--3621},
title = {{WaveGlow : A Flow-Based Generative Network for Speech Synthesis}},
year = {2019}
}
@article{barry2018style,
author = {Barry, Shaun and Kim, Youngmoo},
file = {:home/jongwook/Dropbox/References/Style Transfer for Musical Audio Using Multiple Time-Frequency Representations.pdf:pdf},
journal = {OpenReview:BybQ7zWCb},
title = {{Style Transfer for Musical Audio Using Multiple Time-Frequency Representations}},
year = {2018}
}
@article{creswell2017gan,
archivePrefix = {arXiv},
arxivId = {1710.07035},
author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A},
doi = {10.1007/978-3-319-56991-8_32},
eprint = {1710.07035},
file = {:home/jongwook/Dropbox/References/Generative Adversarial Networks An Overview(2).pdf:pdf},
isbn = {9781509011216},
issn = {1701.07274},
journal = {{IEEE} Signal Processing Magazine},
keywords = {GAN,Survey},
mendeley-tags = {GAN,Survey},
pages = {53--65},
pmid = {15040217},
title = {{Generative Adversarial Networks: An Overview}},
url = {http://arxiv.org/abs/1710.07035},
year = {2017}
}
@inproceedings{wu2016gan,
abstract = {We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.},
archivePrefix = {arXiv},
arxivId = {1610.07584},
author = {Wu, Jiajun and Zhang, Chengkai and Xue, Tianfan and Freeman, William T. and Tenenbaum, Joshua B.},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1610.07584},
file = {:home/jongwook/Dropbox/References/Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling.pdf:pdf},
issn = {10495258},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling}},
url = {http://arxiv.org/abs/1610.07584},
year = {2016}
}
@inproceedings{ryynanen2005event,
abstract = {This paper proposes a method for the automatic transcription of real-world music signals, including a variety of musical genres. The method transcribes notes played with pitched musical instru-ments. Percussive sounds, such as drums, may be present but they are not transcribed. Musical notations (i.e., MIDI files) are produced from acoustic stereo input files using probabilistic note event modeling. Note events are described with a hidden Markov model (HMM). The model uses three acoustic features extracted with a multiple fundamental frequency (F0) estimator to calculate the likelihoods of different notes and performs temporal segmen-tation of notes. The transitions between notes are controlled with a musicological model involving musical key estimation and bigram models. The final transcription is obtained by searching for several paths through the note models. Evaluation was carried out with a realistic music database. Using strict evaluation criteria, 39% of all the notes were found (recall) and 41% of the transcribed notes were correct (precision). Taken the complexity of the considered transcription task, the results are encouraging.},
author = {Ryyn{\"{a}}nen, Matti P. and Klapuri, Anssi},
booktitle = {Proceedings of the {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics},
doi = {10.1109/ASPAA.2005.1540233},
file = {:home/jongwook/Dropbox/References/Polyphonic Music Transcription Using Note Event Modeling.pdf:pdf},
isbn = {0780391543},
pages = {319--322},
title = {{Polyphonic Music Transcription Using Note Event Modeling}},
url = {http://www.cs.tut.fi/sgn/arg/matti/ryynklapuri_polytrans_final.pdf%5Cnhttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1540233},
year = {2005}
}
@article{viterbi1967decoding,
author = {Viterbi, Andrew J.},
file = {:home/jongwook/Dropbox/References/Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.pdf:pdf},
journal = {{IEEE} Transactions on Information Theory},
number = {2},
pages = {260--269},
title = {{Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm}},
url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1054010},
volume = {13},
year = {1967}
}
@inproceedings{kingma2015adam,
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
file = {:home/jongwook/Dropbox/References/Adam A Method for Stochastic Optimization.pdf:pdf},
isbn = {9781450300728},
issn = {09252312},
keywords = {Optimizer},
mendeley-tags = {Optimizer},
pages = {1--15},
pmid = {172668},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2015}
}
@article{wessel1979timbre,
abstract = {Research on musical timbre typically seeks representations of the perceptual structure inherent in a set of sounds that have implications for expressive control over the sounds in composition and performance. With digital analysis-based sound synthesis and with experiments on tone quality perception, we can obtain representations of sounds that suggest ways to provide low-dimensional control over their perceptually important properties. In this paper, we will describe a system for taking subjective measures of perceptual contrast between sound objects and using this data as input to some computer programs. The computer programs use multidimensional scaling algorithms to generate geometric representations from the input data. In the timbral spaces that result from the scaling programs, the various tones can be represented as points and a good statistical relationship can be sought between the distances in the space and the contrast judgments between the corresponding tones. The spatial representation is given a psychoacoustical interpretation by relating its dimensions to the acoustical properties of the tones. Controls are then applied directly to these properties in synthesis. The control schemes to be described are for additive synthesis and allow for the manipulation of the evolving spectral energy distribution and various temporal features of the tones. Tests of the control schemes have been carried out in musical contexts. Particular emphasis will be given here to the construction of melodic lines in which the timbre is manipulated on a note-to-note basis. Implications for the design of human control interfaces and of software for real-time digital sound synthesizers will be discussed. Musical},
author = {Wessel, David L.},
doi = {10.2307/3680283},
file = {:home/jongwook/Dropbox/References/Timbre Space as a Musical Control Structure.pdf:pdf},
issn = {01489267},
journal = {Computer Music Journal},
pages = {45--52},
title = {{Timbre Space as a Musical Control Structure}},
year = {1979}
}
@article{kalchbrenner2018wavernn,
archivePrefix = {arXiv},
arxivId = {1802.08435},
author = {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and {Van Den Oord}, A{\"{a}}ron and Dieleman, Sander and Kavukcuoglu, Koray},
eprint = {1802.08435},
file = {:home/jongwook/Dropbox/References/Efficient Neural Audio Synthesis.pdf:pdf},
journal = {arXiv preprint arXiv:1802.08435},
title = {{Efficient Neural Audio Synthesis}},
year = {2018}
}
@incollection{hamanaka2013computational,
author = {Hamanaka, Masatoshi and Hirata, Keiji and Tojo, Satoshi},
booktitle = {Guide to Computing for Expressive Music Performance},
doi = {10.1007/978-1-4471-4123-5},
file = {:home/jongwook/Dropbox/References/Computational Music Theory and Its Applications to Expressive Performance and Composition.pdf:pdf},
isbn = {978-1-4471-4122-8},
pages = {205--234},
title = {{Computational Music Theory and Its Applications to Expressive Performance and Composition}},
year = {2013}
}
@inproceedings{oord2018parallel,
abstract = {The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.},
archivePrefix = {arXiv},
arxivId = {1711.10433},
author = {van den Oord, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and van den Driessche, George and Lockhart, Edward and Cobo, Luis C. and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1711.10433},
file = {:home/jongwook/Dropbox/References/Parallel WaveNet Fast High-Fidelity Speech Synthesis.pdf:pdf},
title = {{Parallel WaveNet: Fast High-Fidelity Speech Synthesis}},
url = {http://arxiv.org/abs/1711.10433},
year = {2018}
}
@inproceedings{arjovsky2017principled,
archivePrefix = {arXiv},
arxivId = {1605.07725},
author = {Arjovsky, Martin and Bottou, Leon},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
doi = {10.2507/daaam.scibook.2010.27},
eprint = {1605.07725},
file = {:home/jongwook/Dropbox/References/Towards Principled Methods for Training Generative Adversarial Networks.pdf:pdf},
isbn = {1584880309},
issn = {17269687},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {1--17},
title = {{Towards Principled Methods for Training Generative Adversarial Networks}},
year = {2017}
}
@inproceedings{cogliati2015temporal,
author = {Cogliati, Andrea and Duan, Zhiyao},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:home/jongwook/Dropbox/References/Piano Music Transcription Modeling Note Temporal Evolution.pdf:pdf},
isbn = {9781467369978},
pages = {429--433},
title = {{Piano Music Transcription Modeling Note Temporal Evolution}},
year = {2015}
}
@inproceedings{huang2017counterpoint,
author = {Huang, Cheng-Zhi Anna and Cooijmans, Tim and Roberts, Adam and Courville, Aaron and Eck, Douglas},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Counterpoint by Convolution.pdf:pdf},
pages = {211--218},
title = {{Counterpoint by Convolution}},
year = {2017}
}
@article{bojanowski2017latent,
abstract = {Generative Adversarial Networks (GANs) have been shown to be able to sample impressively realistic images. GAN training consists of a saddle point optimization problem that can be thought of as an adversarial game between a generator which produces the images, and a discriminator, which judges if the images are real. Both the generator and the discriminator are commonly parametrized as deep convolutional neural networks. The goal of this paper is to disentangle the contribution of the optimization procedure and the network parametrization to the success of GANs. To this end we introduce and study Generative Latent Optimization (GLO), a framework to train a generator without the need to learn a discriminator, thus avoiding challenging adversarial optimization problems. We show experimentally that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.},
archivePrefix = {arXiv},
arxivId = {1707.05776},
author = {Bojanowski, Piotr and Joulin, Armand and Lopez-Paz, David and Szlam, Arthur},
eprint = {1707.05776},
file = {:home/jongwook/Dropbox/References/Optimizing the Latent Space of Generative Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1707.05776},
title = {{Optimizing the Latent Space of Generative Networks}},
url = {http://arxiv.org/abs/1707.05776},
year = {2017}
}
@article{zhang2017stackgan2,
archivePrefix = {arXiv},
arxivId = {1710.10916},
author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
doi = {10.1109/ICCV.2017.629},
eprint = {1710.10916},
file = {:home/jongwook/Dropbox/References/StackGAN Realistic Image Synthesis with Stacked Generative Adversarial Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1710.10916},
keywords = {GAN},
mendeley-tags = {GAN},
pmid = {202927},
title = {{StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1710.10916},
year = {2017}
}
@article{alemi2017information,
archivePrefix = {arXiv},
arxivId = {1711.00464},
author = {Alemi, Alexander A. and Poole, Ben and Fischer, Ian and Dillon, Joshua V. and Saurous, Rif A. and Murphy, Kevin},
eprint = {1711.00464},
file = {:home/jongwook/Dropbox/References/An Information-Theoretic Analysis of Deep Latent-Variable Models.pdf:pdf},
journal = {arXiv preprint arXiv:1711.00464},
keywords = {Information Theory},
mendeley-tags = {Information Theory},
title = {{An Information-Theoretic Analysis of Deep Latent-Variable Models}},
url = {http://arxiv.org/abs/1711.00464},
year = {2017}
}
@article{mirza2014conditional,
abstract = {GAN;},
author = {Mirza, Mehdi and Osindero, Simon},
file = {:home/jongwook/Dropbox/References/Conditional Generative Adversarial Nets.pdf:pdf},
journal = {arXiv preprint arXiv:1411.1784},
title = {{Conditional Generative Adversarial Nets}},
year = {2014}
}
@article{blei2003lda,
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {:home/jongwook/Dropbox/References/Latent Dirichlet Allocation.pdf:pdf},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}
@article{ozerov2007separation,
author = {Ozerov, Alexey and Philippe, Pierrick and Bimbot, Fr{\'{e}}d{\'{e}}ric and Gribonval, R{\'{e}}mi},
doi = {10.1109/TASL.2007.899291},
file = {:home/jongwook/Dropbox/References/Adaptation of Bayesian Models for Single-Channel Source Separation and its Application to VoiceMusic Separation in Popular Songs.pdf:pdf},
isbn = {1558-7916},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
number = {5},
pages = {1564--1578},
title = {{Adaptation of Bayesian Models for Single-Channel Source Separation and its Application to Voice/Music Separation in Popular Songs}},
volume = {15},
year = {2007}
}
@inproceedings{dixon2000piano,
abstract = {We present work towards a computer system for the automatic transcription of piano perfor- mances. The system takes audio files containing polyphonic piano music as input, and produces MIDI output, representing the pitch, timing and volume of the musical notes. The aim of this work is not to reduce the performance data to common music notation, but to extract the per- formance parameters for a quantitative study of musical expression in piano performance. Stan- dard signal processing techniques based on the short time Fourier transform are used to create a time-frequency representation of the signal, and adaptive peak-picking and pattern matching al- gorithms are employed to find themusical notes. In order to perform large scale testing, the test process is automated by synthesizing audio data fromMIDI files using high quality sofware syn- thesis, and comparing results with the original MIDI data. The test data used is Mozart piano sonatas performed by a concert pianist. 1 INTRODUCTION This paper addresses the problem of ext},
author = {Dixon, Simon},
booktitle = {Proceedings of the Australasian Computer Music Conference},
file = {:home/jongwook/Dropbox/References/On the Computer Recognition of Solo Piano Music.pdf:pdf},
pages = {31--37},
title = {{On the Computer Recognition of Solo Piano Music}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.5.6425&rep=rep1&type=pdf},
year = {2000}
}
@article{bell1995blind,
archivePrefix = {arXiv},
arxivId = {arXiv:1511.06440v1},
author = {Bell, Anthony J. and Sejnowski, Terrence J.},
doi = {doi:10.1162/neco.1995.7.6.1129},
eprint = {arXiv:1511.06440v1},
file = {:home/jongwook/Dropbox/References/An Information-Maximization Approach to Blind Separation and Blind Deconvolution.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
number = {6},
pages = {1129--1159},
pmid = {7584893},
title = {{An Information-Maximization Approach to Blind Separation and Blind Deconvolution}},
volume = {7},
year = {1995}
}
@article{davy2006bayesian,
abstract = {This paper deals with the computational analysis of musical audio from recorded audio waveforms. This general problem includes, as subtasks, music transcription, extraction of musical pitch, dynamics, timbre, instrument identity, and source separation. Analysis of real musical signals is a highly ill-posed task which is made complicated by the presence of transient sounds, background interference, or the complex structure of musical pitches in the time-frequency domain. This paper focuses on models and algorithms for computer transcription of multiple musical pitches in audio, elaborated from previous work by two of the authors. The audio data are supposedly presegmented into fixed pitch regimes such as individual chords. The models presented apply to pitched (tonal) music and are formulated via a Gabor representation of nonstationary signals. A Bayesian probabilistic structure is employed for representation of prior information about the parameters of the notes. This paper introduces a numerical Bayesian inference strategy for estimation of the pitches and other parameters of the waveform. The improved algorithm is much quicker and makes the approach feasible in realistic situations. Results are presented for estimation of a known number of notes present in randomly generated note clusters from a real musical instrument database.},
author = {Davy, Manuel and Godsill, Simon and Idier, J{\'{e}}r{\^{o}}me},
doi = {10.1121/1.2168548},
file = {:home/jongwook/Dropbox/References/Bayesian Analysis of Polyphonic Western Tonal Music.pdf:pdf},
isbn = {0001-4966 (Print)},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {4},
pages = {2498--517},
pmid = {16642862},
title = {{Bayesian Analysis of Polyphonic Western Tonal Music.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16642862},
volume = {119},
year = {2006}
}
@book{sutton2018reinforcement,
author = {Sutton, Richard S and Barto, Andrew G},
file = {:home/jongwook/Dropbox/References/Reinforcement Learning An Introduction (2nd Edition).pdf:pdf},
keywords = {RL},
mendeley-tags = {RL},
publisher = {MIT press Cambridge},
title = {{Reinforcement Learning: An Introduction (2nd Edition)}},
year = {2018}
}
@article{subakan2017gan,
archivePrefix = {arXiv},
arxivId = {1710.10779},
author = {Subakan, Cem and Smaragdis, Paris},
eprint = {1710.10779},
file = {:home/jongwook/Dropbox/References/Generative Adversarial Source Separation.pdf:pdf},
journal = {arXiv preprint arXiv:1710.10779},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Generative Adversarial Source Separation}},
url = {http://arxiv.org/abs/1710.10779},
year = {2017}
}
@inproceedings{mauch2013adt,
author = {Mauch, M and Ewert, S},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/The Audio Degradation Toolbox and Its Application To Robustness Evaluation.pdf:pdf},
title = {{The Audio Degradation Toolbox and Its Application To Robustness Evaluation}},
year = {2013}
}
@article{cemgil2000tempogram,
author = {Cemgil, Ali Taylan and Kappen, Bert and Desain, Peter and Honing, Henkjan},
doi = {10.1080/09298210008565462},
file = {:home/jongwook/Dropbox/References/On Tempo Tracking Tempogram Representation and Kalman filtering.pdf:pdf},
isbn = {0929821000856},
issn = {0929-8215},
journal = {Journal of New Music Research},
number = {1967},
pages = {259--273},
title = {{On Tempo Tracking: Tempogram Representation and Kalman filtering}},
volume = {29},
year = {2000}
}
@article{maron1961naive,
author = {Maron, M. E.},
doi = {10.1145/321075.321084},
file = {:home/jongwook/Dropbox/References/Automatic Indexing An Experimental Inquiry.pdf:pdf},
isbn = {0004-5411},
issn = {00045411},
journal = {Journal of the ACM},
number = {3},
pages = {404--417},
title = {{Automatic Indexing: An Experimental Inquiry}},
volume = {8},
year = {1961}
}
@article{arik2017cloning,
abstract = {Voice cloning is a highly desired feature for personalized speech interfaces. Neural network based speech synthesis has been shown to gen-erate high quality speech for a large number of speakers. In this paper, we introduce a neural voice cloning system that takes a few audio sam-ples as input. We study two approaches: speaker adaptation and speaker encoding. Speaker adapta-tion is based on fine-tuning a multi-speaker gener-ative model with a few cloning samples. Speaker encoding is based on training a separate model to directly infer a new speaker embedding from cloning audios and to be used with a multi-speaker generative model. In terms of naturalness of the speech and its similarity to original speaker, both approaches can achieve good performance, even with very few cloning audios. 1 While speaker adaptation can achieve better naturalness and simi-larity, the cloning time or required memory for the speaker encoding approach is significantly less, making it favorable for low-resource deployment.},
archivePrefix = {arXiv},
arxivId = {1802.06006},
author = {Ark, Serca O and Chen, Jitong and Peng, Kainan and Ping, Wei and Zhou, Yanqi},
eprint = {1802.06006},
file = {:home/jongwook/Dropbox/References/Neural Voice Cloning with a Few Samples.pdf:pdf},
journal = {arXiv preprint arXiv:1802.06006},
title = {{Neural Voice Cloning with a Few Samples}},
url = {https://arxiv.org/pdf/1802.06006.pdf},
year = {2018}
}
@inproceedings{thickstun2017musicnet,
archivePrefix = {arXiv},
arxivId = {1611.09827},
author = {Thickstun, John and Harchaoui, Zaid and Kakade, Sham},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1611.09827},
file = {:home/jongwook/Dropbox/References/Learning Features of Music from Scratch.pdf:pdf},
title = {{Learning Features of Music from Scratch}},
year = {2017}
}
@article{seo2017wgan,
archivePrefix = {arXiv},
arxivId = {1712.05882},
author = {Seo, Junghoon and Jeon, Taegyun},
eprint = {1712.05882},
file = {:home/jongwook/Dropbox/References/On Reproduction of On the Regularization of Wasserstein GANs.pdf:pdf},
issn = {0002-3264},
journal = {arXiv preprint arXiv:1712.05882},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{On Reproduction of On the Regularization of Wasserstein GANs}},
url = {http://arxiv.org/abs/1712.05882},
year = {2017}
}
@inproceedings{von2010comparison,
author = {von dem Knesebeck, Adrian and Z{\"{o}}lzer, U},
booktitle = {Proceedings of the International Conference on Digital Audio Effects {(DAFx)}},
file = {:home/jongwook/Dropbox/References/Comparison of Pitch Trackers for Real-Time Guitar Effects.pdf:pdf},
isbn = {9783200019409},
title = {{Comparison of Pitch Trackers for Real-Time Guitar Effects}},
year = {2010}
}
@inproceedings{gregor2015draw,
archivePrefix = {arXiv},
arxivId = {1502.04623},
author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1502.04623},
file = {:home/jongwook/Dropbox/References/DRAW A Recurrent Neural Network For Image Generation.pdf:pdf},
title = {{DRAW: A Recurrent Neural Network For Image Generation}},
url = {http://arxiv.org/abs/1502.04623},
year = {2015}
}
@inproceedings{isola2017pix2pix,
archivePrefix = {arXiv},
arxivId = {1611.07004},
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}},
doi = {10.1109/CVPR.2017.632},
eprint = {1611.07004},
file = {:home/jongwook/Dropbox/References/Image-to-Image Translation with Conditional Adversarial Networks.pdf:pdf},
isbn = {978-1-5386-0457-1},
issn = {08883270},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {1125--1134},
pmid = {14706220},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}},
year = {2017}
}
@inproceedings{weninger2013nmf,
author = {Weninger, Felix and Kirst, Christian and Bungartz, Hans-joachim},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
doi = {10.1109/ICASSP.2013.6637598},
file = {:home/jongwook/Dropbox/References/A Discriminative Approach to Polyphonic Piano Note Transcription Using Supervised Non-Negative Matrix Factorization.pdf:pdf},
isbn = {9781479903566},
pages = {6--10},
title = {{A Discriminative Approach to Polyphonic Piano Note Transcription Using Supervised Non-Negative Matrix Factorization}},
year = {2013}
}
@inproceedings{fedus2018equilibrium,
archivePrefix = {arXiv},
arxivId = {1710.08446},
author = {Fedus, William and Rosca, Mihaela and Lakshminarayanan, Balaji and Dai, Andrew M. and Mohamed, Shakir and Goodfellow, Ian},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1710.08446},
file = {:home/jongwook/Dropbox/References/Many Paths to Equilibrium GANs Do Not Need to Decrease a Divergence At Every Step.pdf:pdf},
title = {{Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step}},
url = {http://arxiv.org/abs/1710.08446},
year = {2018}
}
@inproceedings{boersma1993praat,
author = {Boersma, Paul},
booktitle = {Proceedings of the Institute of Phonetic Sciences},
doi = {10.1371/journal.pone.0069107},
file = {:home/jongwook/Dropbox/References/Accurate Short-Term Analysis of the Fundamental Frequency and the Harmonics-To-Noise Ratio of a Sampled Sound.pdf:pdf},
pages = {97--110},
title = {{Accurate Short-Term Analysis of the Fundamental Frequency and the Harmonics-To-Noise Ratio of a Sampled Sound}},
volume = {17},
year = {1993}
}
@inproceedings{huang2012separation,
author = {Huang, Po Sen and Chen, Scott Deeann and Smaragdis, Paris and Hasegawa-Johnson, Mark},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
doi = {10.1109/ICASSP.2012.6287816},
file = {:home/jongwook/Dropbox/References/Singing-Voice Separation from Monaural Recording using Robust Principal Component Analysis.pdf:pdf},
isbn = {9781467300469},
issn = {15206149},
pages = {57--60},
title = {{Singing-Voice Separation from Monaural Recording using Robust Principal Component Analysis}},
year = {2012}
}
@article{winter2017ivegan,
archivePrefix = {arXiv},
arxivId = {1711.08646},
author = {Winter, Robin and Clevert, Djork-Arn{\'{e}}},
eprint = {1711.08646},
file = {:home/jongwook/Dropbox/References/IVE-GAN Invariant Encoding Generative Adversarial Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1711.08646},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{IVE-GAN: Invariant Encoding Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1711.08646},
year = {2017}
}
@article{radford2015dcgan,
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
doi = {10.1051/0004-6361/201527329},
eprint = {1511.06434},
file = {:home/jongwook/Dropbox/References/Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv preprint arXiv:1511.06434v2},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {1--16},
pmid = {23459267},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
@article{canadasquesada2010gaussian,
author = {{Ca{\~{n}}adas Quesada}, F. J. and {Ruiz Reyes}, N. and {Vera Candeas}, P. and Carabias, J. J. and Maldonado, S.},
doi = {10.1080/09298211003695579},
file = {:home/jongwook/Dropbox/References/A Multiple-F0 Estimation Approach Based on Gaussian Spectral Modelling for Polyphonic Music Transcription.pdf:pdf},
issn = {09298215},
journal = {Journal of New Music Research},
number = {1},
pages = {93--107},
title = {{A Multiple-F0 Estimation Approach Based on Gaussian Spectral Modelling for Polyphonic Music Transcription}},
volume = {39},
year = {2010}
}
@inproceedings{grimaldi2004taste,
author = {Grimaldi, Marco and Cunningham, P{\'{a}}draig},
booktitle = {Proceedings of the {ACM SIGMM} international workshop on Multimedia information retrieval {(MIR)}},
doi = {10.1145/1026711.1026740},
file = {:home/jongwook/Dropbox/References/Experimenting with Music Taste Prediction by User Profiling.pdf:pdf},
isbn = {1581139403},
title = {{Experimenting with Music Taste Prediction by User Profiling}},
year = {2004}
}
@inproceedings{marolt1999nn,
author = {Marolt, Matija},
booktitle = {Proceedings of the International Computer Music Conference {(ICMC)}},
file = {:home/jongwook/Dropbox/References/A Comparison of Feed Forward Neural Network Architectures for Piano Music Transcription.pdf:pdf},
title = {{A Comparison of Feed Forward Neural Network Architectures for Piano Music Transcription}},
url = {http://lgm.fri.uni-lj.si/$\sim$matic/clanki/ICMC99.pdf},
year = {1999}
}
@inproceedings{ledig2017superresolution,
archivePrefix = {arXiv},
arxivId = {1609.04802},
author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}},
doi = {10.1109/CVPR.2017.19},
eprint = {1609.04802},
file = {:home/jongwook/Dropbox/References/Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network(2).pdf:pdf},
isbn = {978-1-5386-0457-1},
issn = {0018-5043},
keywords = {GAN,Super-Resolution},
mendeley-tags = {GAN,Super-Resolution},
pmid = {428914},
title = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
url = {http://arxiv.org/abs/1609.04802},
year = {2017}
}
@inproceedings{michelsanti2017gan,
archivePrefix = {arXiv},
arxivId = {1709.01703},
author = {Michelsanti, Daniel and Tan, Zheng Hua},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association {(INTERSPEECH)}},
doi = {10.21437/Interspeech.2017-1620},
eprint = {1709.01703},
file = {:home/jongwook/Dropbox/References/Conditional Generative Adversarial Networks for Speech Enhancement and Noise-Robust Speaker Verification.pdf:pdf},
issn = {19909772},
keywords = {GAN,Generative Adversarial Networks,Speaker Verification,Speech Enhancement},
mendeley-tags = {GAN},
title = {{Conditional Generative Adversarial Networks for Speech Enhancement and Noise-Robust Speaker Verification}},
year = {2017}
}
@book{rowe2003musicianship,
author = {Rowe, Robert},
doi = {Book Review},
file = {:home/jongwook/Dropbox/References/Machine Musicianship.pdf:pdf},
isbn = {9780262681490},
issn = {09298215},
pmid = {3862},
title = {{Machine Musicianship}},
year = {2003}
}
@inproceedings{zhu2017cyclegan,
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}},
doi = {10.1109/ICCV.2017.244},
eprint = {1703.10593},
file = {:home/jongwook/Dropbox/References/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks(2).pdf:pdf},
isbn = {978-1-5386-1032-9},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {2223--2232},
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}},
url = {http://arxiv.org/abs/1703.10593},
year = {2017}
}
@inproceedings{poliner2007improving,
author = {Poliner, Graham E and Ellis, Daniel P W},
booktitle = {Proceedings of the {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics {(WASPAA)}},
file = {:home/jongwook/Dropbox/References/Improving Generalization for Classification-Based Polyphonic Piano Transcription.pdf:pdf},
isbn = {9781424416196},
pages = {86--89},
title = {{Improving Generalization for Classification-Based Polyphonic Piano Transcription}},
year = {2007}
}
@inproceedings{jansson2017separation,
author = {Jansson, Andreas and Humphrey, Eric and Montecchio, Nicola and Bittner, Rachel and Kumar, Aparna and Weyde, Tillman},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Singing Voice Separation With Deep U-Net Convolutional Networks.pdf:pdf},
keywords = {CNN},
mendeley-tags = {CNN},
pages = {745--751},
title = {{Singing Voice Separation With Deep U-Net Convolutional Networks}},
year = {2017}
}
@misc{chollet2015keras,
author = {Chollet, Fran{\c{c}}ois},
title = {{Keras: The Python Deep Learning Library. https://keras.io/}},
url = {https://keras.io},
year = {2015}
}
@article{klapuri2004transcription,
author = {Klapuri, Anssi P.},
doi = {10.1080/0929821042000317840},
file = {:home/jongwook/Dropbox/References/Automatic Music Transcription as We Know it Today.pdf:pdf},
isbn = {0929-8215},
issn = {0929-8215},
journal = {Journal of New Music Research},
number = {3},
pages = {269--282},
title = {{Automatic Music Transcription as We Know it Today}},
volume = {33},
year = {2004}
}
@book{klapuri2006transcription,
author = {Klapuri, Anssi P. and Davy, Manuel},
file = {:home/jongwook/Dropbox/References/Signal Processing Methods for Music Transcription.pdf:pdf},
isbn = {0387306676},
publisher = {Springer},
title = {{Signal Processing Methods for Music Transcription}},
year = {2006}
}
@article{ng2011sparse,
author = {Ng, Andrew},
file = {:home/jongwook/Dropbox/References/Sparse Autoencoder.pdf:pdf},
journal = {Stanford {CS294a} Lecture notes},
keywords = {Auto-Encoders},
mendeley-tags = {Auto-Encoders},
title = {{Sparse Autoencoder}},
year = {2011}
}
@article{noll1967cepstrum,
author = {Noll, A Michael},
doi = {10.1121/1.1910339},
file = {:home/jongwook/Dropbox/References/Cepstrum Pitch Determination.pdf:pdf},
isbn = {0001-4966 (Print) 0001-4966 (Linking)},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {2},
pages = {293--309},
pmid = {6040805},
title = {{Cepstrum Pitch Determination}},
volume = {41},
year = {1967}
}
@article{gatys2015style,
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
file = {:home/jongwook/Dropbox/References/A Neural Algorithm of Artistic Style.pdf:pdf},
journal = {arXiv preprint arXiv:1508.06576},
keywords = {Style Transfer},
mendeley-tags = {Style Transfer},
title = {{A Neural Algorithm of Artistic Style}},
year = {2015}
}
@inproceedings{raczynski2009nmf,
author = {Raczy{\'{n}}ski, Stanis{\l}aw A. and Ono, Nobutaka and Sagayama, Shigeki},
booktitle = {Proceedings of the {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics {(WASPAA)}},
file = {:home/jongwook/Dropbox/References/Note Detection With Dynamic Bayesian Networks as a Postanalysis Step for NMF-Based Multiple Pitch Estimation Techniques.pdf:pdf},
isbn = {9781424436798},
pages = {49--52},
title = {{Note Detection With Dynamic Bayesian Networks as a Postanalysis Step for NMF-Based Multiple Pitch Estimation Techniques}},
year = {2009}
}
@inproceedings{benetos2011transcription,
author = {Benetos, E. and Dixon, S.},
booktitle = {Proceedings of the Sound and Music Computing {(SMC)} Conference},
doi = {10.1121/1.4790351},
file = {:home/jongwook/Dropbox/References/Multiple-Instrument Polyphonic Music Transcription using a Convolutive Probabilistic Model.pdf:pdf},
isbn = {9781457706936},
issn = {00014966},
pmid = {23464042},
title = {{Multiple-Instrument Polyphonic Music Transcription using a Convolutive Probabilistic Model}},
url = {http://www.smcnetwork.org/system/files/smc2011_submission_51_0.pdf},
year = {2011}
}
@article{reis2012genetic,
abstract = {This paper presents a new method for multiple fundamental frequency (F0) estimation on piano recordings. We propose a framework based on a genetic algorithm in order to analyze the overlapping overtones and search for the most likely F0 combination. The search process is aided by adaptive spectral envelope modeling and dynamic noise level estimation: while the noise is dynamically estimated, the spectral envelope of previously recorded piano samples (internal database) is adapted in order to best match the piano played on the input signals and aid the search process for the most likely combination of F0s. For comparison, several state-of-the-art algorithms were run across various musical pieces played by different pianos and then compared using three different metrics. The proposed algorithm ranked first place on Hybrid Decay/Sustain Score metric, which has better correlation with the human hearing perception and ranked second place on both onset-only and onset-offset metrics. A previous genetic algorithm approach is also included in the comparison to show how the proposed system brings significant improvements on both quality of the results and computing time.},
author = {Reis, Gustavo and {Fernand{\'{e}}z De Vega}, Francisco and Ferreira, An{\'{i}}bal},
doi = {10.1109/TASL.2012.2201475},
file = {:home/jongwook/Dropbox/References/Automatic Transcription of Polyphonic Piano Music Using Genetic Algorithms, Adaptive Spectral Envelope Modeling, and Dynamic Noise Level.pdf:pdf},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Acoustic signal analysis,automatic music transcription,fundamental frequency (F0) estimation,music information retrieval,pitch perception},
number = {8},
pages = {2313--2328},
title = {{Automatic Transcription of Polyphonic Piano Music Using Genetic Algorithms, Adaptive Spectral Envelope Modeling, and Dynamic Noise Level Estimation}},
volume = {20},
year = {2012}
}
@inproceedings{bello2002database,
abstract = {We describe a new method for the estimation of multiple pitch information\nin recorded piano music. The method\n\nworks in the time-domain and makes use of a self-generating database\nof all possible notes. First, we show how\n\naccurate polyphonic pitch detection can be achieved given an adequate\ndatabase. Then an algorithm is proposed\n\nthat generates the database from the music, using estimation of predominant\npitches in the frequency-domain and\n\npitch-shifting techniques. Both systems generate a MIDI representation\nof the original signal. This method -that can\n\nbe generalized to any solo instrument- overcomes the usual constraints\nof the traditional frequency-domain approach\n\nregarding intervals and quantity of notes.},
author = {Bello, Juan Pablo and Daubet, Laurent and Sandler, Mark},
booktitle = {Proceedings of the {AES} Convention},
file = {:home/jongwook/Dropbox/References/Time-Domain Polyphonic Transcription Using Self-Generating Databases.pdf:pdf},
number = {112},
title = {{Time-Domain Polyphonic Transcription Using Self-Generating Databases}},
year = {2002}
}
@article{shao2017riemannian,
archivePrefix = {arXiv},
arxivId = {1711.08014},
author = {Shao, Hang and Kumar, Abhishek and Fletcher, P. Thomas},
eprint = {1711.08014},
file = {:home/jongwook/Dropbox/References/The Riemannian Geometry of Deep Generative Models.pdf:pdf},
journal = {arXiv preprint arXiv:1711.08014},
keywords = {Riemannian},
mendeley-tags = {Riemannian},
title = {{The Riemannian Geometry of Deep Generative Models}},
url = {http://arxiv.org/abs/1711.08014},
year = {2017}
}
@article{durrieu2011separation,
author = {Durrieu, Jean Louis and David, Bertrand and Richard, Gal},
doi = {10.1109/JSTSP.2011.2158801},
file = {:home/jongwook/Dropbox/References/A Musically Motivated Mid-Level Representation for Pitch Estimation and Musical Audio Source Separation.pdf:pdf},
isbn = {1932-4553},
issn = {19324553},
journal = {{IEEE} Journal on Selected Topics in Signal Processing},
number = {6},
pages = {1180--1191},
title = {{A Musically Motivated Mid-Level Representation for Pitch Estimation and Musical Audio Source Separation}},
volume = {5},
year = {2011}
}
@inproceedings{miron2017sourceseparation,
author = {Miron, Marius and Janer, Jordi and G{\'{o}}mez, Emilia},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Monaural Score-Informed Source Separation for Classical Music Using Convolutional Neural Networks.pdf:pdf},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {55--62},
title = {{Monaural Score-Informed Source Separation for Classical Music Using Convolutional Neural Networks}},
year = {2017}
}
@article{fuentes2013harmonic,
author = {Fuentes, Beno{\^{i}}t and Badeau, Roland and Richard, Ga{\"{e}}l},
file = {:home/jongwook/Dropbox/References/Harmonic Adaptive Latent Component Analysis of Audio and Application to Music Transcription.pdf:pdf},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
title = {{Harmonic Adaptive Latent Component Analysis of Audio and Application to Music Transcription}},
year = {2013}
}
@article{louppe2017nondifferentiable,
archivePrefix = {arXiv},
arxivId = {1707.07113},
author = {Louppe, Gilles and Cranmer, Kyle},
eprint = {1707.07113},
file = {:home/jongwook/Dropbox/References/Adversarial Variational Optimization of Non-Differentiable Simulators.pdf:pdf},
journal = {arXiv preprint arXiv:1707.07113},
keywords = {Variational},
mendeley-tags = {Variational},
title = {{Adversarial Variational Optimization of Non-Differentiable Simulators}},
url = {http://arxiv.org/abs/1707.07113},
year = {2017}
}
@article{hinton2006dbn,
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Hinton, Geoffrey and Osindero, Simon and Teh, Yee-Whye},
doi = {10.1162/neco.2006.18.7.1527},
eprint = {1111.6189v1},
file = {:home/jongwook/Dropbox/References/A Fast Learning Algorithm for Deep Belief Nets.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
number = {7},
pages = {1527--1554},
pmid = {16764513},
title = {{A Fast Learning Algorithm for Deep Belief Nets}},
volume = {18},
year = {2006}
}
@inproceedings{choi2019drum,
abstract = {We introduce DrummerNet, a drum transcription system that is trained in an unsupervised manner. DrummerNet does not require any ground-truth transcription and, with the data-scalability of deep neural networks, learns from a large unlabeled dataset. In DrummerNet, the target drum signal is first passed to a (trainable) transcriber, then reconstructed in a (fixed) synthesizer according to the transcription estimate. By training the system to minimize the distance between the input and the output audio signals, the transcriber learns to transcribe without ground truth transcription. Our experiment shows that DrummerNet performs favorably compared to many other recent drum transcription systems, both supervised and unsupervised.},
archivePrefix = {arXiv},
arxivId = {1906.03697},
author = {Choi, Keunwoo and Cho, Kyunghyun},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
eprint = {1906.03697},
file = {:home/jongwook/Dropbox/References/Deep Unsupervised Drum Transcription.pdf:pdf},
title = {{Deep Unsupervised Drum Transcription}},
url = {http://arxiv.org/abs/1906.03697},
year = {2019}
}
@inproceedings{hjelm2018bsgan,
abstract = {We introduce a novel approach to training generative adversarial networks, where we train a generator to match a target distribution that converges to the data distribution at the limit of a perfect discriminator. This objective can be interpreted as training a generator to produce samples that lie on the decision boundary of the current discriminator in training at each update, and we call a GAN trained using this algorithm a boundary-seeking GAN (BGAN). This approach can be used to train a generator with discrete output when the generator outputs a parametric conditional distribution. We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. Finally, we notice that the proposed boundary-seeking algorithm works even with continuous variables, and demonstrate its effectiveness with various natural image benchmarks.},
archivePrefix = {arXiv},
arxivId = {1702.08431},
author = {Hjelm, R Devon and Jacob, Athul Paul and Che, Tong and Cho, Kyunghyun and Bengio, Yoshua},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1702.08431},
file = {:home/jongwook/Dropbox/References/Boundary-Seeking Generative Adversarial Networks.pdf:pdf},
title = {{Boundary-Seeking Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1702.08431},
year = {2018}
}
@article{peeling2010factorization,
abstract = {We introduce a framework for probabilistic generative models of time-frequency coefficients of audio signals, using a matrix factorization parametrization to jointly model spectral characteristics such as harmonicity and temporal activations and excitations. The models represent the observed data as the superposition of statistically independent sources, and we consider variance-based models used in source separation and intensity-based models for non-negative matrix factorization. We derive a generalized expectation-maximization algorithm for inferring the parameters of the model and then adapt this algorithm for the task of polyphonic transcription of music using labeled training data. The performance of the system is compared to that of existing discriminative and model-based approaches on a dataset of solo piano music.},
author = {Peeling, Paul H. and Cemgil, A. Taylan and Godsill, Simon J.},
doi = {10.1109/TASL.2009.2029769},
file = {:home/jongwook/Dropbox/References/Generative Spectrogram Factorization Models for Polyphonic Piano Transcription.pdf:pdf},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Frequency estimation,Matrix decomposition,Music information retrieval (MIR),Spectral analysis,Time-frequency analysis},
number = {3},
pages = {519--527},
title = {{Generative Spectrogram Factorization Models for Polyphonic Piano Transcription}},
volume = {18},
year = {2010}
}
@article{wang2017tacotron,
archivePrefix = {arXiv},
arxivId = {1703.10135},
author = {Wang, Yuxuan and Skerry-Ryan, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Others},
doi = {10.21437/Interspeech.2017-1452},
eprint = {1703.10135},
file = {:home/jongwook/Dropbox/References/Tacotron A Fully End-to-End Text-To-Speech Synthesis Model.pdf:pdf},
issn = {19909772},
journal = {arXiv preprint arXiv:1703.10135},
keywords = {Sequence-To-sequence,Text-To-speech synthesis,end-To-end model.},
pages = {4006--4010},
title = {{Tacotron: A Fully End-to-End Text-To-Speech Synthesis Model}},
volume = {2017-Augus},
year = {2017}
}
@inproceedings{zhang2017stackgan,
archivePrefix = {arXiv},
arxivId = {1612.03242},
author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
booktitle = {Proceedings of the {IEEE} International Conference on Computer Vision {(ICCV)}},
doi = {10.1109/ICCV.2017.629},
eprint = {1612.03242},
file = {:home/jongwook/Dropbox/References/StackGAN Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
pmid = {202927},
title = {{StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1612.03242},
year = {2017}
}
@inproceedings{slaney1996automatic,
author = {Slaney, Malcolm and Covell, Michele and Lassiter, Bud},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:home/jongwook/Dropbox/References/Automatic Audio Morphing.pdf:pdf},
pages = {1001--1004},
title = {{Automatic Audio Morphing}},
year = {1996}
}
@inproceedings{cheng2016attackdecay,
abstract = {We demonstrate that piano transcription performance for a known piano can be improved by explicitly modelling piano acoustical features. The proposed method is based on non-negative matrix factorisation, with the following three refinements: (1) introduction of attack and harmonic decay components; (2) use of a spike-shaped note activation that is shared by these components; (3) modelling the harmonic decay with an exponential function. Transcription is performed in a supervised way, with the training and test datasets produced by the same piano. First we train parameters for the attack and decay components on isolated notes, then update only the note activations for transcription. Experiments show that the proposed model achieves 82% on note-wise and 79% on frame-wise F-measures on the 'ENSTDkCl' subset of the MAPS database, outper-forming the current published state of the art.},
author = {Cheng, Tian and Mauch, Matthias and Benetos, Emmanouil and Dixon, Simon},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/An AttackDecay Model for Piano Transcription.pdf:pdf},
title = {{An Attack/Decay Model for Piano Transcription}},
url = {https://code.soundsoftware.ac.uk/projects/},
year = {2016}
}
@book{pejrolo2017creating,
author = {Pejrolo, Andrea and Metcalfe, Scott B},
publisher = {Oxford University Press},
title = {{Creating sounds from scratch: A Practical guide to music synthesis for producers and composers}},
year = {2017}
}
@inproceedings{grindlay2010subspace,
abstract = {In this paper we present a general probabilistic model suitable for transcribing single-channel audio recordings containing multiple polyphonic sources. Our system requires no prior knowledge of the instruments in the mixture, although it can benefit from this information if available. In contrast to many existing polyphonic transcription systems, our approach explicitly models the individual instruments and is thereby able to assign detected notes to their respective sources. We use a set of training instruments to learn a model space which is then used during transcription to constrain the properties of models fit to the target mixture. In addition, we encourage model sparsity using a simple approach related to tempering. We evaluate our method on both recorded and synthesized two-instrument mixtures, obtaining average framelevel F-measures of up to 0.60 for synthesized audio and 0.53 for recorded audio. If knowledge of the instrument types in the mixture is available, we can increase these measures to 0.68 and 0.58, respectively, by initializing the model with parameters from similar instruments.},
author = {Grindlay, Graham and Ellis, Daniel P. W.},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/A Probabilistic Subspace Model for Multi-Instrument Polyphonic Transcription.pdf:pdf},
isbn = {9789039353813},
number = {Ismir},
pages = {21--26},
title = {{A Probabilistic Subspace Model for Multi-Instrument Polyphonic Transcription}},
url = {http://academiccommons.columbia.edu/catalog/ac:148409},
year = {2010}
}
@inproceedings{humphrey2011nlse,
author = {Humphrey, Eric J and Glennon, Aron P and Bello, Juan Pablo},
booktitle = {Proceedings of the International Conference on Machine Learning and Applications and Workshops {(ICMLA)}},
file = {:home/jongwook/Dropbox/References/Non-Linear Semantic Embedding for Organizing Large Instrument Sample Libraries.pdf:pdf},
organization = {IEEE},
pages = {142--147},
title = {{Non-Linear Semantic Embedding for Organizing Large Instrument Sample Libraries}},
volume = {2},
year = {2011}
}
@article{benetos2019amt,
author = {Benetos, Emmanouil and Dixon, Simon and Duan, Zhiyao and Ewert, Sebastian},
doi = {10.1109/MSP.2018.2869928},
file = {:home/jongwook/Dropbox/References/Automatic Music Transcription An Overview.pdf:pdf},
issn = {15580792},
journal = {{IEEE} Signal Processing Magazine},
number = {1},
pages = {20--30},
publisher = {IEEE},
title = {{Automatic Music Transcription: An Overview}},
volume = {36},
year = {2019}
}
@article{bello2011structure,
author = {Bello, Juan P.},
file = {:home/jongwook/Dropbox/References/Measuring Structural Similarity in Music.pdf:pdf},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Structure},
mendeley-tags = {Structure},
number = {7},
pages = {2013--2025},
title = {{Measuring Structural Similarity in Music}},
volume = {19},
year = {2011}
}
@inproceedings{donahue2017gan,
archivePrefix = {arXiv},
arxivId = {arXiv:1705.07904v2},
author = {Donahue, Chris and Lipton, Zachary C. and Balsubramani, Akshay and McAuley, Julian},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {arXiv:1705.07904v2},
file = {:home/jongwook/Dropbox/References/Semantically Decomposing the Latent Spaces of Generative Adversarial Networks(2).pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Semantically Decomposing the Latent Spaces of Generative Adversarial Networks}},
year = {2018}
}
@inproceedings{perraudin2013griffinlim,
author = {Perraudin, Nathana{\"{e}}l and Balazs, Peter and S{\o}ndergaard, Peter L},
booktitle = {Proceedings of the {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics {(WASPAA)}},
file = {:home/jongwook/Dropbox/References/A Fast Griffin-Lim Algorithm.pdf:pdf},
isbn = {9781479909728},
title = {{A Fast Griffin-Lim Algorithm}},
year = {2013}
}
@article{im2016gran,
abstract = {Gatys et al. (2015) showed that optimizing pixels to match features in a convolutional network with respect reference image features is a way to render images of high visual quality. We show that unrolling this gradient-based optimization yields a recurrent computation that creates images by incrementally adding onto a visual "canvas". We propose a recurrent generative model inspired by this view, and show that it can be trained using adversarial training to generate very good image samples. We also propose a way to quantitatively compare adversarial networks by having the generators and discriminators of these networks compete against each other.},
archivePrefix = {arXiv},
arxivId = {1602.05110},
author = {Im, Daniel Jiwoong and Kim, Chris Dongjoo and Jiang, Hui and Memisevic, Roland},
doi = {10.1007/s10909-016-1606-9},
eprint = {1602.05110},
file = {:home/jongwook/Dropbox/References/Generating Images with Recurrent Adversarial Networks.pdf:pdf},
issn = {15737357},
journal = {arXiv preprint arXiv:1602.05110},
title = {{Generating Images with Recurrent Adversarial Networks}},
url = {http://arxiv.org/abs/1602.05110},
year = {2016}
}
@article{fernandez2013ai,
author = {Fern{\'{a}}ndez, Jose D and Vico, Francisco},
file = {:home/jongwook/Dropbox/References/AI Methods in Algorithmic Composition A Comprehensive Survey.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {513--582},
title = {{AI Methods in Algorithmic Composition: A Comprehensive Survey}},
volume = {48},
year = {2013}
}
@article{donahue2017segan,
archivePrefix = {arXiv},
arxivId = {1711.05747},
author = {Donahue, Chris and Li, Bo and Prabhavalkar, Rohit},
eprint = {1711.05747},
file = {:home/jongwook/Dropbox/References/Exploring Speech Enhancement with Generative Adversarial Networks for Robust Speech Recognition.pdf:pdf},
journal = {arXiv preprint arXiv:1711.05747},
keywords = {GAN,Speech},
mendeley-tags = {GAN,Speech},
title = {{Exploring Speech Enhancement with Generative Adversarial Networks for Robust Speech Recognition}},
url = {http://arxiv.org/abs/1711.05747},
year = {2017}
}
@inproceedings{dieleman2013multiscale,
author = {Dieleman, Sander and Schrauwen, Benjamin},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
doi = {10.1109/IJCNN.2005.1556436},
file = {:home/jongwook/Dropbox/References/Multiscale Approaches to Music Audio Feature Learning.pdf:pdf},
isbn = {9780615900650},
issn = {14673045},
keywords = {Features,K,Multiscale,Technology and Engineering,feature learning,means,multiple timescales,music information retrieval},
mendeley-tags = {Features,Multiscale},
pages = {116--121},
pmid = {21555788},
title = {{Multiscale Approaches to Music Audio Feature Learning}},
year = {2013}
}
@article{leglaive2016prior,
author = {Leglaive, Simon and Badeau, Roland and Richard, Ga{\"{e}}l},
doi = {10.1109/TASLP.2016.2614140},
file = {:home/jongwook/Dropbox/References/Multichannel Audio Source Separation with Probabilistic Reverberation Priors.pdf:pdf},
issn = {23299290},
journal = {{IEEE/ACM} Transactions on Audio, Speech, and Language Processing},
number = {12},
pages = {2453--2465},
title = {{Multichannel Audio Source Separation with Probabilistic Reverberation Priors}},
volume = {24},
year = {2016}
}
@inproceedings{he2016resnet,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}},
file = {:home/jongwook/Dropbox/References/Deep Residual Learning for Image Recognition.pdf:pdf},
pages = {770--778},
title = {{Deep Residual Learning for Image Recognition}},
year = {2016}
}
@article{engel2019gansynth,
abstract = {Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.},
archivePrefix = {arXiv},
arxivId = {1902.08710},
author = {Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
eprint = {1902.08710},
file = {:home/jongwook/Dropbox/References/GANSynth Adversarial Neural Audio Synthesis.pdf:pdf},
journal = {arxiv preprint arXiv:1902.08710},
title = {{GANSynth: Adversarial Neural Audio Synthesis}},
url = {http://arxiv.org/abs/1902.08710},
year = {2019}
}
@article{peeters2011timbre,
abstract = {The analysis of musical signals to extract audio descriptors that can potentially characterize their timbre has been disparate and often too focused on a particular small set of sounds. The Timbre Toolbox provides a comprehensive set of descriptors that can be useful in perceptual research, as well as in music information retrieval and machine-learning approaches to content-based retrieval in large sound databases. Sound events are first analyzed in terms of various input representations (short-term Fourier transform, harmonic sinusoidal components, an auditory model based on the equivalent rectangular bandwidth concept, the energy envelope). A large number of audio descriptors are then derived from each of these representations to capture temporal, spectral, spectrotemporal, and energetic properties of the sound events. Some descriptors are global, providing a single value for the whole sound event, whereas others are time-varying. Robust descriptive statistics are used to characterize the time-varying descriptors. To examine the information redundancy across audio descriptors, correlational analysis followed by hierarchical clustering is performed. This analysis suggests ten classes of relatively independent audio descriptors, showing that the Timbre Toolbox is a multidimensional instrument for the measurement of the acoustical structure of complex sound signals. {\textcopyright} 2011 Acoustical Society of America.},
author = {Peeters, Geoffroy and Giordano, Bruno L. and Susini, Patrick and Misdariis, Nicolas and McAdams, Stephen},
doi = {10.1121/1.3642604},
file = {:home/jongwook/Dropbox/References/The Timbre Toolbox Extracting Audio Descriptors from Musical Signals.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {5},
pages = {2902--2916},
title = {{The Timbre Toolbox: Extracting Audio Descriptors from Musical Signals}},
volume = {130},
year = {2011}
}
@inproceedings{heusel2017ttur,
archivePrefix = {arXiv},
arxivId = {1706.08500},
author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1706.08500},
file = {:home/jongwook/Dropbox/References/GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium}},
url = {http://arxiv.org/abs/1706.08500},
year = {2017}
}
@inproceedings{petzka2018regularization,
archivePrefix = {arXiv},
arxivId = {1712.05882},
author = {Petzka, Henning and Fischer, Asja and Lukovnicov, Denis},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1712.05882},
file = {:home/jongwook/Dropbox/References/On the Regularization of Wasserstein GANs.pdf:pdf},
issn = {0002-3264},
keywords = {GAN},
title = {{On the Regularization of Wasserstein GANs}},
url = {http://arxiv.org/abs/1712.05882},
year = {2018}
}
@inproceedings{tikhonov2017generation,
archivePrefix = {arXiv},
arxivId = {1705.05458},
author = {Tikhonov, Alexey and Yamshchikov, Ivan P.},
booktitle = {Proceedings of the International Symposium on Computer Music Multidisciplinary Research {(CMMR)}},
eprint = {1705.05458},
file = {:home/jongwook/Dropbox/References/Music Generation with Variational Recurrent Autoencoder Supported by History.pdf:pdf},
keywords = {artificial intelligence,variational recurrent autoencoder},
title = {{Music Generation with Variational Recurrent Autoencoder Supported by History}},
url = {http://arxiv.org/abs/1705.05458},
year = {2017}
}
@article{wang2017pix2pixhd,
archivePrefix = {arXiv},
arxivId = {1711.11585},
author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
eprint = {1711.11585},
file = {:home/jongwook/Dropbox/References/High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs.pdf:pdf},
journal = {arXiv preprint arXiv:1711.11585},
title = {{High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs}},
year = {2017}
}
@book{everitt1981mixture,
author = {Everitt, B. S. and Hand, D. J.},
file = {:home/jongwook/Dropbox/References/Finite Mixture Distributions.pdf:pdf},
publisher = {Chapman and Hall},
title = {{Finite Mixture Distributions}},
year = {1981}
}
@inproceedings{arora2017gan,
abstract = {We show that training of generative adversarial network (GAN) may not have good generalization properties; e.g., training may appear successful but the trained distribution may be far from target distribution in standard metrics. However, generalization does occur for a weaker metric called neural net distance. It is also shown that an approximate pure equilibrium exists in the discriminator/generator game for a special class of generators with natural training objectives when generator capacity and training set sizes are moderate. This existence of equilibrium inspires MIX+GAN protocol, which can be combined with any existing GAN training, and empirically shown to improve some of them.},
archivePrefix = {arXiv},
arxivId = {1703.00573},
author = {Arora, Sanjeev and Ge, Rong and Liang, Yingyu and Ma, Tengyu and Zhang, Yi},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1703.00573},
file = {:home/jongwook/Dropbox/References/Generalization and Equilibrium in Generative Adversarial Nets (GANs).pdf:pdf},
issn = {1938-7228},
title = {{Generalization and Equilibrium in Generative Adversarial Nets (GANs)}},
url = {http://arxiv.org/abs/1703.00573},
year = {2017}
}
@article{kassler1966mir,
author = {Kassler, Michael},
doi = {10.2307/832213},
file = {:home/jongwook/Dropbox/References/Toward Musical Information Retrieval.pdf:pdf},
isbn = {00316016},
issn = {00316016},
journal = {Perspectives of New Music},
keywords = {MIR},
mendeley-tags = {MIR},
number = {2},
pages = {59},
publisher = {JSTOR},
title = {{Toward Musical Information Retrieval}},
volume = {4},
year = {1966}
}
@inproceedings{gowrishankar2016transcription,
author = {Gowrishankar, B. S. and Bhajantri, Nagappa U.},
booktitle = {Proceedings of the International Conference on Signal Processing, Communication, Power and Embedded System {(SCOPES)}},
file = {:home/jongwook/Dropbox/References/An Exhaustive Review of Automatic Music Transcription Techniques - Survey of Music Transcription Techniques.pdf:pdf},
isbn = {9781509046201},
pages = {140--152},
title = {{An Exhaustive Review of Automatic Music Transcription Techniques - Survey of Music Transcription Techniques}},
year = {2016}
}
@inproceedings{sonderby2016amortised,
archivePrefix = {arXiv},
arxivId = {1610.04490},
author = {S{\o}nderby, Casper Kaae and Caballero, Jose and Theis, Lucas and Shi, Wenzhe and Husz{\'{a}}r, Ferenc},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
doi = {10.1007/s00138-014-0623-4},
eprint = {1610.04490},
file = {:home/jongwook/Dropbox/References/Amortised MAP Inference for Image Super-resolution.pdf:pdf},
isbn = {9783901608353},
issn = {14321769},
keywords = {GAN,Super-Resolution},
mendeley-tags = {GAN,Super-Resolution},
title = {{Amortised MAP Inference for Image Super-resolution}},
url = {http://arxiv.org/abs/1610.04490},
year = {2017}
}
@inproceedings{chorowski2015speech,
archivePrefix = {arXiv},
arxivId = {1506.07503},
author = {Chorowski, Jan K. and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {10.1016/j.asr.2015.02.035},
eprint = {1506.07503},
file = {:home/jongwook/Dropbox/References/Attention-Based Models for Speech Recognition.pdf:pdf},
issn = {18791948},
pages = {577--585},
title = {{Attention-Based Models for Speech Recognition}},
year = {2015}
}
@inproceedings{bittner2014medleydb,
author = {Bittner, Rachel M and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan Pablo},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/MedleyDB A Multitrack Dataset for Annotation-Intensive MIR Research.pdf:pdf},
keywords = {Dataset},
mendeley-tags = {Dataset},
pages = {155--160},
title = {{MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research.}},
volume = {14},
year = {2014}
}
@article{kameoka2007multipitch,
abstract = {A voiced spectrum model and ML ptich contour(cubic spline model) estimation},
author = {Kameoka, Hirokazu and Nishimoto, Takuya and Sagayama, Shigeki},
doi = {10.1109/TASL.2006.885248},
file = {:home/jongwook/Dropbox/References/A Multipitch Analyzer Based on Harmonic.pdf:pdf},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
number = {3},
pages = {982--994},
title = {{A Multipitch Analyzer Based on Harmonic}},
volume = {15},
year = {2007}
}
@inproceedings{vogl2017drum,
author = {Vogl, Richard and Dorfer, Matthias and Widmer, Gerhard and Knees, Peter},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Drum Transcription via Joint Beat and Drum Modeling using Convolutional Recurrent Neural Networks.pdf:pdf},
pages = {150--157},
title = {{Drum Transcription via Joint Beat and Drum Modeling using Convolutional Recurrent Neural Networks}},
year = {2017}
}
@inproceedings{chen2016infogan,
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
file = {:home/jongwook/Dropbox/References/InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {2172--2180},
title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
year = {2016}
}
@inproceedings{burraston2004automata,
author = {Burraston, Dave and Edmonds, Ernest and Livingstone, Dan and Miranda, Eduardo Reck},
booktitle = {Proceedings of the International Computer Music Conference {(ICMC)}},
file = {:home/jongwook/Dropbox/References/Cellular Automata in MIDI based Computer Music.pdf:pdf},
pages = {71--78},
title = {{Cellular Automata in MIDI based Computer Music}},
volume = {4},
year = {2004}
}
@inproceedings{caetano2010automatic,
abstract = {The aim of sound morphing is to obtain a result that falls perceptually between two (or more) sounds. In order to do this, we should be able to morph perceptually relevant features of sounds instead of blindly interpolating the parameters of a model. In this work we present automatic timbral morphing techniques applied to musical instrument sounds using high-level descriptors as features. High-level descriptors are measures of the acoustic correlates of salient timbre dimensions derived from perceptual studies, so that matching the descriptors becomes the goal itself to render the results more perceptually meaningful.},
author = {Caetano, Marcelo and Rodet, Xavier},
booktitle = {Proceedings of the International Computer Music Conference {(ICMC)}},
file = {:home/jongwook/Dropbox/References/Automatic Timbral Morphing of Musical Instrument Sounds by High-Level Descriptors.pdf:pdf},
isbn = {0971319286},
pages = {254--261},
title = {{Automatic Timbral Morphing of Musical Instrument Sounds by High-Level Descriptors}},
url = {http://hal.archives-ouvertes.fr/hal-00604390/},
year = {2010}
}
@inproceedings{thickstun2018invariances,
archivePrefix = {arXiv},
arxivId = {1711.04845},
author = {Thickstun, John and Harchaoui, Zaid and Foster, Dean and Kakade, Sham M.},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
eprint = {1711.04845},
file = {:home/jongwook/Dropbox/References/Invariances and Data Augmentation for Supervised Music Transcription.pdf:pdf},
title = {{Invariances and Data Augmentation for Supervised Music Transcription}},
url = {http://arxiv.org/abs/1711.04845},
year = {2018}
}
@inproceedings{montecchio2009score,
author = {Montecchio, Nicola and Orio, Nicola},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/A Discrete Filter Bank Approach to Audio to Score Mathcing for Polyphonic Music.pdf:pdf},
isbn = {9780981353708},
pages = {495--500},
title = {{A Discrete Filter Bank Approach to Audio to Score Mathcing for Polyphonic Music}},
year = {2009}
}
@article{benetos2013multi,
abstract = {A method for automatic transcription of polyphonic music is proposed in this work that models the temporal evolution of musical tones. The model extends the shift-invariant probabilistic latent component analysis method by supporting the use of spectral templates that correspond to sound states such as attack, sustain, and decay. The order of these templates is controlled using hidden Markov model-based temporal constraints. In addition, the model can exploit multiple templates per pitch and instrument source. The shift-invariant aspect of the model makes it suitable for music signals that exhibit frequency modulations or tuning changes. Pitch-wise hidden Markov models are also utilized in a postprocessing step for note tracking. For training, sound state templates were extracted for various orchestral instruments using isolated note samples. The proposed transcription system was tested on multiple-instrument recordings from various datasets. Experimental results show that the proposed model is superior to a non-temporally constrained model and also outperforms various state-of-the-art transcription systems for the same experiment.},
author = {Benetos, Emmanouil and Dixon, Simon},
doi = {10.1121/1.4790351},
file = {:home/jongwook/Dropbox/References/Multiple-Instrument Polyphonic Music Ttranscription Using a Temporally Constrained Shift-Invariant Model.pdf:pdf},
isbn = {9781457706936},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {3},
pages = {1727--1741},
pmid = {23464042},
title = {{Multiple-Instrument Polyphonic Music Ttranscription Using a Temporally Constrained Shift-Invariant Model}},
url = {http://asa.scitation.org/doi/10.1121/1.4790351},
volume = {133},
year = {2013}
}
@inproceedings{sarroff2014synthesis,
author = {Sarroff, Andy M and Casey, Michael A},
booktitle = {Proceedings of the International Computer Music Conference {(ICMC)}},
file = {:home/jongwook/Dropbox/References/Musical Audio Synthesis Using Autoencoding Neural Nets.pdf:pdf},
keywords = {Synthesis},
mendeley-tags = {Synthesis},
title = {{Musical Audio Synthesis Using Autoencoding Neural Nets}},
year = {2014}
}
@inproceedings{mroueh2018gan,
author = {Mroueh, Youssef and Li, Chun-Liang and Sercu, Tom and Raj, Anant and Cheng, Yu},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:home/jongwook/Dropbox/References/Sobolev GAN.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Sobolev GAN}},
year = {2018}
}
@inproceedings{pertusa2008gaussian,
abstract = {Abstract The goal of a polyphonic music transcription system is to extract a score from an audio signal. A multiple fundamental frequency estimator is the main piece of these systems, whereas tempo detection and key estimation complement them to correctly extract the ...},
author = {Pertusa, A and Inesta, J M},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
doi = {10.1109/ICASSP.2008.4517557},
file = {:home/jongwook/Dropbox/References/Multiple Fundamental Frequency Estimation Using Gaussian Smoothness.pdf:pdf},
isbn = {1-4244-1484-9},
issn = {1520-6149},
pages = {105--108},
title = {{Multiple Fundamental Frequency Estimation Using Gaussian Smoothness}},
url = {file:///Users/julio/Documents/Papers2/2008/Pertusa/2008Pertusa-Multiple fundamental frequency estimation using Gaussian smoothness-1.pdf%5Cnpapers2://publication/uuid/135D8F33-929C-4AC0-A909-A2D4F72B5A59%5Cnhttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnu},
year = {2008}
}
@inproceedings{abdallah2004sparse,
abstract = {We present a system for adaptive spectral basis decompo- sition that learns to identify independent spectral features given a sequence of short-term Fourier spectra. When ap- plied to recordings of polyphonic piano music, the indi- vidual notes are identified as salient features, and hence each short-term spectrum is decomposed into a sum of note spectra; the resulting encoding can be used as a ba- sis for polyphonic transcription. The system is based on a probabilistic model equivalent to a form of noisy inde- pendent component analysis (ICA) or sparse coding with non-negativity constraints. We introduce a novel mod- ification to this model that recognises that a short-term Fourier spectrum can be thought of as a noisy realisation of the power spectral density of an underlying Gaussian process, where the noise is essentially multiplicative and non-Gaussian. Results are presented for an analysis of a live recording of polyphonic piano music.},
author = {Abdallah, Samer and Plumbley, Mark},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Polyphonic Music Transcription by Non-Negative Sparse Coding of Power Spectra.pdf:pdf},
isbn = {84-88042-44-2},
title = {{Polyphonic Music Transcription by Non-Negative Sparse Coding of Power Spectra}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.110.4043&rep=rep1&type=pdf},
year = {2004}
}
@article{li2017dan,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.09549v3},
author = {Li, Chengtao and Alvarez-melis, David and Jegelka, Stefanie},
eprint = {arXiv:1706.09549v3},
file = {:home/jongwook/Dropbox/References/Distributional Adversarial Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1706.09549},
title = {{Distributional Adversarial Networks}},
year = {2017}
}
@inproceedings{tjoa2017accompaniment,
author = {Tjoa, Steven K and Meinard, M and College, Harvey Mudd},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Make Your Own Accompaniment Adapting Full-Mix Recordings To Match Solo-Only User Recordings.pdf:pdf},
pages = {79--86},
title = {{Make Your Own Accompaniment: Adapting Full-Mix Recordings To Match Solo-Only User Recordings}},
year = {2017}
}
@article{dannenberg2006alignment,
author = {Dannenberg, Roger B. and Raphael, Christopher},
doi = {10.1145/1145287.1145311},
file = {:home/jongwook/Dropbox/References/Music Score Alignment and Computer Accompaniment.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the {ACM}},
number = {8},
pages = {38},
title = {{Music Score Alignment and Computer Accompaniment}},
volume = {49},
year = {2006}
}
@book{chomsky1966generative,
author = {Chomsky, Noam},
file = {:home/jongwook/Dropbox/References/Topics in the Theory of Generative Grammar.pdf:pdf},
publisher = {Mouton},
title = {{Topics in the Theory of Generative Grammar}},
year = {1966}
}
@inproceedings{grindlay2009eigeninstruments,
author = {Grindlay, Graham and Ellis, Daniel P. W.},
booktitle = {Proceedings of the {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics {(WASPAA)}},
file = {:home/jongwook/Dropbox/References/Multi-Voice Polyphonic Music Transcription Using Eigeninstruments.pdf:pdf},
isbn = {9781424436798},
pages = {53--56},
title = {{Multi-Voice Polyphonic Music Transcription Using Eigeninstruments}},
year = {2009}
}
@inproceedings{bonada2008wideband,
author = {Bonada, Jordi},
booktitle = {Proceedings of the International Conference on Digital Audio Effects {(DAFx)}},
file = {:home/jongwook/Dropbox/References/Wide-Band Harmonic Sinusoidal Modeling.pdf:pdf},
isbn = {9789512295173},
title = {{Wide-Band Harmonic Sinusoidal Modeling}},
year = {2008}
}
@book{cook2002synthesis,
author = {Cook, Perry R},
file = {:home/jongwook/Dropbox/References/Real Sound Synthesis for Interactive Applications.pdf:pdf},
keywords = {Synthesis},
mendeley-tags = {Synthesis},
publisher = {CRC Press},
title = {{Real Sound Synthesis for Interactive Applications}},
year = {2002}
}
@inproceedings{mroueh2017fishergan,
abstract = {Generative Adversarial Networks (GANs) are powerful models for learning complex distributions. Stable training of GANs has been addressed in many recent works which explore different metrics between distributions. In this paper we introduce Fisher GAN which fits within the Integral Probability Metrics (IPM) framework for training GANs. Fisher GAN defines a critic with a data dependent constraint on its second order moments. We show in this paper that Fisher GAN allows for stable and time efficient training that does not compromise the capacity of the critic, and does not need data independent constraints such as weight clipping. We analyze our Fisher IPM theoretically and provide an algorithm based on Augmented Lagrangian for Fisher GAN. We validate our claims on both image sample generation and semi-supervised classification using Fisher GAN.},
archivePrefix = {arXiv},
arxivId = {1705.09675},
author = {Mroueh, Youssef and Sercu, Tom},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1705.09675},
file = {:home/jongwook/Dropbox/References/Fisher GAN.pdf:pdf},
title = {{Fisher GAN}},
url = {http://arxiv.org/abs/1705.09675},
year = {2017}
}
@inproceedings{conklin2016generation,
author = {Conklin, Darrell},
booktitle = {Proceedings of the {AISB} Symposium on Artificial Intelligence and Creativity in the Arts and Sciences},
doi = {10.1080/09298215.2016.1173708},
file = {:home/jongwook/Dropbox/References/Music Generation from Statistical Models.pdf:pdf},
issn = {17445027},
title = {{Music Generation from Statistical Models}},
year = {2003}
}
@article{zeiler2012adadelta,
archivePrefix = {arXiv},
arxivId = {1212.5701},
author = {Zeiler, Matthew D.},
eprint = {1212.5701},
file = {:home/jongwook/Dropbox/References/ADADELTA an Adaptive Learning Rate Method.pdf:pdf},
isbn = {1212.5701},
journal = {arXiv preprint arXiv:1212.5701},
keywords = {Optimizer},
mendeley-tags = {Optimizer},
title = {{ADADELTA: an Adaptive Learning Rate Method}},
url = {http://arxiv.org/abs/1212.5701},
year = {2012}
}
@article{camacho2008swipe,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Camacho, Arturo and Harris, John G.},
doi = {10.1121/1.2951592},
eprint = {arXiv:1011.1669v3},
file = {:home/jongwook/Dropbox/References/A Sawtooth Waveform Inspired Pitch Estimator for Speech and Music.pdf:pdf},
isbn = {1520-8524 (Electronic)},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {3},
pages = {1638--1652},
pmid = {19045655},
title = {{A Sawtooth Waveform Inspired Pitch Estimator for Speech and Music}},
volume = {124},
year = {2008}
}
@article{ulyanov2017age,
archivePrefix = {arXiv},
arxivId = {1704.02304},
author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
eprint = {1704.02304},
file = {:home/jongwook/Dropbox/References/It Takes (Only) Two Adversarial Generator-Encoder Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1704.02304},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{It Takes (Only) Two: Adversarial Generator-Encoder Networks}},
url = {http://arxiv.org/abs/1704.02304},
year = {2017}
}
@incollection{ono2010hpss,
author = {Ono, Nobutaka and Miyamoto, Kenichi and Kameoka, Hirokazu and Roux, Jonathan Le},
booktitle = {Advances in Music Information Retrieval},
file = {:home/jongwook/Dropbox/References/Harmonic and Percussive Sound Separation and Its Application to {MIR}-Related Tasks.pdf:pdf},
pages = {213--236},
publisher = {Springer},
title = {{Harmonic and Percussive Sound Separation and Its Application to {MIR}-Related Tasks}},
year = {2010}
}
@inproceedings{defferrard2016fma,
archivePrefix = {arXiv},
arxivId = {1612.01840},
author = {Defferrard, Micha{\"{e}}l and Benzi, Kirell and Vandergheynst, Pierre and Bresson, Xavier},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
eprint = {1612.01840},
file = {:home/jongwook/Dropbox/References/FMA A Dataset For Music Analysis.pdf:pdf},
keywords = {Dataset},
mendeley-tags = {Dataset},
pages = {316--323},
title = {{FMA: A Dataset For Music Analysis}},
url = {http://arxiv.org/abs/1612.01840},
year = {2016}
}
@inproceedings{molina2014humming,
author = {Molina, Emilio and Tard{\'{o}}n, Lorenzo J. and Barbancho, Isabel and Barbancho, Ana M.},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/The Importance of F0 Tracking in Query-by-singing-humming.pdf:pdf},
keywords = {F0},
mendeley-tags = {F0},
pages = {277--282},
title = {{The Importance of F0 Tracking in Query-by-singing-humming.}},
year = {2014}
}
@article{duchi2011adagrad,
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
file = {:home/jongwook/Dropbox/References/Adaptive subgradient methods for online learning and stochastic optimization.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Optimizer},
mendeley-tags = {Optimizer},
number = {Jul},
pages = {2121--2159},
title = {{Adaptive subgradient methods for online learning and stochastic optimization}},
volume = {12},
year = {2011}
}
@inproceedings{goodfellow2014gan,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {10.1017/CBO9781139924801},
eprint = {arXiv:1011.1669v3},
file = {:home/jongwook/Dropbox/References/Generative Adversarial Nets.pdf:pdf},
isbn = {9781139924801},
issn = {01420615},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {2672--2680},
pmid = {1107015359},
title = {{Generative Adversarial Nets}},
year = {2014}
}
@inproceedings{humphrey2015timely,
author = {Humphrey, Eric J and Bello, Juan P},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Four Timely Insights on Automatic Chord Estimation.pdf:pdf},
title = {{Four Timely Insights on Automatic Chord Estimation}},
year = {2015}
}
@article{shelhamer2017fcn,
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
file = {:home/jongwook/Dropbox/References/Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {01628828},
journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
number = {4},
pages = {640--651},
pmid = {16190471},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
volume = {39},
year = {2017}
}
@inproceedings{dong2018musegan,
abstract = {Generating music has a few notable differences from generating images and videos. First, music is an art of time, necessitating a temporal model. Second, music is usually composed of multiple instruments/tracks with their own temporal dynamics, but collectively they unfold over time interdependently. Lastly, musical notes are often grouped into chords, arpeggios or melodies in polyphonic music, and thereby introducing a chronological ordering of notes is not naturally suitable. In this paper, we propose three models for symbolic multi-track music generation under the framework of generative adversarial networks (GANs). The three models, which differ in the underlying assumptions and accordingly the network architectures, are referred to as the jamming model, the composer model and the hybrid model. We trained the proposed models on a dataset of over one hundred thousand bars of rock music and applied them to generate piano-rolls of five tracks: bass, drums, guitar, piano and strings. A few intra-track and inter-track objective metrics are also proposed to evaluate the generative results, in addition to a subjective user study. We show that our models can generate coherent music of four bars right from scratch (i.e. without human inputs). We also extend our models to human-AI cooperative music generation: given a specific track composed by human, we can generate four additional tracks to accompany it. All code, the dataset and the rendered audio samples are available at https://salu133445.github.io/musegan/ .},
archivePrefix = {arXiv},
arxivId = {1709.06298},
author = {Dong, Hao-Wen and Hsiao, Wen-Yi and Yang, Li-Chia and Yang, Yi-Hsuan},
booktitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
eprint = {1709.06298},
file = {:home/jongwook/Dropbox/References/MuseGAN Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment(2).pdf:pdf},
keywords = {Applications Track},
pages = {34--41},
title = {{MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment}},
url = {http://arxiv.org/abs/1709.06298},
year = {2017}
}
@inproceedings{miyato2018cgan,
author = {Miyato, Takeru and Koyama, Masanori},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:home/jongwook/Dropbox/References/cGANs with Projection Discriminator.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{cGANs with Projection Discriminator}},
year = {2018}
}
@article{bertin2010nmf,
abstract = {This paper presents theoretical and experimental results about constrained non-negative matrix factorization (NMF) in a Bayesian framework. A model of superimposed Gaussian components including harmonicity is proposed, while temporal continuity is enforced through an inverse-Gamma Markov chain prior. We then exhibit a space-alternating generalized expectation-maximization (SAGE) algorithm to estimate the parameters. Computational time is reduced by initializing the system with an original variant of multiplicative harmonic NMF, which is described as well. The algorithm is then applied to perform polyphonic piano music transcription. It is compared to other state-of-the-art algorithms, especially NMF-based. Convergence issues are also discussed on a theoretical and experimental point of view. Bayesian NMF with harmonicity and temporal continuity constraints is shown to outperform other standard NMF-based transcription systems, providing a meaningful mid-level representation of the data. However, temporal smoothness has its drawbacks, as far as transients are concerned in particular, and can be detrimental to transcription performance when it is the only constraint used. Possible improvements of the temporal prior are discussed.},
author = {Bertin, Nancy and Badeau, Roland and Vincent, Emmanuel},
doi = {10.1109/TASL.2010.2041381},
file = {:home/jongwook/Dropbox/References/Enforcing harmonicity and smoothness in bayesian non-negative matrix factorization applied to polyphonic music transcription.pdf:pdf;:home/jongwook/Dropbox/References/Enforcing Harmonicity and Smoothness in Bayesian Non-Negative Matrix Factorization Applied to Polyphonic Music Transcription.pdf:pdf},
isbn = {1558-7916},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Audio source separation,Bayesian regression,Music transcription,Non-negative matrix factorization (NMF),Unsupervised machine learning},
number = {3},
pages = {538--549},
pmid = {5410052},
title = {{Enforcing Harmonicity and Smoothness in Bayesian Non-Negative Matrix Factorization Applied to Polyphonic Music Transcription}},
volume = {18},
year = {2010}
}
@inproceedings{sotelo2017char2wav,
abstract = {We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder . The reader is an encoder- decoder model with attention. The encoder is a bidirectional recurrent neural net- work that accepts text or phonemes as inputs, while the decoder is a recurrent neu- ral network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.},
archivePrefix = {arXiv},
arxivId = {arXiv:1612.07837v2},
author = {Sotelo, Jose and Mehri, Soroush and Kumar, Kundan and Santos, Joao and Kastner, Kyle and Courville, Aaron and Bengio, Yoshua},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
doi = {10.1227/01.NEU.0000297116.62323.15},
eprint = {arXiv:1612.07837v2},
file = {:home/jongwook/Dropbox/References/Char2Wav End-to-End Speech Synthesis.pdf:pdf},
title = {{Char2Wav: End-to-End Speech Synthesis}},
url = {https://openreview.net/pdf?id=B1VWyySKx},
year = {2017}
}
@inproceedings{leroux2010spectrogram,
author = {{Le Roux}, Jonathan and Kameoka, Hirokazu and Ono, Nobutaka and Sagayama, Shigeki},
booktitle = {Proceedings of the International Conference on Digital Audio Effects {(DAFx)}},
file = {:home/jongwook/Dropbox/References/Fast Signal Reconstruction Frommagnitude Stft Spectrogram Based on Spectrogram Consistency.pdf:pdf},
isbn = {9783200019409},
title = {{Fast Signal Reconstruction Frommagnitude Stft Spectrogram Based on Spectrogram Consistency}},
year = {2010}
}
@article{engel2017nsynth,
archivePrefix = {arXiv},
arxivId = {1704.01279},
author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
eprint = {1704.01279},
file = {:home/jongwook/Dropbox/References/Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders.pdf:pdf},
journal = {arXiv preprint arXiv:1704.01279},
keywords = {Synthesis},
mendeley-tags = {Synthesis},
title = {{Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders}},
url = {http://arxiv.org/abs/1704.01279},
year = {2017}
}
@article{pascual2017segan,
archivePrefix = {arXiv},
arxivId = {1703.09452},
author = {Pascual, Santiago and Bonafonte, Antonio and Serr{\`{a}}, Joan},
doi = {10.21437/Interspeech.2017-1428},
eprint = {1703.09452},
file = {:home/jongwook/Dropbox/References/SEGAN Speech Enhancement Generative Adversarial Network(2).pdf:pdf},
issn = {19909772},
journal = {arXiv preprint arXiv:1703.09452},
keywords = {Convolutional neural networks.,Deep learning,Generative adversarial networks,Speech enhancement},
title = {{SEGAN: Speech Enhancement Generative Adversarial Network}},
year = {2017}
}
@inproceedings{kim2018crepe,
archivePrefix = {arXiv},
arxivId = {1802.06182},
author = {Kim, Jong Wook and Salamon, Justin and Li, Peter and Bello, Juan Pablo},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
eprint = {1802.06182},
file = {:home/jongwook/Dropbox/References/CREPE A Convolutional Representation for Pitch Estimation.pdf:pdf},
title = {{CREPE: A Convolutional Representation for Pitch Estimation}},
url = {http://arxiv.org/abs/1802.06182},
year = {2018}
}
@article{cemgil2006generative,
author = {Cemgil, Ali Taylan and Kappen, Hilbert J. and Member, Senior and Barber, David and Member, Senior and Barber, David},
doi = {10.1109/TSA.2005.852985},
file = {:home/jongwook/Dropbox/References/A Generative Model for Music Transcription(2).pdf:pdf;:home/jongwook/Dropbox/References/A Generative Model for Music Transcription.pdf:pdf},
issn = {1558-7916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
number = {2},
pages = {679--694},
title = {{A Generative Model for Music Transcription}},
volume = {14},
year = {2006}
}
@misc{lerdahl1983gttm,
author = {Lerdahl, Fred and Jackendoff, Ray},
file = {:home/jongwook/Dropbox/References/Generative Theory of Tonal Music.pdf:pdf},
publisher = {MIT Press},
title = {{Generative Theory of Tonal Music}},
year = {1983}
}
@article{ni2012harmonic,
abstract = {We present a new system for simultaneous estimation of keys, chords, and bass notes from music audio. It makes use of a novel chromagram representation of audio that takes perception of loudness into account. Furthermore, it is fully based on machine learning (instead of expert knowledge), such that it is potentially applicable to a wider range of genres as long as training data is available. As compared to other models, the proposed system is fast and memory efficient, while achieving state-of-the-art performance.},
author = {Ni, Yizhao and McVicar, Matt and Santos-Rodriguez, Ral and {De Bie}, Tijl},
doi = {10.1109/TASL.2012.2188516},
file = {:home/jongwook/Dropbox/References/An End-to-End Machine Learning System for Harmonic Analysis of Music.pdf:pdf},
issn = {15587916},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
keywords = {Audio chord estimation,harmony progression analyzer (HPA),loudness-based chromagram,machine learning,meta-song evaluation},
number = {6},
pages = {1771--1783},
title = {{An End-to-End Machine Learning System for Harmonic Analysis of Music}},
volume = {20},
year = {2012}
}
@inproceedings{mauch2014pyin,
author = {Mauch, Matthias and Dixon, Simon},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:home/jongwook/Dropbox/References/pYIN A Fundamental Frequency Estimator Using Probabilistic Threshold Distributions.pdf:pdf},
keywords = {F0},
mendeley-tags = {F0},
organization = {IEEE},
pages = {659--663},
title = {{pYIN: A Fundamental Frequency Estimator Using Probabilistic Threshold Distributions}},
year = {2014}
}
@inproceedings{larochelle2011nade,
author = {Larochelle, Hugo and Murray, Iain},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
file = {:home/jongwook/Dropbox/References/The Neural Autoregressive Distribution Estimator.pdf:pdf},
issn = {15324435},
pages = {29--37},
title = {{The Neural Autoregressive Distribution Estimator}},
volume = {15},
year = {2011}
}
@inproceedings{roth2017gan,
archivePrefix = {arXiv},
arxivId = {1705.09367},
author = {Roth, Kevin and Lucchi, Aurelien and Nowozin, Sebastian and Hofmann, Thomas},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {10.1007/s00138-014-0623-4},
eprint = {1705.09367},
file = {:home/jongwook/Dropbox/References/Stabilizing Training of Generative Adversarial Networks through Regularization.pdf:pdf},
isbn = {0013801406},
issn = {14321769},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Stabilizing Training of Generative Adversarial Networks through Regularization}},
url = {http://arxiv.org/abs/1705.09367},
year = {2017}
}
@inproceedings{kelz2016framewise,
abstract = {In an attempt at exploring the limitations of simple approaches to the task of piano transcription (as usually defined in MIR), we conduct an in-depth analysis of neural network-based framewise transcription. We systematically compare different popular input representations for transcription systems to determine the ones most suitable for use with neural networks. Exploiting recent advances in training techniques and new regularizers, and taking into account hyper-parameter tuning, we show that it is possible, by simple bottom-up frame-wise processing, to obtain a piano transcriber that outperforms the current published state of the art on the publicly available MAPS dataset -- without any complex post-processing steps. Thus, we propose this simple approach as a new baseline for this dataset, for future transcription research to build on and improve.},
archivePrefix = {arXiv},
arxivId = {1612.05153},
author = {Kelz, Rainer and Dorfer, Matthias and Korzeniowski, Filip and B{\"{o}}ck, Sebastian and Arzt, Andreas and Widmer, Gerhard},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
eprint = {1612.05153},
file = {:home/jongwook/Dropbox/References/On the Potential of Simple Framewise Approaches to Piano Transcription.pdf:pdf},
title = {{On the Potential of Simple Framewise Approaches to Piano Transcription}},
url = {http://arxiv.org/abs/1612.05153},
year = {2016}
}
@phdthesis{bello2003thesis,
author = {Bello, Juan Pablo},
file = {:home/jongwook/Dropbox/References/Towards the Automated Analysis of Simple Polyphonic Music A Knowledge-based Approach:},
school = {Queen Mary University of London},
title = {{Towards the Automated Analysis of Simple Polyphonic Music: A Knowledge-based Approach}},
year = {2003}
}
@inproceedings{pons2017timbre,
archivePrefix = {arXiv},
arxivId = {1703.06697},
author = {Pons, Jordi and Slizovskaia, Olga and Gong, Rong and G{\'{o}}mez, Emilia and Serra, Xavier},
booktitle = {Proceedings of the European Signal Processing Conference {(EUSIPCO)}},
eprint = {1703.06697},
file = {:home/jongwook/Dropbox/References/Timbre Analysis of Music Audio Signals with Convolutional Neural Networks.pdf:pdf},
isbn = {9780992862671},
keywords = {CNN,Timbre},
mendeley-tags = {CNN,Timbre},
title = {{Timbre Analysis of Music Audio Signals with Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1703.06697},
year = {2017}
}
@inproceedings{zuo2015crnn,
author = {Zuo, Zhen and Shuai, Bing and Wang, Gang and Liu, Xiao and Wang, Xingxing and Wang, Bing and Chen, Yushi},
booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition Workshops {(CVPRW)}},
doi = {10.1109/CVPRW.2015.7301268},
file = {:home/jongwook/Dropbox/References/Convolutional Recurrent Neural Networks Learning Spatial Dependencies for Image Representation.pdf:pdf},
isbn = {978-1-4673-6759-2},
issn = {21607516},
keywords = {CNN,CRNN,RNN},
mendeley-tags = {CNN,CRNN,RNN},
pages = {18--26},
title = {{Convolutional Recurrent Neural Networks: Learning Spatial Dependencies for Image Representation}},
url = {http://ieeexplore.ieee.org/document/7301268/},
year = {2015}
}
@inproceedings{ness2009tag,
author = {Ness, Steven R and Theocharis, Anthony and Tzanetakis, George and Martins, Luis Gustavo},
booktitle = {Proceedings of the {ACM} International Conference on Multimedia},
doi = {10.1145/1631272.1631393},
file = {:home/jongwook/Dropbox/References/Improving Automatic Music Tag Annotation using Stacked Generalization of Probabilistic SVM Outputs.pdf:pdf},
isbn = {9781605586083},
keywords = {folksonomies,music information retrieval,music recommendation,sound analysis,tags},
pages = {705--708},
title = {{Improving Automatic Music Tag Annotation using Stacked Generalization of Probabilistic SVM Outputs}},
year = {2009}
}
@inproceedings{liu2016cogan,
archivePrefix = {arXiv},
arxivId = {1606.07536},
author = {Liu, Ming-Yu and Tuzel, Oncel},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
doi = {arXiv:1606.07536},
eprint = {1606.07536},
file = {:home/jongwook/Dropbox/References/Coupled Generative Adversarial Networks.pdf:pdf},
isbn = {10495258},
issn = {10495258},
pages = {469--477},
title = {{Coupled Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1606.07536},
year = {2016}
}
@inproceedings{grosche2010tempogram,
author = {Grosche, Peter and Muller, Meinard and Kurth, Frank},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:home/jongwook/Dropbox/References/Cyclic Tempogram - a Mid-Level Tempo Representation for Music Signals.pdf:pdf},
isbn = {9781424442966},
keywords = {audio segmentation,chroma,music signals,tempo,tempogram},
title = {{Cyclic Tempogram - a Mid-Level Tempo Representation for Music Signals}},
year = {2010}
}
@article{oord2016wavenet,
author = {van den Oord, A{\"{a}}ron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
file = {:home/jongwook/Dropbox/References/WaveNet A Generative Model for Raw Audio.pdf:pdf},
journal = {arXiv preprint arXiv:1609.03499},
title = {{WaveNet: A Generative Model for Raw Audio}},
year = {2016}
}
@inproceedings{raphael2002transcription,
abstract = {A novel algorithm for automatic transcription of piano polyphonic music is proposed. It is based on a processing scheme that incorporates the following subtasks: segmentation of notes in time domain, estimation of frequency components based on the structure of time segments, extraction of pitches of underlying notes, and tracking of notes to obtain the final music score. A combination of multiresolution techniques, such as multiresolution Fourier transform and maximum likelihood frequency estimator, enables the user to successfully cope with the problems of constant time-frequency resolution and frequency masking. The algorithm demonstrates a better performance then results obtained by means of existing commercial software.},
author = {Raphael, Christopher},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
doi = {10.1109/ISPA.2005.195447},
file = {:home/jongwook/Dropbox/References/Automatic Transcription of Piano Music.pdf:pdf},
isbn = {953184089X},
issn = {16130073},
title = {{Automatic Transcription of Piano Music}},
year = {2002}
}
@inproceedings{harte2006tonnetz,
author = {Harte, Christopher A. and Sandler, Mark B. and Gasser, Martin},
booktitle = {Proceedings of the {ACM} workshop on Audio and Music Computing Multimedia},
doi = {10.1145/1178723.1178727},
file = {:home/jongwook/Dropbox/References/Detecting Harmonic Change in Musical Audio.pdf:pdf},
isbn = {1595935010},
keywords = {audio,harmonic,music,pitch space,segmentation},
pages = {21},
title = {{Detecting Harmonic Change in Musical Audio}},
year = {2006}
}
@misc{hinton2012rmsprop,
author = {Hinton, Geoffrey},
booktitle = {Coursera: Neural Networks for Machine Learning},
file = {:home/jongwook/Dropbox/References/rmsprop Divide the Gradient by a Running Average of Its Recent Magnitude.pdf:pdf},
title = {{rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude}},
year = {2012}
}
@inproceedings{nowozin2016fgan,
archivePrefix = {arXiv},
arxivId = {arXiv:1606.00709},
author = {Nowozin, Sebastian and Cseke, Botond},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {arXiv:1606.00709},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization}},
year = {2016}
}
@article{griffin1984lim,
author = {Griffin, Daniel and Lim, Jae S.},
doi = {10.1109/TASSP.1984.1164317},
file = {:home/jongwook/Dropbox/References/Signal Estimation from Modified Short-Time Fourier transform.pdf:pdf},
issn = {0096-3518},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
number = {2},
pages = {236--243},
title = {{Signal Estimation from Modified Short-Time Fourier transform}},
volume = {32},
year = {1984}
}
@article{jaiswal2017bcgan,
abstract = {Conditional variants of Generative Adversarial Networks (GANs), known as cGANs, are generative models that can produce data samples ($x$) conditioned on both latent variables ($z$) and known auxiliary information ($c$). Another GAN variant, Bidirectional GAN (BiGAN) is a recently developed framework for learning the inverse mapping from $x$ to $z$ through an encoder trained simultaneously with the generator and the discriminator of an unconditional GAN. We propose the Bidirectional Conditional GAN (BCGAN), which combines cGANs and BiGANs into a single framework with an encoder that learns inverse mappings from $x$ to both $z$ and $c$, trained simultaneously with the conditional generator and discriminator in an end-to-end setting. We present crucial techniques for training BCGANs, which incorporate an extrinsic factor loss along with an associated dynamically-tuned importance weight. As compared to other encoder-based GANs, BCGANs not only encode $c$ more accurately but also utilize $z$ and $c$ more effectively and in a more disentangled way to generate data samples.},
archivePrefix = {arXiv},
arxivId = {1711.07461},
author = {Jaiswal, Ayush and AbdAlmageed, Wael and Wu, Yue and Natarajan, Premkumar},
eprint = {1711.07461},
file = {:home/jongwook/Dropbox/References/Bidirectional Conditional Generative Adversarial Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1711.07461},
title = {{Bidirectional Conditional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1711.07461},
year = {2017}
}
@inproceedings{raczynski2010dbn,
author = {Raczynski, Stanislaw and Vincent, Emmanuel and Bimbot, Fr{\'{e}}d{\'{e}}ric and Sagayama, Shigeki and Raczynski, Stanislaw and Vincent, Emmanuel and Bimbot, Fr{\'{e}}d{\'{e}}ric and Sagayama, Shigeki and Vincent, Emmanuel and Bimbot, Fr{\'{e}}d{\'{e}}ric},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Multiple Pitch Transcription Using DBN-Mased Musicological Models.pdf:pdf},
title = {{Multiple Pitch Transcription Using DBN-Mased Musicological Models}},
year = {2010}
}
@book{kleene1951representation,
author = {Kleene, Stephen Cole},
file = {:home/jongwook/Dropbox/References/Representation of Events in Nerve Nets and Finite Automata.pdf:pdf},
institution = {DTIC Document},
title = {{Representation of Events in Nerve Nets and Finite Automata}},
year = {1951}
}
@article{donahue2018wavegan,
archivePrefix = {arXiv},
arxivId = {1802.04208},
author = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
eprint = {1802.04208},
file = {:home/jongwook/Dropbox/References/Synthesizing Audio with Generative Adversarial Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1802.04208},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Synthesizing Audio with Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1802.04208},
year = {2018}
}
@article{poliner2006discriminative,
abstract = {In this paper we present a discriminative model for polyphonic piano transcription. Support Vector Machines trained on spectral features are used to classify frame-level note instances. The classifier outputs are tem-porally constrained via hidden Markov models, and the proposed system is used to transcribe both synthesized and real piano recordings. A frame-level transcription accuracy of 68% was achieved on a newly generated test set, and direct comparisons to previous approaches are provided.},
author = {Poliner, Graham E. and Ellis, Daniel P.W.},
doi = {10.1155/2007/48317},
file = {:home/jongwook/Dropbox/References/A discriminative model for polyphonic piano transcription.pdf:pdf;:home/jongwook/Dropbox/References/A Discriminative Model for Polyphonic Piano Transcription.pdf:pdf},
issn = {11108657},
journal = {{EURASIP} Journal on Advances in Signal Processing},
pages = {1--16},
title = {{A Discriminative Model for Polyphonic Piano Transcription}},
year = {2006}
}
@inproceedings{itoyama2011bayesian,
abstract = {This paper presents a method of both separating audio mixtures into sound sources and identifying the musical instruments of the sources. A statistical tone model of the power spectrogram, called an integrated model, is defined and source separation and instrument identification are carried out on the basis of Bayesian inference. Since, the parameter distributions of the integrated model depend on each instrument, the instrument name is identified by selecting the one that has the maximum relative instrument weight. Experimental results showed correct instrument identification enables precise source separation even when many overtones overlap.},
author = {Itoyama, Katsutoshi and Goto, Masataka and Komatani, Kazunori and Ogata, Tetsuya and Okuno, Hiroshi G.},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
doi = {10.1109/ICASSP.2011.5947183},
file = {:home/jongwook/Dropbox/References/Simultaneous Processing of Sound Source Separation and Musical Instrument Identification using Bayesian Spectral Modeling.pdf:pdf},
isbn = {9781457705397},
issn = {15206149},
keywords = {Bayesian methods,Source separation,instrument identification,spectrogram},
number = {3},
pages = {3816--3819},
publisher = {IEEE},
title = {{Simultaneous Processing of Sound Source Separation and Musical Instrument Identification using Bayesian Spectral Modeling}},
year = {2011}
}
@inproceedings{teng2017generating,
archivePrefix = {arXiv},
arxivId = {1710.02280},
author = {Teng, Yifei and Zhao, An and Goudeseune, Camille},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
eprint = {1710.02280},
file = {:home/jongwook/Dropbox/References/Generating Nontrivial Melodies for Music as a Service.pdf:pdf},
keywords = {Auto-Encoders,Symbolic},
mendeley-tags = {Auto-Encoders,Symbolic},
title = {{Generating Nontrivial Melodies for Music as a Service}},
url = {http://arxiv.org/abs/1710.02280},
year = {2017}
}
@article{bordes2017draw,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.01722},
author = {Wang, Dilin and Liu, Qiang},
eprint = {arXiv:1611.01722},
file = {:home/jongwook/Dropbox/References/Learning to Draw Samples With Application to Amortized MLE for Generative Adversarial Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1611.01722},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning}},
year = {2016}
}
@inproceedings{bock2011enhanced,
author = {B{\"{o}}ck, Sebastian and Schedl, Markus},
booktitle = {Proceedings of the International Conference on Digital Audio Effects {(DAFx)}},
file = {:home/jongwook/Dropbox/References/Enhanced Beat Tracking with Context-Aware Neural Networks.pdf:pdf},
isbn = {9782954035109},
title = {{Enhanced Beat Tracking with Context-Aware Neural Networks}},
year = {2011}
}
@article{ewert2014separation,
author = {Ewert, Sebastian and Pardo, Bryan and M{\"{u}}ller, Meinard and Plumbley, Mark D},
file = {:home/jongwook/Dropbox/References/Score-Informed Source Separation for Musical Audio Recordings An Overview.pdf:pdf},
journal = {{IEEE} Signal Processing Magazine},
pages = {116--124},
title = {{Score-Informed Source Separation for Musical Audio Recordings: An Overview}},
year = {2014}
}
@inproceedings{karras2017pggan,
archivePrefix = {arXiv},
arxivId = {1710.10196},
author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
doi = {10.1002/joe.20070},
eprint = {1710.10196},
file = {:home/jongwook/Dropbox/References/Progressive Growing of GANs for Improved Quality, Stability, and Variation.pdf:pdf},
isbn = {9789186871208},
issn = {15311864},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Progressive Growing of GANs for Improved Quality, Stability, and Variation}},
url = {http://arxiv.org/abs/1710.10196},
year = {2018}
}
@inproceedings{metz2016unrolled,
archivePrefix = {arXiv},
arxivId = {1611.02163},
author = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1611.02163},
file = {:home/jongwook/Dropbox/References/Unrolled Generative Adversarial Networks.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {1--25},
pmid = {202927},
title = {{Unrolled Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1611.02163},
year = {2017}
}
@inproceedings{raffel2014mir_eval,
author = {Raffel, Colin and Mcfee, Brian and Humphrey, Eric J. and Salamon, Justin and Nieto, Oriol and Liang, Dawen and Ellis, Daniel P. W.},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/mir_eval A Transparent Implementation of Common MIR Metrics.pdf:pdf},
pages = {367--372},
title = {{mir\_eval: A Transparent Implementation of Common MIR Metrics}},
year = {2014}
}
@article{vincent2010adaptive,
author = {Vincent, Emmanuel and Bertin, Nancy and Badeau, Roland},
file = {:home/jongwook/Dropbox/References/Adaptive Harmonic Spectral Decomposition for Multiple Pitch Estimation.pdf:pdf},
journal = {{IEEE} Transactions on Audio, Speech, and Language Processing},
number = {3},
pages = {528--537},
title = {{Adaptive Harmonic Spectral Decomposition for Multiple Pitch Estimation}},
volume = {18},
year = {2010}
}
@inproceedings{chuan2017tonnetz,
abstract = {We propose an end-to-end approach for modeling polyphonic music with a novel graphical representation, based on music theory, in a deep neural network. Despite the success of deep learning in various applications, it remains a challenge to incorporate existing domain knowledge in a network without affecting its training routines. In this paper we present a novel approach for predictive music modeling and music generation that incorporates domain knowledge in its representation. In this work, music is transformed into a 2D representation, inspired by tonnetz from music theory, which graphically encodes musical relationships between pitches. This representation is incorporated in a deep network structure consisting of multilayered convolutional neural networks (CNN, for learning an efficient abstract encoding of the representation) and recurrent neural networks with long short-term memory cells (LSTM, for capturing temporal dependencies in music sequences). We empirically evaluate the nature and the effectiveness of the network by using a dataset of classical music from various composers. We investigate the effect of parameters including the number of convolution feature maps, pooling strategies, and three configurations of the network: LSTM without CNN, LSTM with CNN (pre-trained vs. not pre-trained). Visualizations of the feature maps and filters in the CNN are explored, and a comparison is made between the proposed tonnetz-inspired representation and pianoroll, a commonly used representation of music in computational systems. Experimental results show that the tonnetz representation produces musical sequences that are more tonally stable and contain more repeated patterns than sequences generated by pianoroll-based models, a finding that is directly useful for tackling current challenges in music and AI such as smart music generation.},
author = {Chuan, Ching-hua and Herremans, Dorien},
booktitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
file = {:home/jongwook/Dropbox/References/Modeling Temporal Tonal Relations in Polyphonic Music Through Deep Networks with a Novel Image-Based Representation.pdf:pdf},
keywords = {CNN,LSTM,generation,music,tonnetz},
pages = {2159--2166},
title = {{Modeling Temporal Tonal Relations in Polyphonic Music Through Deep Networks with a Novel Image-Based Representation}},
year = {2017}
}
@inproceedings{logan2000mfcc,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Logan, Beth},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
doi = {10.1.1.11.9216},
eprint = {arXiv:1011.1669v3},
file = {:home/jongwook/Dropbox/References/Mel Frequency Cepstral Coefficients for Music Modeling.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
title = {{Mel Frequency Cepstral Coefficients for Music Modeling}},
year = {2000}
}
@inproceedings{oord2017vqvae,
archivePrefix = {arXiv},
arxivId = {1711.00937},
author = {van den Oord, A{\"{a}}ron and Vinyals, Oriol and Kavukcuoglu, Koray},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1711.00937},
file = {:home/jongwook/Dropbox/References/Neural Discrete Representation Learning.pdf:pdf},
title = {{Neural Discrete Representation Learning}},
url = {http://arxiv.org/abs/1711.00937},
year = {2017}
}
@article{lee1999nmf,
abstract = {Is perception of the whole based on perception of its parts? There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.1149},
author = {Lee, Daniel D. and Seung, H. Sebastian},
doi = {10.1038/44565},
eprint = {arXiv:1408.1149},
file = {:home/jongwook/Dropbox/References/Learning the parts of objects by non-negative matrix factorization.pdf:pdf},
isbn = {0028-0836 (Print)\r0028-0836 (Linking)},
issn = {00280836},
journal = {Nature},
number = {6755},
pages = {788--791},
pmid = {10548103},
title = {{Learning the parts of objects by non-negative matrix factorization}},
volume = {401},
year = {1999}
}
@article{achille2018information,
archivePrefix = {arXiv},
arxivId = {1611.01353},
author = {Achille, Alessandro and Soatto, Stefano},
doi = {10.1109/TPAMI.2017.2784440},
eprint = {1611.01353},
file = {:home/jongwook/Dropbox/References/Information Dropout Learning Optimal Representations Through Noisy Computation.pdf:pdf},
issn = {0162-8828},
journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
title = {{Information Dropout: Learning Optimal Representations Through Noisy Computation}},
url = {http://arxiv.org/abs/1611.01353},
year = {2018}
}
@inproceedings{kim2017discogan,
archivePrefix = {arXiv},
arxivId = {1703.05192},
author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jungkwon Kwon and Kim, Jiwon},
booktitle = {Proceedings of the International Conference on Machine Learning {(ICML)}},
eprint = {1703.05192},
issn = {1938-7228},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Learning to Discover Cross-Domain Relations with Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1703.05192},
year = {2017}
}
@book{rosenblatt1957perceptron,
author = {Rosenblatt, Frank},
file = {:home/jongwook/Dropbox/References/The Perceptron, a Perceiving and Recognizing Automaton (Project Para).pdf:pdf},
publisher = {Cornell Aeronautical Laboratory},
title = {{The Perceptron, a Perceiving and Recognizing Automaton (Project Para)}},
year = {1957}
}
@inproceedings{boulangerlewandowski2012nmf,
abstract = {In this paper, we present a supervised method to improve the multiple pitch estimation accuracy of the non-negative matrix factorization (NMF) algorithm. The idea is to extend the sparse NMF framework by incorporating pitch information present in time-aligned musical scores in order to extract features that enforce the separability between pitch labels. We introduce two discriminative criteria that maximize inter-class scatter and quantify the predictive potential of a given decomposition using logistic regressors. Those criteria are applied to both the latent variable and the deterministic autoencoder views of NMF, and we devise efficient update rules for each. We evaluate our method on three polyphonic datasets of piano recordings and orchestral instrument mixes. Both models greatly enhance the quality of the basis spectra learned by NMF and the accuracy of multiple pitch estimation.},
author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Discriminative Non-negative Matrix Factorization for Multiple Pitch Estimation.pdf:pdf},
isbn = {9789727521449},
pages = {205--210},
title = {{Discriminative Non-negative Matrix Factorization for Multiple Pitch Estimation.}},
url = {http://www-etud.iro.umontreal.ca/$\sim$boulanni/ISMIR2012.pdf},
year = {2012}
}
@article{antoniou2017dagan,
archivePrefix = {arXiv},
arxivId = {1711.04340},
author = {Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
eprint = {1711.04340},
file = {:home/jongwook/Dropbox/References/Data Augmentation Generative Adversarial Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1711.04340},
keywords = {GAN},
mendeley-tags = {GAN},
pages = {1--13},
title = {{Data Augmentation Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1711.04340},
year = {2017}
}
@article{chen2018tcvae,
archivePrefix = {arXiv},
arxivId = {1802.04942},
author = {Chen, Tian Qi and Li, Xuechen and Grosse, Roger and Duvenaud, David},
eprint = {1802.04942},
file = {:home/jongwook/Dropbox/References/Isolating Sources of Disentanglement in Variational Autoencoders.pdf:pdf},
journal = {arXiv preprint arXiv:1802.04942},
keywords = {VAE},
mendeley-tags = {VAE},
title = {{Isolating Sources of Disentanglement in Variational Autoencoders}},
url = {https://arxiv.org/pdf/1802.04942.pdf},
year = {2018}
}
@inproceedings{che2016mrgan,
author = {Che, Tong and Li, Yanran and Jacob, Athul Paul and Bengio, Yoshua and Li, Wenjie},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
file = {:home/jongwook/Dropbox/References/Mode Regularized Generative Adversarial Networks.pdf:pdf},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Mode Regularized Generative Adversarial Networks}},
year = {2017}
}
@article{ryynanen2008transcription,
abstract = {First, it provides an easy way of obtaining a of a recording, allowing musicians to play it. the produced transcriptions may be used in mu- sic , information retrieval (MIR) from large databases, content-based audio processing, and},
author = {Ryyn{\"{a}}nen, Matti P. and Klapuri, Anssi P.},
doi = {10.1162/comj.2008.32.3.72},
file = {:home/jongwook/Dropbox/References/Automatic Transcription of Melody, Bass Line, and Chords in Polyphonic Music.pdf:pdf},
isbn = {1424407273},
issn = {01489267},
journal = {Computer Music Journal},
number = {3},
pages = {72--86},
title = {{Automatic Transcription of Melody, Bass Line, and Chords in Polyphonic Music}},
volume = {32},
year = {2008}
}
@article{nayebi2015gruv,
author = {Nayebi, Aran and Vitelli, Matt},
file = {:home/jongwook/Dropbox/References/GRUV Algorithmic Music Generation using Recurrent Neural Networks.pdf:pdf},
journal = {Stanford {CS224d} Class Project},
title = {{GRUV: Algorithmic Music Generation using Recurrent Neural Networks}},
year = {2015}
}
@inproceedings{cho2014seq2seq,
author = {Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing {(EMNLP)}},
file = {:home/jongwook/Dropbox/References/Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
keywords = {RNN},
mendeley-tags = {RNN},
title = {{Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation}},
year = {2014}
}
@inproceedings{lim2017chord,
author = {Lim, Hyungui and Rhyu, Seungyeon and Lee, Kyogu},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Chord Generation From Symbolic Melody Using BLSTM Networks.pdf:pdf},
keywords = {LSTM,Symbolic},
mendeley-tags = {LSTM,Symbolic},
pages = {621--627},
title = {{Chord Generation From Symbolic Melody Using BLSTM Networks}},
year = {2017}
}
@phdthesis{wang2017piano,
author = {Wang, Siying},
file = {:home/jongwook/Dropbox/References/Computational Methods for the Alignment and Score-Informed Transcription of Piano Music.pdf:pdf},
school = {Queen Mary University of London},
title = {{Computational Methods for the Alignment and Score-Informed Transcription of Piano Music}},
type = {Doctoral Thesis},
year = {2017}
}
@inproceedings{benetos2011polyphonic,
author = {Benetos, Emmanouil and Dixon, Simon},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:home/jongwook/Dropbox/References/Polyphonic Music Transcription Using Note Onset and Offset Detection.pdf:pdf},
title = {{Polyphonic Music Transcription Using Note Onset and Offset Detection}},
year = {2011}
}
@article{mcfee2019opensource,
abstract = {In the early years of music information retrieval (MIR), research problems were often centered around conceptually simple tasks, and methods were evaluated on small, idealized data sets. A canonical example of this is genre recognition-i.e., Which one of n genres describes this song?-which was often evaluated on the GTZAN data set (1,000 musical excerpts balanced across ten genres) [1]. As task definitions were simple, so too were signal analysis pipelines, which often derived from methods for speech processing and recognition and typically consisted of simple methods for feature extraction, statistical modeling, and evaluation. When describing a research system, the expected level of detail was superficial: it was sufficient to state, e.g., the number of mel-frequency cepstral coefficients used, the statistical model (e.g., a Gaussian mixture model), the choice of data set, and the evaluation criteria, without stating the underlying software dependencies or implementation details. Because of an increased abundance of methods, the proliferation of software toolkits, the explosion of machine learning, and a focus shift toward more realistic problem settings, modern research systems are substantially more complex than their predecessors. Modern MIR researchers must pay careful attention to detail when processing metadata, implementing evaluation criteria, and disseminating results. {\textcopyright} 1991-2012 IEEE.},
author = {McFee, Brian and Kim, Jong Wook and Cartwright, Mark and Salamon, Justin and Bittner, Rachel M. and Bello, Juan Pablo},
doi = {10.1109/MSP.2018.2875349},
file = {:home/jongwook/Dropbox/References/Open-Source Practices for Music Signal Processing Research Recommendations for Transparent, Sustainable, and Reproducible Audio Research.pdf:pdf},
issn = {15580792},
journal = {{IEEE} Signal Processing Magazine},
number = {1},
pages = {128--137},
publisher = {IEEE},
title = {{Open-Source Practices for Music Signal Processing Research: Recommendations for Transparent, Sustainable, and Reproducible Audio Research}},
volume = {36},
year = {2019}
}
@inproceedings{miyamoto2007transcription,
author = {Miyamoto, Kenichi and Kameoka, Hirokazu and Takeda, Haruto and Nishimoto, Takuya and Sagayama, Shigeki},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
file = {:home/jongwook/Dropbox/References/Probabilistic Approach To Automaticmusic Transcription From Audio Signals.pdf:pdf},
pages = {697--700},
title = {{Probabilistic Approach To Automaticmusic Transcription From Audio Signals}},
year = {2007}
}
@inproceedings{mcleod2018eval,
abstract = {Automatic Music Transcription (AMT) is an important task in music information retrieval. Prior work has focused on multiple fundamental frequency estimation (multi-pitch detection), the conversion of an audio signal into a time-frequency representation such as a MIDI file. It is less common to annotate this output with musical features such as voicing information, metrical structure, and harmonic information, though these are important aspects of a complete transcription. Evaluation of these features is most often performed separately and independent of multi-pitch detection; however, these features are non-independent. We therefore introduce M V 2H, a quantitative, automatic, joint evaluation metric based on musicological principles, and show its effectiveness through the use of specific examples. The metric is modularised in such a way that it can still be used with partially performed annotation-for example, when the transcription process has been applied to some transduced format such as MIDI (which may itself be the result of multi-pitch detection). The code for the evaluation metric described here is available at https://www.github.com/apmcleod/MV2H.},
author = {Mcleod, Andrew and Steedman, Mark},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Evaluating Automatic Polyphonic Music Transcription.pdf:pdf},
pages = {42--49},
title = {{Evaluating Automatic Polyphonic Music Transcription}},
url = {https://www.github.com/apmcleod/MV2H.},
year = {2018}
}
@inproceedings{orio2003following,
author = {Orio, Nicola and Lemouton, Serge and Schwarz, Diemo and Schnell, Norbert},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression {(NIME)}},
file = {:home/jongwook/Dropbox/References/Score Following State of the Art and New Developments.pdf:pdf},
pages = {36--41},
title = {{Score Following: State of the Art and New Developments.}},
year = {2003}
}
@inproceedings{goto2002rwc,
author = {Goto, Masataka and Hashiguchi, Hiroki and Nishimura, Takuichi and Oka, Ryuichi},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/RWC Music Database Popular, Classical and Jazz Music Databases.pdf:pdf},
isbn = {2844261663},
keywords = {Dataset},
mendeley-tags = {Dataset},
pages = {287--288},
title = {{RWC Music Database: Popular, Classical and Jazz Music Databases}},
year = {2002}
}
@article{kumar2018ecommercegan,
archivePrefix = {arXiv},
arxivId = {1801.03244},
author = {Kumar, Ashutosh and Biswas, Arijit and Sanyal, Subhajit},
eprint = {1801.03244},
file = {:home/jongwook/Dropbox/References/eCommerceGAN A Generative Adversarial Network for E-commerce.pdf:pdf},
isbn = {1234567245},
journal = {arXiv preprint arXiv:1801.03244},
keywords = {GAN,acm reference format,deep learning,e-commerce,generative adversarial networks,order embedding,product recommendation},
mendeley-tags = {GAN},
title = {{eCommerceGAN: A Generative Adversarial Network for E-commerce}},
url = {http://arxiv.org/abs/1801.03244},
year = {2018}
}
@inproceedings{li2017mmdgan,
abstract = {Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.},
archivePrefix = {arXiv},
arxivId = {1705.08584},
author = {Li, Chun-Liang and Chang, Wei-Cheng and Cheng, Yu and Yang, Yiming and P{\'{o}}czos, Barnab{\'{a}}s},
booktitle = {Advances in Neural Information Processing Systems {(NIPS)}},
eprint = {1705.08584},
file = {:home/jongwook/Dropbox/References/MMD GAN Towards Deeper Understanding of Moment Matching Network.pdf:pdf},
title = {{MMD GAN: Towards Deeper Understanding of Moment Matching Network}},
url = {http://arxiv.org/abs/1705.08584},
year = {2017}
}
@article{fevotte2011nmf,
abstract = {This paper describes algorithms for nonnegative matrix factorization (NMF) with the beta-divergence (beta-NMF). The beta-divergence is a family of cost functions parametrized by a single shape parameter beta that takes the Euclidean distance, the Kullback-Leibler divergence and the Itakura-Saito divergence as special cases (beta = 2,1,0, respectively). The proposed algorithms are based on a surrogate auxiliary function (a local majorization of the criterion function). We first describe a majorization-minimization (MM) algorithm that leads to multiplicative updates, which differ from standard heuristic multiplicative updates by a beta-dependent power exponent. The monotonicity of the heuristic algorithm can however be proven for beta in (0,1) using the proposed auxiliary function. Then we introduce the concept of majorization-equalization (ME) algorithm which produces updates that move along constant level sets of the auxiliary function and lead to larger steps than MM. Simulations on synthetic and real data illustrate the faster convergence of the ME approach. The paper also describes how the proposed algorithms can be adapted to two common variants of NMF : penalized NMF (i.e., when a penalty function of the factors is added to the criterion function) and convex-NMF (when the dictionary is assumed to belong to a known subspace).},
archivePrefix = {arXiv},
arxivId = {arXiv:1010.1763v3},
author = {F{\'{e}}votte, C{\'{e}}dric and Idier, J{\'{e}}r{\^{o}}me},
doi = {10.1162/NECO_a_00168},
eprint = {arXiv:1010.1763v3},
file = {:home/jongwook/Dropbox/References/Algorithms for nonnegative matrix factorization with the $\beta$-divergence.pdf:pdf},
issn = {1530888X},
journal = {Neural Computation},
keywords = {majorization-equalization,majorization-minimization,me,mm,multiplicative algorithms,nmf,nonnegative matrix factorization,$\beta$ -divergence},
number = {9},
pages = {2421--2456},
title = {{Algorithms for nonnegative matrix factorization with the $\beta$-divergence}},
volume = {23},
year = {2011}
}
@article{abdallah2006unsupervised,
author = {Abdallah, Samer and Plumbley, Mark},
file = {:home/jongwook/Dropbox/References/Unsupervised Analysis of Polyphonic Music Using Sparse Coding.pdf:pdf},
journal = {{IEEE} Transactions on Neural Networks},
keywords = {abilistic modelling,learning overcomplete dictionaries,polyphonic music,prob-,redundancy reduction,sparse factorial coding,unsupervised learning},
number = {1},
pages = {179--196},
title = {{Unsupervised Analysis of Polyphonic Music Using Sparse Coding}},
volume = {17},
year = {2006}
}
@article{xiao2017dnagan,
archivePrefix = {arXiv},
arxivId = {1711.05415},
author = {Xiao, Taihong and Hong, Jiapeng and Ma, Jinwen},
eprint = {1711.05415},
file = {:home/jongwook/Dropbox/References/DNA-GAN Learning Disentangled Representations from Multi-Attribute Images.pdf:pdf},
journal = {arXiv preprint arXiv:1711.05415},
title = {{DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images}},
year = {2017}
}
@article{vincent2013separation,
author = {Vincent, Emmanuel and Bertin, Nancy and Gribonval, R{\'{e}}mi and Bimbot, Fr{\'{e}}d{\'{e}}ric},
file = {:home/jongwook/Dropbox/References/From Blind to Guided Audio Source Separation How Models and Side Information Can Improve the Separation of Sound.pdf:pdf},
journal = {{IEEE} Signal Processing Magazine},
number = {3},
pages = {107--115},
title = {{From Blind to Guided Audio Source Separation: How Models and Side Information Can Improve the Separation of Sound}},
url = {http://hal.inria.fr/docs/00/92/23/78/PDF/vincent_SPM14.pdf},
volume = {31},
year = {2014}
}
@inproceedings{ewert2017admm,
archivePrefix = {arXiv},
arxivId = {1707.00160},
author = {Ewert, Sebastian and Sandler, Mark B.},
booktitle = {Proceedings of the {IEEE} Workshop on Applications of Signal Processing to Audio and Acoustics {(WASPAA)}},
eprint = {1707.00160},
file = {:home/jongwook/Dropbox/References/An Augmented Lagrangian Method for Piano Transcription Using Equal Loudness Thresholding and LSTM-Based Decoding.pdf:pdf;:home/jongwook/Dropbox/References//An Augmented Lagrangian Method for Piano Transcription Using Equal Loudness Thresholding and LSTM-Based Decoding.pdf:pdf},
isbn = {9781538616314},
keywords = {LSTM,Piano,Transcription},
mendeley-tags = {LSTM,Piano,Transcription},
pages = {146--150},
title = {{An Augmented Lagrangian Method for Piano Transcription Using Equal Loudness Thresholding and LSTM-Based Decoding}},
url = {http://arxiv.org/abs/1707.00160},
year = {2017}
}
@inproceedings{luo2017deepclustering,
abstract = {Deep clustering is the first method to handle general audio separation scenarios with multiple sources of the same type and an arbitrary number of sources, performing impressively in speaker-independent speech separation tasks. However, little is known about its effectiveness in other challenging situations such as music source separation. Contrary to conventional networks that directly estimate the source signals, deep clustering generates an embedding for each time-frequency bin, and separates sources by clustering the bins in the embedding space. We show that deep clustering outperforms conventional networks on a singing voice separation task, in both matched and mismatched conditions, even though conventional networks have the advantage of end-to-end training for best signal approximation, presumably because its more flexible objective engenders better regularization. Since the strengths of deep clustering and conventional network architectures appear complementary, we explore combining them in a single hybrid network trained via an approach akin to multi-task learning. Remarkably, the combination significantly outperforms either of its components.},
author = {Luo, Yi and Chen, Zhuo and Hershey, John R. and {Le Roux}, Jonathan and Mesgarani, Nima},
booktitle = {Proceedings of the {IEEE} International Conference on Acoustics, Speech and Signal Processing {(ICASSP)}},
doi = {10.1109/ICASSP.2017.7952118},
file = {:home/jongwook/Dropbox/References/Deep Clustering and Conventional Networks for Music Separation Stronger Together.pdf:pdf},
isbn = {9781509041176},
issn = {15206149},
keywords = {Deep clustering,Deep learning,Music separation,Singing voice separation},
pages = {61--65},
title = {{Deep Clustering and Conventional Networks for Music Separation: Stronger Together}},
year = {2017}
}
@inproceedings{dumoulin2017ali,
archivePrefix = {arXiv},
arxivId = {1606.00704},
author = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Mastropietro, Olivier and Lamb, Alex and Arjovsky, Martin and Courville, Aaron and Mastropietro, Olivier and Courville, Aaron},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1606.00704},
file = {:home/jongwook/Dropbox/References/Adversarially Learned Inference.pdf:pdf},
keywords = {ALI,GAN},
mendeley-tags = {ALI,GAN},
title = {{Adversarially Learned Inference}},
url = {http://arxiv.org/abs/1606.00704},
year = {2017}
}
@inproceedings{kim2018factor,
archivePrefix = {arXiv},
arxivId = {1802.05983},
author = {Kim, Hyunjik and Mnih, Andriy},
booktitle = {Proceedings of the {NIPS} Workshop on Learning Disentangled Representations},
eprint = {1802.05983},
file = {:home/jongwook/Dropbox/References/Disentangling by Factorising.pdf:pdf},
title = {{Disentangling by Factorising}},
year = {2017}
}
@article{agostini2013aid,
author = {Agostini, Andrea and Ghisi, Daniele},
doi = {10.1080/07494467.2013.774221},
file = {:home/jongwook/Dropbox/References/Real-Time Computer-Aided Composition with bach.pdf:pdf},
issn = {07494467},
journal = {Contemporary Music Review},
number = {1},
pages = {41--48},
title = {{Real-Time Computer-Aided Composition with bach}},
volume = {32},
year = {2013}
}
@article{mescheder2017adversarial,
archivePrefix = {arXiv},
arxivId = {1701.04722},
author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
eprint = {1701.04722},
file = {:home/jongwook/Dropbox/References/Adversarial Variational Bayes Unifying Variational Autoencoders and Generative Adversarial Networks.pdf:pdf},
issn = {1938-7228},
journal = {arXiv preprint arXiv:1701.04722},
keywords = {GAN,VAE},
mendeley-tags = {GAN,VAE},
title = {{Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1701.04722},
year = {2017}
}
@article{cho2010chord,
author = {Cho, Taemin and Weiss, Ron J and Bello, Juan P},
file = {:home/jongwook/Dropbox/References/Exploring Common Variations in State-of-the-Art Chord Recognition Systems.pdf:pdf},
journal = {Sound and Music Computing},
pages = {11--22},
title = {{Exploring Common Variations in State-of-the-Art Chord Recognition Systems}},
year = {2010}
}
@article{li2007recommender,
author = {Li, Qing and Myaeng, Sung Hyon and Kim, Byeong Man},
doi = {10.1016/j.ipm.2006.07.005},
file = {:home/jongwook/Dropbox/References/A Probabilistic Music Recommender Considering User Opinions and Audio Features.pdf:pdf},
isbn = {0306-4573},
issn = {03064573},
journal = {Information Processing and Management},
number = {2},
pages = {473--487},
title = {{A Probabilistic Music Recommender Considering User Opinions and Audio Features}},
volume = {43},
year = {2007}
}
@inproceedings{lee2017gp,
archivePrefix = {arXiv},
arxivId = {1711.00165},
author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {1711.00165},
file = {:home/jongwook/Dropbox/References/Deep Neural Networks as Gaussian Processes.pdf:pdf},
pages = {1--14},
title = {{Deep Neural Networks as Gaussian Processes}},
url = {http://arxiv.org/abs/1711.00165},
year = {2018}
}
@inproceedings{hawthorne2019maestro,
archivePrefix = {arXiv},
arxivId = {arXiv:1810.12247v2},
author = {Hawthorne, Curtis and Stasyuk, Andriy and Roberts, Adam and Simon, Ian and Huang, Cheng-zhi Anna and Dieleman, Sander and Elsen, Erich and Engel, Jesse and Eck, Douglas and Brain, Google},
booktitle = {Proceedings of the International Conference on Learning Representations {(ICLR)}},
eprint = {arXiv:1810.12247v2},
file = {:home/jongwook/Dropbox/References/Enabling Factorized Piano Music Modeling and Generation with the MAESTRO dataset.pdf:pdf},
title = {{Enabling Factorized Piano Music Modeling and Generation with the MAESTRO dataset}},
year = {2019}
}
@article{odena2016acgan,
archivePrefix = {arXiv},
arxivId = {1610.09585},
author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
eprint = {1610.09585},
file = {:home/jongwook/Dropbox/References/Conditional Image Synthesis With Auxiliary Classifier GANs.pdf:pdf},
issn = {1938-7228},
journal = {arXiv preprint arXiv:1610.09585},
keywords = {GAN},
mendeley-tags = {GAN},
title = {{Conditional Image Synthesis With Auxiliary Classifier GANs}},
url = {http://arxiv.org/abs/1610.09585},
year = {2016}
}
@article{smaragdis2009relative,
abstract = {Perceived-pitch tracking of potentially aperiodic sounds, as well as pitch tracking of multiple simultaneous sources, is shown to be feasible using a probabilistic methodology. The use of a shift-invariant representation in the constant-Q domain allows the modeling of perceived pitch changes as vertical shifts of spectra. This enables the tracking of these changes in sounds with an arbitrary spectral profile, even those where pitch would be an ill-defined quantity. It is further possible to extend this approach to a mixture model, which allows simultaneous tracking of varying mixed sounds. Demonstrations on real recordings highlight the robustness of such a model under various adverse conditions, and also show some of its unique conceptual differences when compared to traditional pitch tracking approaches.},
author = {Smaragdis, Paris},
doi = {10.1121/1.3106529},
file = {:home/jongwook/Dropbox/References/Relative-Pitch Tracking of Multiple Arbitrary Sounds.pdf:pdf},
issn = {1520-8524},
journal = {The Journal of the Acoustical Society of America},
number = {5},
pages = {3406--13},
pmid = {19425679},
title = {{Relative-Pitch Tracking of Multiple Arbitrary Sounds.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19425679},
volume = {125},
year = {2009}
}
@inproceedings{heittola2009separation,
author = {Heittola, Toni and Klapuri, Anssi and Virtanen, Tuomas},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/Musical Instrument Recognition in Polyphonic Audio Using Source-Filter Model for Sound Separation.pdf:pdf},
isbn = {9780981353708},
pages = {327--332},
title = {{Musical Instrument Recognition in Polyphonic Audio Using Source-Filter Model for Sound Separation}},
year = {2009}
}
@inproceedings{ycart2017sequence,
author = {Ycart, Adrien and Benetos, Emmanouil},
booktitle = {Proceedings of the International Society for Music Information Retrieval {(ISMIR)} Conference},
file = {:home/jongwook/Dropbox/References/A Study on LSTM Networks for Polyphonic Music Sequence Modelling.pdf:pdf},
keywords = {LSTM,Transcription},
mendeley-tags = {LSTM,Transcription},
pages = {421--427},
title = {{A Study on LSTM Networks for Polyphonic Music Sequence Modelling}},
year = {2017}
}
@article{saito2017gan,
archivePrefix = {arXiv},
arxivId = {1709.08041},
author = {Saito, Yuki and Takamichi, Shinnosuke and Saruwatari, Hiroshi},
doi = {10.1109/TASLP.2017.2761547},
eprint = {1709.08041},
file = {:home/jongwook/Dropbox/References/Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks.pdf:pdf},
issn = {2329-9290},
journal = {{IEEE/ACM} Transactions on Audio, Speech, and Language Processing},
keywords = {GAN,Speech},
mendeley-tags = {GAN,Speech},
number = {1},
pages = {84--96},
title = {{Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1709.08041},
volume = {26},
year = {2017}
}
@inproceedings{jansen2017audio,
author = {Jansen, Aren and Plakal, Manoj and Pandya, Ratheet and Ellis, Daniel P W and Hershey, Shawn and Liu, Jiayang and Moore, R Channing and Saurous, Rif A},
booktitle = {Proceedings of the {NIPS} Workshop on Machine Learning for Audio Signal Processing {(ML4Audio)}},
file = {:home/jongwook/Dropbox/References/Towards Learning Semantic Audio Representations from Unlabeled Data.pdf:pdf},
title = {{Towards Learning Semantic Audio Representations from Unlabeled Data}},
url = {http://media.aau.dk/smc/wp-content/uploads/2017/12/ML4AudioNIPS17_paper_18.pdf},
year = {2017}
}
